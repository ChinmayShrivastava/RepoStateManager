{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 16207 nodes and 36362 edges\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "repo_id = '45526e5b-f544-4016-8381-f88f5ca095ea'\n",
    "G = pickle.load(open(repo_id+'/state_0.pkl', 'rb'))\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Superagent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSuperagent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RepoStateManager/venv/lib/python3.11/site-packages/networkx/classes/reportviews.py:360\u001b[0m, in \u001b[0;36mNodeDataView.__getitem__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXError(\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not support slicing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtry list(G.nodes.data())[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;241m.\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;241m.\u001b[39mstop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m     )\n\u001b[0;32m--> 360\u001b[0m ddict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    361\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Superagent'"
     ]
    }
   ],
   "source": [
    "G.nodes(data=True)['Superagent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__\n",
      "SimpleMongoReader\n",
      "RocksetVectorStore\n",
      "SQLJoinQueryEngine\n",
      "BaseStructStoreIndex\n",
      "SentenceSplitter\n",
      "TreeSummarize\n",
      "IPYNBReader\n",
      "AzureOpenAIEmbedding\n",
      "Perplexity\n",
      "CassandraVectorStore\n",
      "LlamaCPP\n",
      "FaissVectorStore\n",
      "LangchainPromptTemplate\n",
      "LLMMultiSelector\n",
      "BagelVectorStore\n",
      "AimCallback\n",
      "DFFullProgram\n",
      "OpenAIQuestionGenerator\n",
      "CondenseQuestionChatEngine\n",
      "DocumentSummaryIndex\n",
      "GoogleVectorStore\n",
      "FaissReader\n",
      "MockChatLLM\n",
      "OpenLLM\n",
      "ZepVectorStore\n",
      "LinearLayer\n",
      "Accumulate\n",
      "EvaporateExtractor\n",
      "CondensePlusContextChatEngine\n",
      "PaLM\n",
      "PGVectoRsStore\n",
      "NLStructStoreQueryEngine\n",
      "ElasticsearchReader\n",
      "SlackReader\n",
      "OpenAIEmbedding\n",
      "ReActAgent\n",
      "MongoDocumentStore\n",
      "OpenAI\n",
      "Neo4jGraphStore\n",
      "Anthropic\n",
      "MetaProvider\n",
      "MultiModalVectorIndexRetriever\n",
      "Refine\n",
      "AwadbReader\n",
      "GradientBaseModelLLM\n",
      "GradientFinetuneEngine\n",
      "VectorStoreIndex\n",
      "OpenAIFinetuneEngine\n",
      "LangchainEmbedding\n",
      "SQLDatabase\n",
      "FirestoreKVStore\n",
      "SummaryExtractor\n",
      "LLMPredictor\n",
      "OnDemandLoaderTool\n",
      "WatsonX\n",
      "PandasIndex\n",
      "MetalVectorStore\n",
      "PGVectorSQLParser\n",
      "DocArrayInMemoryVectorStore\n",
      "DiscordReader\n",
      "TokenCountingHandler\n",
      "UnstructuredElementNodeParser\n",
      "S3DBKVStore\n",
      "PDFReader\n",
      "BufferedGitBlobDataIterator\n",
      "QdrantReader\n",
      "ReplicateMultiModal\n",
      "OpensearchVectorStore\n",
      "MultiModalRetrieverEvaluator\n",
      "SummaryIndexRetriever\n",
      "SimpleSummarize\n",
      "MockObject2\n",
      "ColbertIndex\n",
      "FakeGoogleDataclass\n",
      "ElasticsearchStore\n",
      "SQLStructDatapointExtractor\n",
      "TreeSelectLeafRetriever\n",
      "TwoLayerNN\n",
      "LLMLookaheadAnswerInserter\n",
      "MockForkStepEngine\n",
      "MockIBMModelModule\n",
      "BarkTTS\n",
      "DatasetGenerator\n",
      "SentenceTransformersFinetuneEngine\n",
      "ZillizCloudPipelineRetriever\n",
      "HuggingFaceLLM\n",
      "DynamoDBDocumentStore\n",
      "RssReader\n",
      "AnthropicProvider\n",
      "TransformRetriever\n",
      "TitleExtractor\n",
      "MilvusVectorStore\n",
      "SQLTableNodeMapping\n",
      "BaseOpenAIAgent\n",
      "MboxReader\n",
      "_BaseGradientLLM\n",
      "MultiModalRelevancyEvaluator\n",
      "EpsillaVectorStore\n",
      "FeedbackQueryTransformation\n",
      "MyScaleReader\n",
      "RedisDocumentStore\n",
      "AsyncParamTuner\n",
      "TypesenseVectorStore\n",
      "GuidanceQuestionGenerator\n",
      "MockMongoCollection\n",
      "MyScaleVectorStore\n",
      "ContextChatEngine\n",
      "FunctionTool\n",
      "ObjectIndex\n",
      "SimpleKVStore\n",
      "ChatPromptTemplate\n",
      "KuzuGraphStore\n",
      "MonsterLLM\n",
      "RetrieverTool\n",
      "PsychicReader\n",
      "JSONReader\n",
      "DocArrayHnswVectorStore\n",
      "PromptHelper\n",
      "MetalReader\n",
      "BeirEvaluator\n",
      "Vertex\n",
      "SimpleVectorStore\n",
      "ClarifaiEmbedding\n",
      "TestLLM\n",
      "SummaryIndexEmbeddingRetriever\n",
      "BeautifulSoupWebReader\n",
      "SimpleObjectNodeMapping\n",
      "SQLDocumentContextBuilder\n",
      "BaseIndex\n",
      "MultiModalLLMCompletionProgram\n",
      "BaseFinetuningHandler\n",
      "CognitiveSearchVectorStore\n",
      "PairwiseComparisonEvaluator\n",
      "BaseSynthesizer\n",
      "HTMLTagReader\n",
      "GlobalsHelper\n",
      "SQLStructStoreIndex\n",
      "MockOutputParser\n",
      "VectorIndexAutoRetriever\n",
      "Xinference\n",
      "BaseEvaporateProgram\n",
      "PptxReader\n",
      "MyScaleSettings\n",
      "FirestoreIndexStore\n",
      "LlamaLogger\n",
      "FlagEmbeddingReranker\n",
      "DynamoDBVectorStore\n",
      "LongLLMLinguaPostprocessor\n",
      "QueryFusionRetriever\n",
      "TokenCounter\n",
      "DFRowsProgram\n",
      "EmbeddingSingleSelector\n",
      "NoSuchCorpusException\n",
      "TreeIndexInserter\n",
      "CrossEncoderFinetuneEngine\n",
      "MockEvaluator\n",
      "LlamaDebugHandler\n",
      "TreeIndex\n",
      "HuggingFaceEmbedding\n",
      "MilvusReader\n",
      "GeminiEmbedding\n",
      "SQLAutoVectorQueryEngine\n",
      "LangChainLLM\n",
      "QASummaryQueryEngineBuilder\n",
      "KnowledgeGraphQueryEngine\n",
      "GradientEmbedding\n",
      "ReActAgentWorker\n",
      "RetryQueryEngine\n",
      "BaseTTS\n",
      "HotpotQARetriever\n",
      "MockEmbedding\n",
      "VellumPromptRegistry\n",
      "SimpleIndexStore\n",
      "MistralAI\n",
      "DocumentSummaryIndexEmbeddingRetriever\n",
      "DeepLakeVectorStore\n",
      "KVDocumentStore\n",
      "PandasCSVReader\n",
      "KnowledgeGraphIndex\n",
      "GuidelineEvaluator\n",
      "RetryGuidelineQueryEngine\n",
      "TimescaleVectorStore\n",
      "AwaDBVectorStore\n",
      "DatabaseReader\n",
      "TokenEscaper\n",
      "SubQuestionQueryEngine\n",
      "FLAREInstructQueryEngine\n",
      "ImageVisionLLMReader\n",
      "Portkey\n",
      "TwitterTweetReader\n",
      "ClipEmbedding\n",
      "NotionPageReader\n",
      "OpenAIPydanticProgram\n",
      "SimpleDirectoryReader\n",
      "ObsidianReader\n",
      "BaseRetriever\n",
      "CollectionParams\n",
      "YouRetriever\n",
      "RecursiveRetriever\n",
      "TrafilaturaWebReader\n",
      "IsDoneOutputParser\n",
      "AstraDBVectorStore\n",
      "OpenAIAgent\n",
      "EmptyIndex\n",
      "RouterQueryEngine\n",
      "EventContext\n",
      "SteamshipFileReader\n",
      "MultiModalFaithfulnessEvaluator\n",
      "TreeRootRetriever\n",
      "Clarifai\n",
      "LangchainOutputParser\n",
      "FalkorDBGraphStore\n",
      "LlamaAPI\n",
      "VectaraIndex\n",
      "MultiModalVectorStoreIndex\n",
      "ImageReader\n",
      "GeminiMultiModal\n",
      "ImageCaptionReader\n",
      "FilterField\n",
      "BagelReader\n",
      "Cohere\n",
      "RetrieverRouterQueryEngine\n",
      "SentenceTransformerRerank\n",
      "MongoIndexStore\n",
      "PydanticMultiSelector\n",
      "MyMultipleNegativesRankingLoss\n",
      "AI21\n",
      "GoogleUnivSentEncoderEmbedding\n",
      "SimpleLLMHandler\n",
      "GoogleSheetsReader\n",
      "RagDatasetGenerator\n",
      "ChromaReader\n",
      "OpenRouter\n",
      "AutoMergingRetriever\n",
      "CohereEmbedding\n",
      "PydanticSingleSelector\n",
      "LLMRerank\n",
      "MultiStepQueryEngine\n",
      "PromptTemplate\n",
      "LLMQuestionGenerator\n",
      "ChatGPTRetrievalPluginReader\n",
      "ConditionalException\n",
      "HWPReader\n",
      "InstructorEmbedding\n",
      "VectaraQueryEngine\n",
      "SingleStoreVectorStore\n",
      "NotionToolSpec\n",
      "EntityExtractor\n",
      "SimpleDocumentStore\n",
      "RedisKVStore\n",
      "Bedrock\n",
      "TairVectorStore\n",
      "HyDEQueryTransform\n",
      "VoyageEmbedding\n",
      "MockMongoClient\n",
      "PandasQueryEngine\n",
      "SlackToolSpec\n",
      "SelectorPromptTemplate\n",
      "ImageOutputQueryTransform\n",
      "SimpleWebPageReader\n",
      "DashVectorStore\n",
      "GooglePaLMEmbedding\n",
      "SQLContextContainerBuilder\n",
      "DefaultRefineProgram\n",
      "AzureOpenAI\n",
      "RedisVectorStore\n",
      "TestSQLDatabase\n",
      "BufferedAsyncIterator\n",
      "PGVectorStore\n",
      "MockStreamCompletionWithRetry\n",
      "BedrockEmbedding\n",
      "OptimumEmbedding\n",
      "TokenTextSplitter\n",
      "Playground\n",
      "OpenLLMAPI\n",
      "PredibaseLLM\n",
      "MockPineconeIndex\n",
      "ComposableGraphQueryEngine\n",
      "StepDecomposeQueryTransform\n",
      "RunGptLLM\n",
      "GoogleDocsReader\n",
      "BaseKeywordTableRetriever\n",
      "SupabaseVectorStore\n",
      "MockAgentWorker\n",
      "QueryPlanTool\n",
      "SummaryIndex\n",
      "LocalAI\n",
      "BaseCallbackHandler\n",
      "MetadataReplacementPostProcessor\n",
      "MockObject1\n",
      "OpenInferenceCallbackHandler\n",
      "BaseQueryEngine\n",
      "ColbertRetriever\n",
      "SQLRetriever\n",
      "MarkdownReader\n",
      "BaseManagedIndex\n",
      "SimpleToolNodeMapping\n",
      "LoadAndSearchToolSpec\n",
      "Anyscale\n",
      "TransformQueryEngine\n",
      "CorrectnessEvaluator\n",
      "DynamoDBKVStore\n",
      "LLMSingleSelector\n",
      "BaseToolAsyncAdapter\n",
      "VllmServer\n",
      "PydanticOutputParser\n",
      "VectaraRetriever\n",
      "AgentRunner\n",
      "ZillizCloudPipelineIndex\n",
      "ParallelAgentRunner\n",
      "GithubClient\n",
      "ElasticsearchEmbedding\n",
      "Gemini\n",
      "SimpleGraphStore\n",
      "Phone\n",
      "PromptLayerHandler\n",
      "GoogleTextSynthesizer\n",
      "MockLLM\n",
      "MockMongoDB\n",
      "BaseStructDatapointExtractor\n",
      "MongoDBKVStore\n",
      "LanceDBVectorStore\n",
      "CSVReader\n",
      "TextEmbeddingsInference\n",
      "SummaryIndexLLMRetriever\n",
      "ChatGPTRetrievalPluginClient\n",
      "WikipediaReader\n",
      "OpenAIAgentWorker\n",
      "QuestionsAnsweredExtractor\n",
      "RetrieverEvaluator\n",
      "BatchEvalRunner\n",
      "PineconeReader\n",
      "SimpleMultiModalQueryEngine\n",
      "OpenAIMultiModal\n",
      "FlatReader\n",
      "RedisIndexStore\n",
      "IngestionPipeline\n",
      "EmbeddingAdapterFinetuneEngine\n",
      "DashVectorReader\n",
      "SentenceEmbeddingOptimizer\n",
      "GradientModelAdapterLLM\n",
      "MockStreamChatLLM\n",
      "CachedOpenAIApiKeys\n",
      "Generation\n",
      "KVIndexStore\n",
      "AzureCosmosDBMongoDBVectorSearch\n",
      "Vllm\n",
      "WeaviateVectorStore\n",
      "TreeAllLeafRetriever\n",
      "CitationQueryEngine\n",
      "CallbackManager\n",
      "SimpleQueryToolNodeMapping\n",
      "BM25Retriever\n",
      "LanternVectorStore\n",
      "SemanticSimilarityEvaluator\n",
      "PineconeVectorStore\n",
      "LLMRailsEmbedding\n",
      "DeepLakeReader\n",
      "FaithfulnessEvaluator\n",
      "VideoAudioReader\n",
      "GuardrailsOutputParser\n",
      "WandbCallbackHandler\n",
      "DecomposeQueryTransform\n",
      "CogniswitchQueryEngine\n",
      "GithubRepositoryReader\n",
      "GoogleIndex\n",
      "QdrantVectorStore\n",
      "SQLTableRetrieverQueryEngine\n",
      "CohereRerankRelevancyMetric\n",
      "SQLStructStoreQueryEngine\n",
      "EmptyIndexRetriever\n",
      "Konko\n",
      "ToolRetrieverRouterQueryEngine\n",
      "SimpleChatEngine\n",
      "AdapterEmbeddingModel\n",
      "ObjectRetriever\n",
      "Neo4jVectorStore\n",
      "QueryEngineTool\n",
      "JSONQueryEngine\n",
      "OpenAIAssistantAgent\n",
      "MarvinMetadataExtractor\n",
      "ComposableGraph\n",
      "RetrieverQueryEngine\n",
      "BaseKeywordTableIndex\n",
      "Message\n",
      "KGTableRetriever\n",
      "RelevancyEvaluator\n",
      "MockVectorStore\n",
      "SQLAugmentQueryTransform\n",
      "DocumentSummaryIndexLLMRetriever\n",
      "TencentVectorDB\n",
      "RouterRetriever\n",
      "NLSQLTableQueryEngine\n",
      "ElevenLabsTTS\n",
      "LMFormatEnforcerPydanticProgram\n",
      "LangchainNodeParser\n",
      "NLSQLRetriever\n",
      "OpensearchVectorClient\n",
      "StreamingGeneratorCallbackHandler\n",
      "ContextRetrieverOpenAIAgent\n",
      "MistralAIEmbedding\n",
      "CohereRerank\n",
      "MockRefineProgram\n",
      "NebulaGraphStore\n",
      "CohereRerankerFinetuneEngine\n",
      "HuggingFaceInferenceAPI\n",
      "FirestoreDocumentStore\n",
      "RetrySourceQueryEngine\n",
      "JinaEmbedding\n",
      "MockFaissIndex\n",
      "KnowledgeGraphRAGRetriever\n",
      "EverlyAI\n",
      "ChromaVectorStore\n",
      "VectorIndexRetriever\n",
      "WeaviateReader\n",
      "KeywordExtractor\n",
      "MongoDBAtlasVectorSearch\n",
      "LLMTextCompletionProgram\n",
      "GuidancePydanticProgram\n",
      "BaseSQLTableQueryEngine\n",
      "DynamoDBIndexStore\n",
      "LiteLLM\n",
      "GPTTreeIndexBuilder\n",
      "PGVectorSQLQueryEngine\n",
      "OllamaEmbedding\n",
      "VellumPredictor\n",
      "FastEmbedEmbedding\n",
      "BaseComponent\n",
      "ChatMemoryBuffer\n",
      "Model\n"
     ]
    }
   ],
   "source": [
    "# for nodes with type method\n",
    "for n in G.nodes(data=True):\n",
    "    try:\n",
    "        if n[1]['type'] == 'method':\n",
    "            # get the predecessors\n",
    "            preds = G.predecessors(n[0])\n",
    "            print(n[0])\n",
    "            [print(G.nodes[p]['name']) for p in preds]\n",
    "            break\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from chromadb import PersistentClient, HttpClient\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def return_collection(path=None, collection_name=None):\n",
    "    assert path is not None, \"Path isn't specified\"\n",
    "    assert collection_name is not None, \"Collection name isn't specified\"\n",
    "    chroma_client = PersistentClient(path=path)\n",
    "    emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=os.getenv('OPENAI_API_KEY'),\n",
    "        model_name=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name, embedding_function=emb_fn, metadata={\"hnsw:space\": \"cosine\"})\n",
    "    return collection\n",
    "\n",
    "path_ = f\"{repo_id}/meta/storage\"\n",
    "explanations = return_collection(path=path_, collection_name=\"explanations\")\n",
    "explanations.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = return_collection(path=path_, collection_name=\"code\")\n",
    "code.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets = return_collection(path=path_, collection_name=\"triplets\")\n",
    "triplets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['template_upload_pipeline function-call chunking_pipeline',\n",
       "  'template_upload_pipeline function-call embed_pipeline',\n",
       "  'template_upload_pipeline EXTERNAL_DEPENDENCY chunking_pipeline_0',\n",
       "  'template_upload_pipeline EXTERNAL_DEPENDENCY embed_pipeline_0',\n",
       "  'update_database.py function template_upload_pipeline',\n",
       "  'template_upload_pipeline function-call cot_pipeline',\n",
       "  'template_upload_pipeline EXTERNAL_DEPENDENCY cot_pipeline_0',\n",
       "  'chunking_pipeline EXTERNAL_DEPENDENCY open_files_0',\n",
       "  'embed.py function embed_pipeline',\n",
       "  'chunking_pipeline EXTERNAL_DEPENDENCY split_files_1']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is happening with the template upload pipeline?\"\n",
    "res = triplets.query(\n",
    "    query_texts=[query],\n",
    "    n_results=10\n",
    ")\n",
    "[r for r in res['documents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'containedin': 'update_database.py',\n",
       "   'index': '0',\n",
       "   'name': 'template_upload_pipeline',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'dfs.py',\n",
       "   'index': '2',\n",
       "   'name': 'cot_pipeline',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'embed.py',\n",
       "   'index': '3',\n",
       "   'name': 'embed_pipeline',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'chunker.py',\n",
       "   'index': '5',\n",
       "   'name': 'chunking_pipeline',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'embed.py',\n",
       "   'index': '0',\n",
       "   'name': 'get_embedding',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'agents.py',\n",
       "   'index': '1',\n",
       "   'name': 'create_assistant',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'embed.py',\n",
       "   'index': '1',\n",
       "   'name': 'open_data_json',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'agents.py',\n",
       "   'index': '2',\n",
       "   'name': 'create_thread',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'search.py',\n",
       "   'index': '1',\n",
       "   'name': 'open_embeddings_json',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'search.py',\n",
       "   'index': '0',\n",
       "   'name': 'open_data_json',\n",
       "   'type': 'function'}]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is happening with the template upload pipeline?\"\n",
    "res = code.query(\n",
    "    query_texts=[query],\n",
    "    n_results=10\n",
    ")\n",
    "[r for r in res['metadatas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[{'name': 'template_upload_pipeline', 'type': 'function'},\n",
       "   {'name': 'embed_pipeline', 'type': 'function'},\n",
       "   {'name': 'chunking_pipeline', 'type': 'function'},\n",
       "   {'name': 'cot_pipeline', 'type': 'function'},\n",
       "   {'name': 'add_files', 'type': 'function'},\n",
       "   {'name': 'open_data_json', 'type': 'function'},\n",
       "   {'name': 'extract_education_and_experience', 'type': 'function'},\n",
       "   {'name': 'split_files', 'type': 'function'},\n",
       "   {'name': 'open_embeddings_json', 'type': 'function'},\n",
       "   {'name': 'store_data', 'type': 'function'}]],\n",
       " [['The code snippet defines a function called template_upload_pipeline. This function calls three other functions: chunking_pipeline, embed_pipeline, and cot_pipeline. It is likely that these functions are part of a larger pipeline for uploading and processing templates.',\n",
       "   'The function embed_pipeline does two things. First, it calls the open_data_json function to retrieve some data. Then, it calls the save_embeddings function to save the embeddings of that data.',\n",
       "   'The code snippet defines a function called \"chunking_pipeline\" that takes in two optional arguments: \"folder\" and \"filename\". \\n\\nInside the function, it performs a series of operations. \\n\\nFirst, it calls the \"open_files\" function to retrieve a list of files from a specified folder. \\n\\nThen, it calls the \"split_files\" function to split each file into smaller chunks. \\n\\nNext, it calls the \"extract_education_and_experience\" function to extract the education and experience sections from each chunk. \\n\\nFinally, it calls the \"store_data\" function to store the extracted data into a specified file.',\n",
       "   'The code snippet defines a function called cot_pipeline. Inside the function, it calls two other functions: open_data_json and create_cot. It then opens a file with the name specified by the variable \"filename\" in the \"data\" directory and writes the contents of the \"data\" variable as a JSON object with an indentation of 4 spaces.',\n",
       "   'The code defines a function called \"add_files\" that takes in a list of file names. It creates an empty list called \"files\". Then, for each file name in the input list, it uses the \"client.files.create\" method to create a file object with the file name and a purpose of \\'assistants\\'. It appends the id of the created file object to the \"files\" list. Finally, it returns the list of file ids.',\n",
       "   'The function open_data_json lists all the .json files available in the data/ directory. It then asks the user to select one of the files. Once the user enters a filename, it opens the selected file and loads its contents as a JSON object. If no filename is entered, it uses a default file called data.json. Finally, it returns the loaded data and the selected filename.',\n",
       "   'The code defines a function called \"extract_education_and_experience\" that takes in a list of strings called \"split_files\" as input. \\n\\nInside the function, there is an empty list called \"education_experience\". \\n\\nThe code then loops through each element in the \"split_files\" list using a for loop. \\n\\nFor each element, the code splits the string into two sections using the delimiter \"##\" and assigns it to the variable \"split_file\". \\n\\nThe \"split_file\" is then appended to the \"education_experience\" list. \\n\\nFinally, the function returns the \"education_experience\" list, which contains the split sections of each string element in the \"split_files\" list.',\n",
       "   'The function split_files takes in a list of string elements. It splits each element into multiple sections using the delimiter \"!@#\". It then returns a new list containing all the split sections from all the input files.',\n",
       "   \"The function open_embeddings_json opens a file called 'embeddings.json' in the 'data' folder. It reads the contents of the file and stores it in a variable called embeddings. It then returns the embeddings variable.\",\n",
       "   \"The function store_data takes in two arguments, education_experience (a list of lists) and filename (a string with a default value of 'data.json'). It creates an empty dictionary called data. It then iterates over each element in the education_experience list, and for each element, it calls the final_chunks function to get the education_chunks and experience_chunks. It stores these chunks in the data dictionary with the index as the key. \\n\\nNext, it prompts the user to enter a file name. If the user enters a file name, it updates the filename variable. \\n\\nFinally, it opens a file with the specified filename in the 'data' directory and writes the data dictionary as a JSON object with an indentation of 4.\"]]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is happening with the template upload pipeline?\"\n",
    "res = explanations.query(\n",
    "    query_texts=[query],\n",
    "    n_results=10\n",
    ")\n",
    "[r for r in (res['metadatas'], res['documents'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('update_database.py', 'template_upload_pipeline', {'type': 'function'})\n",
      "('template_upload_pipeline', 'chunking_pipeline', {'type': 'function-call'})\n",
      "('template_upload_pipeline', 'embed_pipeline', {'type': 'function-call'})\n",
      "('template_upload_pipeline', 'cot_pipeline', {'type': 'function-call'})\n",
      "('template_upload_pipeline', 'chunking_pipeline_0', {'type': 'EXTERNAL_DEPENDENCY'})\n",
      "('template_upload_pipeline', 'embed_pipeline_0', {'type': 'EXTERNAL_DEPENDENCY'})\n",
      "('template_upload_pipeline', 'cot_pipeline_0', {'type': 'EXTERNAL_DEPENDENCY'})\n"
     ]
    }
   ],
   "source": [
    "for edge in G.edges(data=True):\n",
    "    if 'template_upload_pipeline' in edge:\n",
    "        print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the output connections, what comes out and where it coes in (a train of events). How the functions are being called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chunker.py', 'chunking_pipeline', {'type': 'function'})\n",
      "('chunking_pipeline', 'open_files', {'type': 'function-call'})\n",
      "('chunking_pipeline', 'extract_education_and_experience', {'type': 'function-call'})\n",
      "('chunking_pipeline', 'store_data', {'type': 'function-call'})\n",
      "('chunking_pipeline', 'open_files_0', {'type': 'EXTERNAL_DEPENDENCY'})\n",
      "('chunking_pipeline', 'split_files_1', {'type': 'EXTERNAL_DEPENDENCY'})\n",
      "('chunking_pipeline', 'extract_education_and_experience_0', {'type': 'EXTERNAL_DEPENDENCY'})\n",
      "('chunking_pipeline', 'store_data_0', {'type': 'EXTERNAL_DEPENDENCY'})\n",
      "('template_upload_pipeline', 'chunking_pipeline', {'type': 'function-call'})\n"
     ]
    }
   ],
   "source": [
    "for edge in G.edges(data=True):\n",
    "    if 'chunking_pipeline' in edge:\n",
    "        print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
