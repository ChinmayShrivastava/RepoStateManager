{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 143 nodes and 186 edges\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "repo_id = 'bf16a3a0-75de-4ac5-9ff2-e2f25edf4ba4'\n",
    "G = pickle.load(open(repo_id+'/state_0.pkl', 'rb'))\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_embedding',\n",
       " 'type': 'function',\n",
       " 'index': '0',\n",
       " 'elementname': 'embed!!function!!0!!get_embedding.py',\n",
       " 'explanation': 'The function get_embedding takes in three arguments: chunks (a list), embeddings_model (a pre-trained model), and batch_size (an integer). It returns two values: chunks (the input list) and embeddings (a list of embeddings).\\n\\nThe function loops through the chunks list in batches of size batch_size. It calls the embeddings_model to embed each batch of chunks and appends the embeddings to the embeddings list. It also prints a progress message indicating the range of chunks that have been embedded.\\n\\nFinally, it returns the original chunks list and the embeddings list.',\n",
       " 'info': {'generated': True,\n",
       "  'date_modified': '2023-12-24 19:13:42',\n",
       "  'n_info_edges': 5}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes(data=True)['get_embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from chromadb import PersistentClient, HttpClient\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def return_collection(path=None, collection_name=None):\n",
    "    assert path is not None, \"Path isn't specified\"\n",
    "    assert collection_name is not None, \"Collection name isn't specified\"\n",
    "    chroma_client = PersistentClient(path=path)\n",
    "    emb_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=os.getenv('OPENAI_API_KEY'),\n",
    "        model_name=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name, embedding_function=emb_fn, metadata={\"hnsw:space\": \"cosine\"})\n",
    "    return collection\n",
    "\n",
    "path_ = f\"{repo_id}/meta/storage\"\n",
    "explanations = return_collection(path=path_, collection_name=\"explanations\")\n",
    "explanations.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = return_collection(path=path_, collection_name=\"code\")\n",
    "code.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets = return_collection(path=path_, collection_name=\"triplets\")\n",
    "triplets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['template_upload_pipeline function-call chunking_pipeline',\n",
       "  'template_upload_pipeline function-call embed_pipeline',\n",
       "  'template_upload_pipeline EXTERNAL_DEPENDENCY chunking_pipeline_0',\n",
       "  'template_upload_pipeline EXTERNAL_DEPENDENCY embed_pipeline_0',\n",
       "  'update_database.py function template_upload_pipeline',\n",
       "  'template_upload_pipeline function-call cot_pipeline',\n",
       "  'template_upload_pipeline EXTERNAL_DEPENDENCY cot_pipeline_0',\n",
       "  'chunking_pipeline EXTERNAL_DEPENDENCY open_files_0',\n",
       "  'embed.py function embed_pipeline',\n",
       "  'chunking_pipeline EXTERNAL_DEPENDENCY split_files_1']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is happening with the template upload pipeline?\"\n",
    "res = triplets.query(\n",
    "    query_texts=[query],\n",
    "    n_results=10\n",
    ")\n",
    "[r for r in res['documents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'containedin': 'update_database.py',\n",
       "   'index': '0',\n",
       "   'name': 'template_upload_pipeline',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'dfs.py',\n",
       "   'index': '2',\n",
       "   'name': 'cot_pipeline',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'embed.py',\n",
       "   'index': '3',\n",
       "   'name': 'embed_pipeline',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'chunker.py',\n",
       "   'index': '5',\n",
       "   'name': 'chunking_pipeline',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'embed.py',\n",
       "   'index': '0',\n",
       "   'name': 'get_embedding',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'agents.py',\n",
       "   'index': '1',\n",
       "   'name': 'create_assistant',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'embed.py',\n",
       "   'index': '1',\n",
       "   'name': 'open_data_json',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'agents.py',\n",
       "   'index': '2',\n",
       "   'name': 'create_thread',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'search.py',\n",
       "   'index': '1',\n",
       "   'name': 'open_embeddings_json',\n",
       "   'type': 'function'},\n",
       "  {'containedin': 'search.py',\n",
       "   'index': '0',\n",
       "   'name': 'open_data_json',\n",
       "   'type': 'function'}]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is happening with the template upload pipeline?\"\n",
    "res = code.query(\n",
    "    query_texts=[query],\n",
    "    n_results=10\n",
    ")\n",
    "[r for r in res['metadatas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[{'name': 'template_upload_pipeline', 'type': 'function'},\n",
       "   {'name': 'embed_pipeline', 'type': 'function'},\n",
       "   {'name': 'chunking_pipeline', 'type': 'function'},\n",
       "   {'name': 'cot_pipeline', 'type': 'function'},\n",
       "   {'name': 'add_files', 'type': 'function'},\n",
       "   {'name': 'open_data_json', 'type': 'function'},\n",
       "   {'name': 'extract_education_and_experience', 'type': 'function'},\n",
       "   {'name': 'split_files', 'type': 'function'},\n",
       "   {'name': 'open_embeddings_json', 'type': 'function'},\n",
       "   {'name': 'store_data', 'type': 'function'}]],\n",
       " [['The code snippet defines a function called template_upload_pipeline. This function calls three other functions: chunking_pipeline, embed_pipeline, and cot_pipeline. It is likely that these functions are part of a larger pipeline for uploading and processing templates.',\n",
       "   'The function embed_pipeline does two things. First, it calls the open_data_json function to retrieve some data. Then, it calls the save_embeddings function to save the embeddings of that data.',\n",
       "   'The code snippet defines a function called \"chunking_pipeline\" that takes in two optional arguments: \"folder\" and \"filename\". \\n\\nInside the function, it performs a series of operations. \\n\\nFirst, it calls the \"open_files\" function to retrieve a list of files from a specified folder. \\n\\nThen, it calls the \"split_files\" function to split each file into smaller chunks. \\n\\nNext, it calls the \"extract_education_and_experience\" function to extract the education and experience sections from each chunk. \\n\\nFinally, it calls the \"store_data\" function to store the extracted data into a specified file.',\n",
       "   'The code snippet defines a function called cot_pipeline. Inside the function, it calls two other functions: open_data_json and create_cot. It then opens a file with the name specified by the variable \"filename\" in the \"data\" directory and writes the contents of the \"data\" variable as a JSON object with an indentation of 4 spaces.',\n",
       "   'The code defines a function called \"add_files\" that takes in a list of file names. It creates an empty list called \"files\". Then, for each file name in the input list, it uses the \"client.files.create\" method to create a file object with the file name and a purpose of \\'assistants\\'. It appends the id of the created file object to the \"files\" list. Finally, it returns the list of file ids.',\n",
       "   'The function open_data_json lists all the .json files available in the data/ directory. It then asks the user to select one of the files. Once the user enters a filename, it opens the selected file and loads its contents as a JSON object. If no filename is entered, it uses a default file called data.json. Finally, it returns the loaded data and the selected filename.',\n",
       "   'The code defines a function called \"extract_education_and_experience\" that takes in a list of strings called \"split_files\" as input. \\n\\nInside the function, there is an empty list called \"education_experience\". \\n\\nThe code then loops through each element in the \"split_files\" list using a for loop. \\n\\nFor each element, the code splits the string into two sections using the delimiter \"##\" and assigns it to the variable \"split_file\". \\n\\nThe \"split_file\" is then appended to the \"education_experience\" list. \\n\\nFinally, the function returns the \"education_experience\" list, which contains the split sections of each string element in the \"split_files\" list.',\n",
       "   'The function split_files takes in a list of string elements. It splits each element into multiple sections using the delimiter \"!@#\". It then returns a new list containing all the split sections from all the input files.',\n",
       "   \"The function open_embeddings_json opens a file called 'embeddings.json' in the 'data' folder. It reads the contents of the file and stores it in a variable called embeddings. It then returns the embeddings variable.\",\n",
       "   \"The function store_data takes in two arguments, education_experience (a list of lists) and filename (a string with a default value of 'data.json'). It creates an empty dictionary called data. It then iterates over each element in the education_experience list, and for each element, it calls the final_chunks function to get the education_chunks and experience_chunks. It stores these chunks in the data dictionary with the index as the key. \\n\\nNext, it prompts the user to enter a file name. If the user enters a file name, it updates the filename variable. \\n\\nFinally, it opens a file with the specified filename in the 'data' directory and writes the data dictionary as a JSON object with an indentation of 4.\"]]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is happening with the template upload pipeline?\"\n",
    "res = explanations.query(\n",
    "    query_texts=[query],\n",
    "    n_results=10\n",
    ")\n",
    "[r for r in (res['metadatas'], res['documents'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('update_database.py', 'template_upload_pipeline', {'type': 'function'})\n",
      "('template_upload_pipeline', 'chunking_pipeline', {'type': 'function-call'})\n",
      "('template_upload_pipeline', 'embed_pipeline', {'type': 'function-call'})\n",
      "('template_upload_pipeline', 'cot_pipeline', {'type': 'function-call'})\n",
      "('template_upload_pipeline', 'chunking_pipeline_0', {'type': 'EXTERNAL_DEPENDENCY'})\n",
      "('template_upload_pipeline', 'embed_pipeline_0', {'type': 'EXTERNAL_DEPENDENCY'})\n",
      "('template_upload_pipeline', 'cot_pipeline_0', {'type': 'EXTERNAL_DEPENDENCY'})\n"
     ]
    }
   ],
   "source": [
    "for edge in G.edges(data=True):\n",
    "    if 'template_upload_pipeline' in edge:\n",
    "        print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the output connections, what comes out and where it coes in (a train of events). How the functions are being called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chunker.py', 'chunking_pipeline', {'type': 'function'})\n",
      "('chunking_pipeline', 'open_files', {'type': 'function-call'})\n",
      "('chunking_pipeline', 'extract_education_and_experience', {'type': 'function-call'})\n",
      "('chunking_pipeline', 'store_data', {'type': 'function-call'})\n",
      "('chunking_pipeline', 'open_files_0', {'type': 'EXTERNAL_DEPENDENCY'})\n",
      "('chunking_pipeline', 'split_files_1', {'type': 'EXTERNAL_DEPENDENCY'})\n",
      "('chunking_pipeline', 'extract_education_and_experience_0', {'type': 'EXTERNAL_DEPENDENCY'})\n",
      "('chunking_pipeline', 'store_data_0', {'type': 'EXTERNAL_DEPENDENCY'})\n",
      "('template_upload_pipeline', 'chunking_pipeline', {'type': 'function-call'})\n"
     ]
    }
   ],
   "source": [
    "for edge in G.edges(data=True):\n",
    "    if 'chunking_pipeline' in edge:\n",
    "        print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
