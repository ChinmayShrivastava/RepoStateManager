{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chinmayshrivastava/Documents/GitHub/RepoStateManager/repomanager/statemanager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinmayshrivastava/Documents/GitHub/RepoStateManager/venv/lib/python3.11/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# navigate two directories up\n",
    "dir = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "sys.path.append(dir)\n",
    "# print working directory\n",
    "# mavigate to the top of working directory\n",
    "os.chdir('../../')\n",
    "os.chdir('statemanager')\n",
    "print(os.getcwd())\n",
    "from vspace.vsearch import VectorSearch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chinmayshrivastava/Documents/GitHub/RepoStateManager/repomanager/statemanager'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/chinmayshrivastava/Documents/GitHub/RepoStateManager/repomanager/tests/documentation_vector_search',\n",
       " '/Users/chinmayshrivastava/anaconda3/lib/python311.zip',\n",
       " '/Users/chinmayshrivastava/anaconda3/lib/python3.11',\n",
       " '/Users/chinmayshrivastava/anaconda3/lib/python3.11/lib-dynload',\n",
       " '',\n",
       " '/Users/chinmayshrivastava/Documents/GitHub/RepoStateManager/venv/lib/python3.11/site-packages',\n",
       " '/Users/chinmayshrivastava/Documents/GitHub/RepoStateManager']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"45526e5b-f544-4016-8381-f88f5ca095ea\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 31986 nodes and 53803 edges\n"
     ]
    }
   ],
   "source": [
    "G = pickle.load(open(\"state/{}/state_0.pkl\".format(repo_id), \"rb\"))\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = VectorSearch(\n",
    "    collection_name=\"docs\",\n",
    "    collection_path=\"state/{}/meta/storage\".format(repo_id),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14163"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs.collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16169559955596924,\n",
       " 0.16963738203048706,\n",
       " 0.17061495780944824,\n",
       " 0.17151081562042236,\n",
       " 0.17151081562042236]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"For the user query, match the best use case or insight from a code base.\n",
    "user query: How to rerank\"\"\"\n",
    "nodes = [x[\"node\"] for x in vs.search_all(query)['metadatas'][0]]\n",
    "distances = vs.search_all(query)['distances'][0]\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define Advanced Retriever\n",
      "We define an advanced retriever that performs the following steps:\n",
      "\n",
      "Query generation/rewriting: generate multiple queries given the original user query\n",
      "Perform retrieval for each query over an ensemble of retrievers.\n",
      "Reranking/fusion: fuse results from all queries, and apply a reranking step to \"fuse\" the top relevant results!\n",
      "\n",
      "Then in the next section we'll plug this into our response synthesis module.\n",
      "\n",
      "Step 1: Query Generation/Rewriting\n",
      "The first step is to generate queries from the original query to better match the query intent, and increase precision/recall of the retrieved results. For instance, we might be able to rewrite the query into smaller queries.\n",
      "We can do this by prompting ChatGPT.\n",
      "\n",
      "input:\n",
      "```python\n",
      "from llama_index import PromptTemplate\n",
      "```\n",
      "output:\n",
      "input:\n",
      "```python\n",
      "query_str = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\"\n",
      "```\n",
      "output:\n",
      "input:\n",
      "```python\n",
      "query_gen_prompt_str = (\n",
      "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
      "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
      "    \"related to the following input query:\\n\"\n",
      "    \"Query: {query}\\n\"\n",
      "    \"Queries:\\n\"\n",
      ")\n",
      "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
      "```\n",
      "output:\n",
      "input:\n",
      "```python\n",
      "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
      "    fmt_prompt = query_gen_prompt.format(\n",
      "        num_queries=num_queries - 1, query=query_str\n",
      "    )\n",
      "    response = llm.complete(fmt_prompt)\n",
      "    queries = response.text.split(\"\\n\")\n",
      "    return queries\n",
      "```\n",
      "output:\n",
      "input:\n",
      "```python\n",
      "queries = generate_queries(llm, query_str, num_queries=4)\n",
      "```\n",
      "output:\n",
      "input:\n",
      "```python\n",
      "print(queries)\n",
      "```\n",
      "output:\n",
      "\n",
      "Directly retrieve top 2 most similar nodes\n",
      "\n",
      "input:\n",
      "```python\n",
      "query_engine = index.as_query_engine(\n",
      "    similarity_top_k=2,\n",
      ")\n",
      "response = query_engine.query(\n",
      "    \"What did Sam Altman do in this essay?\",\n",
      ")\n",
      "```\n",
      "output:\n",
      "\n",
      "Advanced RAG\n",
      "The GoogleIndex is built based on GoogleVectorStore and GoogleTextSynthesizer.\n",
      "These components can be combined with other powerful constructs in LlamaIndex to produce advanced RAG applications.\n",
      "Below we show a few examples.\n",
      "\n",
      "Reranker + Google Retriever\n",
      "Converting content into vectors is a lossy process. LLM-based Reranking\n",
      "remediates this by reranking the retrieved content using LLM, which has higher\n",
      "fidelity because it has access to both the actual query and the passage.\n",
      "\n",
      "input:\n",
      "```python\n",
      "from llama_index.response_synthesizers.google.generativeai import (\n",
      "    GoogleTextSynthesizer,\n",
      ")\n",
      "from llama_index.vector_stores.google.generativeai import (\n",
      "    GoogleVectorStore,\n",
      "    google_service_context,\n",
      ")\n",
      "from llama_index import ServiceContext, VectorStoreIndex\n",
      "from llama_index.llms import PaLM\n",
      "from llama_index.postprocessor import LLMRerank\n",
      "from llama_index.query_engine import RetrieverQueryEngine\n",
      "from llama_index.retrievers import VectorIndexRetriever\n",
      "\n",
      "# Set up the query engine with a reranker.\n",
      "store = GoogleVectorStore.from_corpus(corpus_id=\"some-corpus-id\")\n",
      "index = VectorStoreIndex.from_vector_store(\n",
      "    vector_store=store, service_context=google_service_context\n",
      ")\n",
      "response_synthesizer = GoogleTextSynthesizer.from_defaults(\n",
      "    temperature=0.7, answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE\n",
      ")\n",
      "reranker = LLMRerank(\n",
      "    top_n=10, service_context=ServiceContext.from_defaults(llm=PaLM())\n",
      ")\n",
      "query_engine = RetrieverQueryEngine.from_args(\n",
      "    retriever=VectorIndexRetriever(\n",
      "        index=index,\n",
      "        similarity_top_k=20,\n",
      "    ),\n",
      "    response_synthesizer=response_synthesizer,\n",
      "    node_postprocessors=[reranker],\n",
      ")\n",
      "\n",
      "# Query for better result!\n",
      "response = query_engine.query(\"What movie should I watch with my family?\")\n",
      "```\n",
      "output:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for node in nodes:\n",
    "    # get the node from the graph and get the content\n",
    "    print(G.nodes[node][\"content\"])\n",
    "    # print(G.nodes[node[:len(node)-1]+f\"{int(node[-1])-1}\"][\"content\"])\n",
    "    i = i + 1\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.llama_agents.FunctionCaller import LLMReranker\n",
    "from defaults.vsearch import DISTANCE_THRESHOLD_cdb\n",
    "def get_docs(\n",
    "    query: str,\n",
    "    vs: VectorSearch,\n",
    "    G,\n",
    "    top_k: int = 25,\n",
    "    max_depth: int = 3,\n",
    "    to_return: int = 2,\n",
    "    ):\n",
    "    res = vs.search_all(query, top_k=top_k)\n",
    "    nodes = [x[\"node\"] for x in res['metadatas'][0]]\n",
    "    distances = res['distances'][0]\n",
    "    docs = []\n",
    "    for node in nodes:\n",
    "        docs.append(G.nodes[node][\"content\"])\n",
    "    i = 0\n",
    "    _new_docs = []\n",
    "    for doc, dist in zip(docs, distances):\n",
    "        if dist > DISTANCE_THRESHOLD_cdb:\n",
    "            continue\n",
    "        if i>=5:\n",
    "            break\n",
    "        if len(doc.split()) < 10:\n",
    "            continue\n",
    "        if len(doc.split()) > 200:\n",
    "            doc = \" \".join(doc.split()[:200])\n",
    "        _new_docs.append(doc)\n",
    "        i = i + 1\n",
    "    reranker = LLMReranker()\n",
    "    reranked_docs = reranker.rerank(query, _new_docs)\n",
    "    print(reranked_docs)\n",
    "    docs = reranked_docs[:to_return]\n",
    "    return docs, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 3\n",
      "[3, 1, 2]\n",
      "[\"\\n\\nChat Engine - ReAct Agent Mode\\n\\nReAct is an agent based chat mode built on top of a query engine over your data.\\n\\nFor each chat interaction, the agent enter a ReAct loop:\\n\\nfirst decide whether to use the query engine tool and come up with appropriate input\\n(optional) use the query engine tool and observe its output\\ndecide whether to repeat or give final response\\n\\n\\nThis approach is flexible, since it can flexibility choose between querying the knowledge base or not.\\nHowever, the performance is also more dependent on the quality of the LLM.\\nYou might need to do more coercing to make sure it chooses to query the knowledge base at right times, instead of hallucinating an answer.\\n\\nIf you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.\\n\\ninput:\\n```python\\n!pip install llama-index\\n```\\noutput:\\n\", 'Run Some Example Queries\\nWe run some example queries using the agent, showcasing some of the agent\\'s abilities to do chain-of-thought-reasoning and tool use to synthesize the right answer.\\nWe also show queries.\\n\\ninput:\\n```python\\nresponse = agent.chat(\\n    \"Compare and contrast the revenue growth of Uber and Lyft in 2021, then\"\\n    \" give an analysis\"\\n)\\nprint(str(response))\\n```\\noutput:\\n', 'Build Retriever-Enabled OpenAI Agent We build a top-level agent that can orchestrate across the different document agents to answer any user query. This RetrieverOpenAIAgent performs tool retrieval before tool use (unlike a default agent that tries to put all tools in the prompt). Improvements from V0: We make the following improvements compared to the \"base\" version in V0. Adding in reranking: we use Cohere reranker to better filter the candidate set of documents. Adding in a query planning tool: we add an explicit query planning tool that\\'s dynamically created based on the set of retrieved tools. input: ```python # define tool for each document agent all_tools = [] for file_base, agent in agents_dict.items(): summary = extra_info_dict[file_base][\"summary\"] doc_tool = QueryEngineTool( query_engine=agent, metadata=ToolMetadata( name=f\"tool_{file_base}\", description=summary, ), ) all_tools.append(doc_tool) ``` output: input: ```python print(all_tools[0].metadata) ``` output: input: ```python # define an \"object\" index and retriever over these tools from llama_index import VectorStoreIndex from llama_index.objects import ( ObjectIndex, SimpleToolNodeMapping, ObjectRetriever, ) from llama_index.retrievers import BaseRetriever from llama_index.postprocessor import CohereRerank from llama_index.tools import QueryPlanTool from llama_index.query_engine import SubQuestionQueryEngine from llama_index.llms import OpenAI llm = OpenAI(model_name=\"gpt-4-0613\") tool_mapping = SimpleToolNodeMapping.from_objects(all_tools) obj_index = ObjectIndex.from_objects( all_tools, tool_mapping, VectorStoreIndex, ) vector_node_retriever = obj_index.as_node_retriever(similarity_top_k=10) # define a custom retriever with reranking']\n",
      "201\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"For the user query, match the best use case or insight from a code base.\n",
    "user query: What are the different types of agents?\"\"\"\n",
    "a, b = get_docs(query, vs, G)\n",
    "l = sum([len(x.split()) for x in a])\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n\\nChat Engine - ReAct Agent Mode\\n\\nReAct is an agent based chat mode built on top of a query engine over your data.\\n\\nFor each chat interaction, the agent enter a ReAct loop:\\n\\nfirst decide whether to use the query engine tool and come up with appropriate input\\n(optional) use the query engine tool and observe its output\\ndecide whether to repeat or give final response\\n\\n\\nThis approach is flexible, since it can flexibility choose between querying the knowledge base or not.\\nHowever, the performance is also more dependent on the quality of the LLM.\\nYou might need to do more coercing to make sure it chooses to query the knowledge base at right times, instead of hallucinating an answer.\\n\\nIf you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.\\n\\ninput:\\n```python\\n!pip install llama-index\\n```\\noutput:\\n\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['multi_document_agents-v1.ipynb - 5 - 9',\n",
       "   'react_agent_with_query_engine.ipynb - 3 - 1',\n",
       "   'chat_engine_react.ipynb - 0 - 0',\n",
       "   'openai_assistant_query_cookbook.ipynb - 4 - 2',\n",
       "   'openai_agent_query_plan.ipynb - 4 - 8']],\n",
       " 'distances': [[0.14939218759536743,\n",
       "   0.15069550275802612,\n",
       "   0.15483319759368896,\n",
       "   0.15509891510009766,\n",
       "   0.15511804819107056]],\n",
       " 'metadatas': [[{'node': 'multi_document_agents-v1.ipynb - 5',\n",
       "    'type': 'USE_CASE'},\n",
       "   {'node': 'react_agent_with_query_engine.ipynb - 3', 'type': 'INSIGHT'},\n",
       "   {'node': 'chat_engine_react.ipynb - 0', 'type': 'INSIGHT'},\n",
       "   {'node': 'openai_assistant_query_cookbook.ipynb - 4', 'type': 'INSIGHT'},\n",
       "   {'node': 'openai_agent_query_plan.ipynb - 4', 'type': 'USE_CASE'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['how to build a top-level agent that can orchestrate across different document agents to answer user queries',\n",
       "   'the agent can provide analysis based on the given query',\n",
       "   'The agent can decide whether to use the query engine tool and come up with appropriate input',\n",
       "   \"the code snippet shows an example of using the agent's query method to ask about the author's actions after RICS\",\n",
       "   'how to query the OpenAIAgent with a specific question']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerank\n",
    "\n",
    "import nltk\n",
    "\n",
    "# tokinize\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# rank the documents according to the tokenized query\n",
    "def rank_docs(docs, query):\n",
    "    query_tokens = tokenize(query)\n",
    "    scores = []\n",
    "    for doc in docs:\n",
    "        doc_tokens = tokenize(doc)\n",
    "        score = 0\n",
    "        for token in query_tokens:\n",
    "            score += doc_tokens.count(token)\n",
    "        scores.append(score)\n",
    "    reranked_docs = [x for _,x in sorted(zip(scores,docs), reverse=True)]\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n",
      "[1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Simple Fusion Retriever\\nIn this example, we walk through how you can combine retrieval results from multiple queries and multiple indexes.\\nThe retrieved nodes will be returned as the top-k across all queries and indexes, as well as handling de-duplication of any nodes.\\n\\ninput:\\n```python\\nimport os\\nimport openai\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\\n```\\noutput:\\n',\n",
       " 'Create a Hybrid Fusion Retriever\\nIn this step, we fuse our index with a BM25 based retriever. This will enable us to capture both semantic relations and keywords in our input queries.\\nSince both of these retrievers calculate a score, we can use the reciprocal rerank algorithm to re-sort our nodes without using an additional models or excessive computation.\\nThis setup will also query 4 times, once with your original query, and generate 3 more queries.\\nBy default, it uses the following prompt to generate extra queries:\\nQUERY_GEN_PROMPT = (\\n    \"You are a helpful assistant that generates multiple search queries based on a \"\\n    \"single input query. Generate {num_queries} search queries, one on each line, \"\\n    \"related to the following input query:\\\\n\"\\n    \"Query: {query}\\\\n\"\\n    \"Queries:\\\\n\"\\n)\\n\\n\\nFirst, we create our retrievers. Each will retrieve the top-2 most similar nodes:\\n\\ninput:\\n```python\\nfrom llama_index.retrievers import BM25Retriever\\n\\nvector_retriever = index.as_retriever(similarity_top_k=2)\\n\\nbm25_retriever = BM25Retriever.from_defaults(\\n    docstore=index.docstore, similarity_top_k=2\\n)\\n```\\noutput:\\n',\n",
       " 'ids',\n",
       " 'distances',\n",
       " 'metadatas',\n",
       " 'embeddings',\n",
       " 'documents',\n",
       " 'uris',\n",
       " 'data']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"Find the best matching usecase or insight.\n",
    "user query: What is fusion retrieval?\"\"\"\n",
    "\n",
    "docs = []\n",
    "for doc in get_docs(query, vs, G, top_k=5, max_depth=3):\n",
    "    docs.extend(doc)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Create a Hybrid Fusion Retriever\\nIn this step, we fuse our index with a BM25 based retriever. This will enable us to capture both semantic relations and keywords in our input queries.\\nSince both of these retrievers calculate a score, we can use the reciprocal rerank algorithm to re-sort our nodes without using an additional models or excessive computation.\\nThis setup will also query 4 times, once with your original query, and generate 3 more queries.\\nBy default, it uses the following prompt to generate extra queries:\\nQUERY_GEN_PROMPT = (\\n    \"You are a helpful assistant that generates multiple search queries based on a \"\\n    \"single input query. Generate {num_queries} search queries, one on each line, \"\\n    \"related to the following input query:\\\\n\"\\n    \"Query: {query}\\\\n\"\\n    \"Queries:\\\\n\"\\n)\\n\\n\\nFirst, we create our retrievers. Each will retrieve the top-2 most similar nodes:\\n\\ninput:\\n```python\\nfrom llama_index.retrievers import BM25Retriever\\n\\nvector_retriever = index.as_retriever(similarity_top_k=2)\\n\\nbm25_retriever = BM25Retriever.from_defaults(\\n    docstore=index.docstore, similarity_top_k=2\\n)\\n```\\noutput:\\n',\n",
       " 'Simple Fusion Retriever\\nIn this example, we walk through how you can combine retrieval results from multiple queries and multiple indexes.\\nThe retrieved nodes will be returned as the top-k across all queries and indexes, as well as handling de-duplication of any nodes.\\n\\ninput:\\n```python\\nimport os\\nimport openai\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\\n```\\noutput:\\n',\n",
       " 'uris',\n",
       " 'metadatas',\n",
       " 'ids',\n",
       " 'embeddings',\n",
       " 'documents',\n",
       " 'distances',\n",
       " 'data']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_docs(docs, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repomanager.statemanager.llms.completion import get_llm\n",
    "\n",
    "llm = get_llm()\n",
    "llm2 = get_llm(model='gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 3\n",
      "[2, 3, 5]\n",
      "['Define Advanced Retriever We define an advanced retriever that performs the following steps: Query generation/rewriting: generate multiple queries given the original user query Perform retrieval for each query over an ensemble of retrievers. Reranking/fusion: fuse results from all queries, and apply a reranking step to \"fuse\" the top relevant results! Then in the next section we\\'ll plug this into our response synthesis module. Step 1: Query Generation/Rewriting The first step is to generate queries from the original query to better match the query intent, and increase precision/recall of the retrieved results. For instance, we might be able to rewrite the query into smaller queries. We can do this by prompting ChatGPT. input: ```python from llama_index import PromptTemplate ``` output: input: ```python query_str = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\" ``` output: input: ```python query_gen_prompt_str = ( \"You are a helpful assistant that generates multiple search queries based on a \" \"single input query. Generate {num_queries} search queries, one on each line, \" \"related to the following input query:\\\\n\" \"Query: {query}\\\\n\" \"Queries:\\\\n\" ) query_gen_prompt = PromptTemplate(query_gen_prompt_str) ``` output: input: ```python def generate_queries(llm, query_str: str, num_queries: int = 4): fmt_prompt', 'Advanced RAG\\nThe GoogleIndex is built based on GoogleVectorStore and GoogleTextSynthesizer.\\nThese components can be combined with other powerful constructs in LlamaIndex to produce advanced RAG applications.\\nBelow we show a few examples.\\n\\nReranker + Google Retriever\\nConverting content into vectors is a lossy process. LLM-based Reranking\\nremediates this by reranking the retrieved content using LLM, which has higher\\nfidelity because it has access to both the actual query and the passage.\\n\\ninput:\\n```python\\nfrom llama_index.response_synthesizers.google.generativeai import (\\n    GoogleTextSynthesizer,\\n)\\nfrom llama_index.vector_stores.google.generativeai import (\\n    GoogleVectorStore,\\n    google_service_context,\\n)\\nfrom llama_index import ServiceContext, VectorStoreIndex\\nfrom llama_index.llms import PaLM\\nfrom llama_index.postprocessor import LLMRerank\\nfrom llama_index.query_engine import RetrieverQueryEngine\\nfrom llama_index.retrievers import VectorIndexRetriever\\n\\n# Set up the query engine with a reranker.\\nstore = GoogleVectorStore.from_corpus(corpus_id=\"some-corpus-id\")\\nindex = VectorStoreIndex.from_vector_store(\\n    vector_store=store, service_context=google_service_context\\n)\\nresponse_synthesizer = GoogleTextSynthesizer.from_defaults(\\n    temperature=0.7, answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE\\n)\\nreranker = LLMRerank(\\n    top_n=10, service_context=ServiceContext.from_defaults(llm=PaLM())\\n)\\nquery_engine = RetrieverQueryEngine.from_args(\\n    retriever=VectorIndexRetriever(\\n        index=index,\\n        similarity_top_k=20,\\n    ),\\n    response_synthesizer=response_synthesizer,\\n    node_postprocessors=[reranker],\\n)\\n\\n# Query for better result!\\nresponse = query_engine.query(\"What movie should I watch with my family?\")\\n```\\noutput:\\n', 'Multi-Query + Reranker + HyDE + Google Retriever\\nOr combine them all!\\n\\ninput:\\n```python\\n# Google\\'s retriever and AQA model setup.\\nstore = GoogleVectorStore.from_corpus(corpus_id=\"some-corpus-id\")\\nindex = VectorStoreIndex.from_vector_store(\\n    vector_store=store, service_context=google_service_context\\n)\\nresponse_synthesizer = GoogleTextSynthesizer.from_defaults(\\n    temperature=0.7, answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE\\n)\\n\\n# Reranker setup.\\nreranker = LLMRerank(\\n    top_n=10, service_context=ServiceContext.from_defaults(llm=PaLM())\\n)\\nsingle_step_query_engine = index.as_query_engine(\\n    response_synthesizer=response_synthesizer, node_postprocessors=[reranker]\\n)\\n\\n# HyDE setup.\\nhyde = HyDEQueryTransform(include_original=True)\\nhyde_query_engine = TransformQueryEngine(single_step_query_engine, hyde)\\n\\n# Multi-query setup.\\nstep_decompose_transform = StepDecomposeQueryTransform(\\n    llm=PaLM(), verbose=True\\n)\\nquery_engine = MultiStepQueryEngine(\\n    query_engine=hyde_query_engine,\\n    query_transform=step_decompose_transform,\\n    response_synthesizer=response_synthesizer,\\n    index_summary=\"Ask me anything.\",\\n    num_steps=6,\\n)\\n\\n# Query for better result!\\nresponse = query_engine.query(\"What movie should I watch with my family?\")\\n```\\noutput:\\n']\n",
      "docs:  ['Define Advanced Retriever We define an advanced retriever that performs the following steps: Query generation/rewriting: generate multiple queries given the original user query Perform retrieval for each query over an ensemble of retrievers. Reranking/fusion: fuse results from all queries, and apply a reranking step to \"fuse\" the top relevant results! Then in the next section we\\'ll plug this into our response synthesis module. Step 1: Query Generation/Rewriting The first step is to generate queries from the original query to better match the query intent, and increase precision/recall of the retrieved results. For instance, we might be able to rewrite the query into smaller queries. We can do this by prompting ChatGPT. input: ```python from llama_index import PromptTemplate ``` output: input: ```python query_str = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\" ``` output: input: ```python query_gen_prompt_str = ( \"You are a helpful assistant that generates multiple search queries based on a \" \"single input query. Generate {num_queries} search queries, one on each line, \" \"related to the following input query:\\\\n\" \"Query: {query}\\\\n\" \"Queries:\\\\n\" ) query_gen_prompt = PromptTemplate(query_gen_prompt_str) ``` output: input: ```python def generate_queries(llm, query_str: str, num_queries: int = 4): fmt_prompt', 'Advanced RAG\\nThe GoogleIndex is built based on GoogleVectorStore and GoogleTextSynthesizer.\\nThese components can be combined with other powerful constructs in LlamaIndex to produce advanced RAG applications.\\nBelow we show a few examples.\\n\\nReranker + Google Retriever\\nConverting content into vectors is a lossy process. LLM-based Reranking\\nremediates this by reranking the retrieved content using LLM, which has higher\\nfidelity because it has access to both the actual query and the passage.\\n\\ninput:\\n```python\\nfrom llama_index.response_synthesizers.google.generativeai import (\\n    GoogleTextSynthesizer,\\n)\\nfrom llama_index.vector_stores.google.generativeai import (\\n    GoogleVectorStore,\\n    google_service_context,\\n)\\nfrom llama_index import ServiceContext, VectorStoreIndex\\nfrom llama_index.llms import PaLM\\nfrom llama_index.postprocessor import LLMRerank\\nfrom llama_index.query_engine import RetrieverQueryEngine\\nfrom llama_index.retrievers import VectorIndexRetriever\\n\\n# Set up the query engine with a reranker.\\nstore = GoogleVectorStore.from_corpus(corpus_id=\"some-corpus-id\")\\nindex = VectorStoreIndex.from_vector_store(\\n    vector_store=store, service_context=google_service_context\\n)\\nresponse_synthesizer = GoogleTextSynthesizer.from_defaults(\\n    temperature=0.7, answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE\\n)\\nreranker = LLMRerank(\\n    top_n=10, service_context=ServiceContext.from_defaults(llm=PaLM())\\n)\\nquery_engine = RetrieverQueryEngine.from_args(\\n    retriever=VectorIndexRetriever(\\n        index=index,\\n        similarity_top_k=20,\\n    ),\\n    response_synthesizer=response_synthesizer,\\n    node_postprocessors=[reranker],\\n)\\n\\n# Query for better result!\\nresponse = query_engine.query(\"What movie should I watch with my family?\")\\n```\\noutput:\\n']\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'For the user query, use the information attached to answer it in short form.\\nProvide simple examples (if possible):\\nSource 0\\nDefine Advanced Retriever We define an advanced retriever that performs the following steps: Query generation/rewriting: generate multiple queries given the original user query Perform retrieval for each query over an ensemble of retrievers. Reranking/fusion: fuse results from all queries, and apply a reranking step to \"fuse\" the top relevant results! Then in the next section we\\'ll plug this into our response synthesis module. Step 1: Query Generation/Rewriting The first step is to generate queries from the original query to better match the query intent, and increase precision/recall of the retrieved results. For instance, we might be able to rewrite the query into smaller queries. We can do this by prompting ChatGPT. input: ```python from llama_index import PromptTemplate ``` output: input: ```python query_str = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\" ``` output: input: ```python query_gen_prompt_str = ( \"You are a helpful assistant that generates multiple search queries based on a \" \"single input query. Generate {num_queries} search queries, one on each line, \" \"related to the following input query:\\\\n\" \"Query: {query}\\\\n\" \"Queries:\\\\n\" ) query_gen_prompt = PromptTemplate(query_gen_prompt_str) ``` output: input: ```python def generate_queries(llm, query_str: str, num_queries: int = 4): fmt_prompt\\nSource 1\\nAdvanced RAG\\nThe GoogleIndex is built based on GoogleVectorStore and GoogleTextSynthesizer.\\nThese components can be combined with other powerful constructs in LlamaIndex to produce advanced RAG applications.\\nBelow we show a few examples.\\n\\nReranker + Google Retriever\\nConverting content into vectors is a lossy process. LLM-based Reranking\\nremediates this by reranking the retrieved content using LLM, which has higher\\nfidelity because it has access to both the actual query and the passage.\\n\\ninput:\\n```python\\nfrom llama_index.response_synthesizers.google.generativeai import (\\n    GoogleTextSynthesizer,\\n)\\nfrom llama_index.vector_stores.google.generativeai import (\\n    GoogleVectorStore,\\n    google_service_context,\\n)\\nfrom llama_index import ServiceContext, VectorStoreIndex\\nfrom llama_index.llms import PaLM\\nfrom llama_index.postprocessor import LLMRerank\\nfrom llama_index.query_engine import RetrieverQueryEngine\\nfrom llama_index.retrievers import VectorIndexRetriever\\n\\n# Set up the query engine with a reranker.\\nstore = GoogleVectorStore.from_corpus(corpus_id=\"some-corpus-id\")\\nindex = VectorStoreIndex.from_vector_store(\\n    vector_store=store, service_context=google_service_context\\n)\\nresponse_synthesizer = GoogleTextSynthesizer.from_defaults(\\n    temperature=0.7, answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE\\n)\\nreranker = LLMRerank(\\n    top_n=10, service_context=ServiceContext.from_defaults(llm=PaLM())\\n)\\nquery_engine = RetrieverQueryEngine.from_args(\\n    retriever=VectorIndexRetriever(\\n        index=index,\\n        similarity_top_k=20,\\n    ),\\n    response_synthesizer=response_synthesizer,\\n    node_postprocessors=[reranker],\\n)\\n\\n# Query for better result!\\nresponse = query_engine.query(\"What movie should I watch with my family?\")\\n```\\noutput:\\n\\nQuery:\\nWhat are fusion techniques?\\nAnswer:\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What are fusion techniques?'\n",
    "_query = \"\"\"For the user query, match the best use case or insight from a code base.\n",
    "user query: {}\"\"\".format(query)\n",
    "# docs = []\n",
    "# for doc in get_docs(_query, vs, G, top_k=5, max_depth=3)[0]:\n",
    "#     docs.extend(doc)\n",
    "# reranked_docs = rank_docs(docs, _query)\n",
    "# top_three = reranked_docs[:3]\n",
    "docs = get_docs(_query, vs, G, to_return=2)[0]\n",
    "print(\"docs: \", docs)\n",
    "print(len(docs))\n",
    "prompt = \"\"\"For the user query, use the information attached to answer it in short form.\n",
    "Provide simple examples (if possible):\\n\"\"\"\n",
    "i = 0\n",
    "for doc in docs:\n",
    "    prompt += \"Source {}\\n\".format(i)\n",
    "    prompt += doc + \"\\n\"\n",
    "    i += 1\n",
    "prompt += \"Query:\\n\"\n",
    "prompt += query + \"\\n\"\n",
    "prompt += \"Answer:\\n\"\n",
    "# print the number of words in the prompt\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "['Define Advanced Retriever We define an advanced retriever that performs the following steps: Query generation/rewriting: generate multiple queries given the original user query Perform retrieval for each query over an ensemble of retrievers. Reranking/fusion: fuse results from all queries, and apply a reranking step to \"fuse\" the top relevant results! Then in the next section we\\'ll plug this into our response synthesis module. Step 1: Query Generation/Rewriting The first step is to generate queries from the original query to better match the query intent, and increase precision/recall of the retrieved results. For instance, we might be able to rewrite the query into smaller queries. We can do this by prompting ChatGPT. input: ```python from llama_index import PromptTemplate ``` output: input: ```python query_str = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\" ``` output: input: ```python query_gen_prompt_str = ( \"You are a helpful assistant that generates multiple search queries based on a \" \"single input query. Generate {num_queries} search queries, one on each line, \" \"related to the following input query:\\\\n\" \"Query: {query}\\\\n\" \"Queries:\\\\n\" ) query_gen_prompt = PromptTemplate(query_gen_prompt_str) ``` output: input: ```python def generate_queries(llm, query_str: str, num_queries: int = 4): fmt_prompt', 'Advanced RAG\\nThe GoogleIndex is built based on GoogleVectorStore and GoogleTextSynthesizer.\\nThese components can be combined with other powerful constructs in LlamaIndex to produce advanced RAG applications.\\nBelow we show a few examples.\\n\\nReranker + Google Retriever\\nConverting content into vectors is a lossy process. LLM-based Reranking\\nremediates this by reranking the retrieved content using LLM, which has higher\\nfidelity because it has access to both the actual query and the passage.\\n\\ninput:\\n```python\\nfrom llama_index.response_synthesizers.google.generativeai import (\\n    GoogleTextSynthesizer,\\n)\\nfrom llama_index.vector_stores.google.generativeai import (\\n    GoogleVectorStore,\\n    google_service_context,\\n)\\nfrom llama_index import ServiceContext, VectorStoreIndex\\nfrom llama_index.llms import PaLM\\nfrom llama_index.postprocessor import LLMRerank\\nfrom llama_index.query_engine import RetrieverQueryEngine\\nfrom llama_index.retrievers import VectorIndexRetriever\\n\\n# Set up the query engine with a reranker.\\nstore = GoogleVectorStore.from_corpus(corpus_id=\"some-corpus-id\")\\nindex = VectorStoreIndex.from_vector_store(\\n    vector_store=store, service_context=google_service_context\\n)\\nresponse_synthesizer = GoogleTextSynthesizer.from_defaults(\\n    temperature=0.7, answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE\\n)\\nreranker = LLMRerank(\\n    top_n=10, service_context=ServiceContext.from_defaults(llm=PaLM())\\n)\\nquery_engine = RetrieverQueryEngine.from_args(\\n    retriever=VectorIndexRetriever(\\n        index=index,\\n        similarity_top_k=20,\\n    ),\\n    response_synthesizer=response_synthesizer,\\n    node_postprocessors=[reranker],\\n)\\n\\n# Query for better result!\\nresponse = query_engine.query(\"What movie should I watch with my family?\")\\n```\\noutput:\\n']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f'{docs}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Fusion techniques are methods used to combine or merge results from multiple queries or retrievers to generate a final set of relevant results. This can be done through various approaches such as ranking, weighting, or combining the results based on their relevance scores. The goal is to improve the overall quality and accuracy of the retrieved information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import display and markdown\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(llm.complete(prompt).text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Fusion techniques refer to the methods used to combine or \"fuse\" results from multiple queries in an advanced retriever. After generating multiple queries from the original user query and retrieving results for each, the advanced retriever uses fusion techniques to combine these results. This is followed by a reranking step to prioritize the most relevant results. For example, if the original query is \"What movie should I watch with my family?\", the retriever might generate related queries like \"Family-friendly movies\", \"Popular movies for families\", etc., retrieve results for each, and then fuse and rerank these results to provide the best answer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(llm2.complete(prompt).text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root': 2.535797987573841e-05,\n",
       " 'LICENSE': 2.6558362187526002e-05,\n",
       " 'CHANGELOG.md': 2.6558362187526002e-05,\n",
       " '.pre-commit-config.yaml': 2.6558362187526002e-05,\n",
       " 'Makefile': 2.8612681822994494e-05,\n",
       " 'CITATION.cff': 2.6558362187526002e-05,\n",
       " 'pyproject.toml': 2.6558362187526002e-05,\n",
       " 'README.md': 9.657432073854376e-05,\n",
       " 'CONTRIBUTING.md': 2.6558362187526002e-05,\n",
       " 'poetry.lock': 2.6558362187526002e-05,\n",
       " '.readthedocs.yaml': 2.6558362187526002e-05,\n",
       " 'experimental': 2.7223581194882335e-05,\n",
       " 'splitter_playground': 2.8673215955606596e-05,\n",
       " 'app.py': 4.9838092699503767e-05,\n",
       " 'cli': 2.8673215955606596e-05,\n",
       " 'configuration.py': 3.858561624856383e-05,\n",
       " 'cli_add.py': 3.0910707139595646e-05,\n",
       " '__init__.py': 0.0006499568398591388,\n",
       " 'cli_init.py': 3.0910707139595646e-05,\n",
       " 'cli_query.py': 3.0910707139595646e-05,\n",
       " '__main__.py': 2.885513885056203e-05,\n",
       " 'openai_fine_tuning': 2.8673215955606596e-05,\n",
       " 'openai_fine_tuning.ipynb': 3.586639057649473e-05,\n",
       " 'launch_training.py': 3.351801748366019e-05,\n",
       " 'validate_json.py': 3.8600368126162755e-05,\n",
       " 'classifier': 2.8673215955606596e-05,\n",
       " 'TitanicModel.ipynb': 3.351801748366019e-05,\n",
       " 'utils.py': 0.00014988520612378177,\n",
       " 'data': 0.00012977482168272304,\n",
       " 'train.csv': 3.2778176795107434e-05,\n",
       " 'llama_index': 0.0009357380766111104,\n",
       " 'service_context.py': 3.96330034275986e-05,\n",
       " 'async_utils.py': 3.86617125467603e-05,\n",
       " 'exec_utils.py': 3.86617125467603e-05,\n",
       " 'constants.py': 3.86617125467603e-05,\n",
       " 'types.py': 9.124868213253399e-05,\n",
       " 'VERSION': 3.86617125467603e-05,\n",
       " 'py.typed': 3.86617125467603e-05,\n",
       " 'schema.py': 5.8252856984362784e-05,\n",
       " 'img_utils.py': 3.86617125467603e-05,\n",
       " 'vector_stores': 4.683990408780988e-05,\n",
       " 'cassandra.py': 2.565865235938723e-05,\n",
       " 'elasticsearch.py': 2.8628025728518726e-05,\n",
       " 'milvus.py': 2.7359546833052083e-05,\n",
       " 'loading.py': 4.5504948975681114e-05,\n",
       " 'azurecosmosmongo.py': 2.565865235938723e-05,\n",
       " 'dynamodb.py': 2.565865235938723e-05,\n",
       " 'myscale.py': 2.6412205008617335e-05,\n",
       " 'tencentvectordb.py': 2.565865235938723e-05,\n",
       " 'pinecone.py': 2.701041929418009e-05,\n",
       " 'opensearch.py': 2.565865235938723e-05,\n",
       " 'awadb.py': 2.741591224177029e-05,\n",
       " 'supabase.py': 2.565865235938723e-05,\n",
       " 'rocksetdb.py': 2.565865235938723e-05,\n",
       " 'bagel.py': 2.7103513282162543e-05,\n",
       " 'qdrant.py': 2.6412205008617335e-05,\n",
       " 'registry.py': 4.1935108431177175e-05,\n",
       " 'cogsearch.py': 2.565865235938723e-05,\n",
       " 'weaviate_utils.py': 2.565865235938723e-05,\n",
       " 'mongodb.py': 2.565865235938723e-05,\n",
       " 'pgvecto_rs.py': 2.565865235938723e-05,\n",
       " 'zep.py': 2.565865235938723e-05,\n",
       " 'epsilla.py': 2.565865235938723e-05,\n",
       " 'faiss.py': 3.1018591051469385e-05,\n",
       " 'timescalevector.py': 2.565865235938723e-05,\n",
       " 'dashvector.py': 2.741591224177029e-05,\n",
       " 'redis.py': 2.863784786687862e-05,\n",
       " 'weaviate.py': 3.0471604679458753e-05,\n",
       " 'lancedb.py': 2.6689738293711165e-05,\n",
       " 'postgres.py': 2.565865235938723e-05,\n",
       " 'deeplake.py': 2.85790474514367e-05,\n",
       " 'qdrant_utils.py': 2.565865235938723e-05,\n",
       " 'neo4jvector.py': 2.565865235938723e-05,\n",
       " 'metal.py': 2.6412205008617335e-05,\n",
       " 'chroma.py': 2.6412205008617335e-05,\n",
       " 'tair.py': 2.565865235938723e-05,\n",
       " 'lantern.py': 2.565865235938723e-05,\n",
       " 'simple.py': 3.185533794293468e-05,\n",
       " 'chatgpt_plugin.py': 2.565865235938723e-05,\n",
       " 'singlestoredb.py': 2.6797540209724712e-05,\n",
       " 'astra.py': 2.565865235938723e-05,\n",
       " 'typesense.py': 2.64924360184994e-05,\n",
       " 'google': 4.292967425575106e-05,\n",
       " 'generativeai': 6.130775767209199e-05,\n",
       " 'genai_extension.py': 4.300530328946892e-05,\n",
       " 'base.py': 0.000328796493167291,\n",
       " 'docarray': 3.097161088708195e-05,\n",
       " 'hnsw.py': 3.193280299530685e-05,\n",
       " 'in_memory.py': 3.193280299530685e-05,\n",
       " 'token_counter': 3.86617125467603e-05,\n",
       " 'mock_embed_model.py': 3.666386789223265e-05,\n",
       " 'retrievers': 4.7310056003531476e-05,\n",
       " 'recursive_retriever.py': 2.683355812513221e-05,\n",
       " 'bm25_retriever.py': 2.683355812513221e-05,\n",
       " 'fusion_retriever.py': 2.683355812513221e-05,\n",
       " 'you_retriever.py': 2.683355812513221e-05,\n",
       " 'transform_retriever.py': 2.683355812513221e-05,\n",
       " 'auto_merging_retriever.py': 2.683355812513221e-05,\n",
       " 'router_retriever.py': 2.683355812513221e-05,\n",
       " 'ingestion': 4.017347401849311e-05,\n",
       " 'cache.py': 2.8880539514990203e-05,\n",
       " 'pipeline.py': 2.8880539514990203e-05,\n",
       " 'indices': 5.5579829620058494e-05,\n",
       " 'postprocessor.py': 2.632927075657671e-05,\n",
       " 'managed.tar.gz': 2.632927075657671e-05,\n",
       " 'base_retriever.py': 3.648868150497472e-05,\n",
       " 'prompt_helper.py': 2.632927075657671e-05,\n",
       " 'tree': 2.7614932103470912e-05,\n",
       " 'tree_root_retriever.py': 2.7152645391267427e-05,\n",
       " 'select_leaf_retriever.py': 2.7152645391267427e-05,\n",
       " 'select_leaf_embedding_retriever.py': 2.7152645391267427e-05,\n",
       " 'inserter.py': 2.7152645391267427e-05,\n",
       " 'all_leaf_retriever.py': 2.7152645391267427e-05,\n",
       " 'struct_store': 3.6928133535677114e-05,\n",
       " 'pandas.py': 5.03210741204466e-05,\n",
       " 'sql_query.py': 2.7741514152240674e-05,\n",
       " 'json_query.py': 2.7741514152240674e-05,\n",
       " 'container_builder.py': 2.7741514152240674e-05,\n",
       " 'sql_retriever.py': 2.7741514152240674e-05,\n",
       " 'sql.py': 2.7741514152240674e-05,\n",
       " 'managed': 2.6994489763933046e-05,\n",
       " 'vectara': 2.7112986009866407e-05,\n",
       " 'query.py': 3.113335333082793e-05,\n",
       " 'retriever.py': 4.9234796701638645e-05,\n",
       " 'colbert_index': 2.7112986009866407e-05,\n",
       " 'zilliz': 2.7112986009866407e-05,\n",
       " 'knowledge_graph': 3.575676668433914e-05,\n",
       " 'retrievers.py': 8.532280827904133e-05,\n",
       " 'empty': 3.5215239114736184e-05,\n",
       " 'keyword_table': 2.632927075657671e-05,\n",
       " 'simple_base.py': 2.7583270397311495e-05,\n",
       " 'rake_base.py': 2.7583270397311495e-05,\n",
       " 'vector_store': 3.740889476834224e-05,\n",
       " 'auto_retriever': 2.7690383294520923e-05,\n",
       " 'output_parser.py': 4.048037390224586e-05,\n",
       " 'prompts.py': 5.413438330171884e-05,\n",
       " 'auto_retriever.py': 2.996918903345466e-05,\n",
       " 'composability': 4.0866783466932055e-05,\n",
       " 'graph.py': 2.8917240504218257e-05,\n",
       " 'common': 3.78505239629959e-05,\n",
       " 'common_tree': 2.632927075657671e-05,\n",
       " 'list': 0.0003143265408686665,\n",
       " 'multi_modal': 2.9358266471343444e-05,\n",
       " 'document_summary': 2.632927075657671e-05,\n",
       " 'query': 0.00030564916637516106,\n",
       " 'embedding_utils.py': 3.248427915576454e-05,\n",
       " 'query_transform': 3.248427915576454e-05,\n",
       " 'feedback_transform.py': 2.997856071034807e-05,\n",
       " 'tools': 4.944452322845471e-05,\n",
       " 'query_engine.py': 2.7100903248469673e-05,\n",
       " 'query_plan.py': 2.7100903248469673e-05,\n",
       " 'download.py': 4.26083277719356e-05,\n",
       " 'ondemand_loader_tool.py': 2.7100903248469673e-05,\n",
       " 'function_tool.py': 2.7100903248469673e-05,\n",
       " 'retriever_tool.py': 2.7100903248469673e-05,\n",
       " 'tool_spec': 2.7100903248469673e-05,\n",
       " 'notion': 2.9122073163351662e-05,\n",
       " 'slack': 2.9122073163351662e-05,\n",
       " 'load_and_search': 2.9122073163351662e-05,\n",
       " 'embeddings': 9.17400948920954e-05,\n",
       " 'cohereai.py': 2.6471683059350805e-05,\n",
       " 'multi_modal_base.py': 2.6471683059350805e-05,\n",
       " 'google_palm.py': 2.6471683059350805e-05,\n",
       " 'adapter.py': 2.6471683059350805e-05,\n",
       " 'gradient.py': 2.695850198246853e-05,\n",
       " 'pooling.py': 2.6471683059350805e-05,\n",
       " 'voyageai.py': 2.8901447003969253e-05,\n",
       " 'google.py': 2.9416060865262863e-05,\n",
       " 'gemini.py': 3.1291429715541635e-05,\n",
       " 'instructor.py': 2.6471683059350805e-05,\n",
       " 'adapter_utils.py': 2.6471683059350805e-05,\n",
       " 'huggingface_optimum.py': 2.6471683059350805e-05,\n",
       " 'jinaai.py': 2.6471683059350805e-05,\n",
       " 'openai.py': 4.0559841706928625e-05,\n",
       " 'azure_openai.py': 2.695850198246853e-05,\n",
       " 'huggingface_utils.py': 2.6471683059350805e-05,\n",
       " 'huggingface.py': 2.695850198246853e-05,\n",
       " 'ollama_embedding.py': 2.6471683059350805e-05,\n",
       " 'llm_rails.py': 2.6471683059350805e-05,\n",
       " 'langchain.py': 6.994804387339805e-05,\n",
       " 'mistralai.py': 2.6471683059350805e-05,\n",
       " 'clarifai.py': 2.695850198246853e-05,\n",
       " 'clip.py': 2.7938527864625497e-05,\n",
       " 'text_embeddings_inference.py': 2.6471683059350805e-05,\n",
       " 'bedrock.py': 2.695850198246853e-05,\n",
       " 'fastembed.py': 2.924784698476064e-05,\n",
       " 'node_parser': 6.786261037069115e-05,\n",
       " 'node_utils.py': 2.9251757942216366e-05,\n",
       " 'interface.py': 3.603529075211291e-05,\n",
       " 'relational': 2.9251757942216366e-05,\n",
       " 'markdown_element.py': 3.0202593941676538e-05,\n",
       " 'unstructured_element.py': 3.0202593941676538e-05,\n",
       " 'hierarchical.py': 3.0202593941676538e-05,\n",
       " 'base_element.py': 3.0202593941676538e-05,\n",
       " 'file': 3.000531059144647e-05,\n",
       " 'simple_file.py': 2.6655183186805743e-05,\n",
       " 'html.py': 2.6655183186805743e-05,\n",
       " 'markdown.py': 2.6655183186805743e-05,\n",
       " 'json.py': 0.00011248073014218226,\n",
       " 'text': 0.0003688186728144326,\n",
       " 'sentence_window.py': 5.3462974331195244e-05,\n",
       " 'token.py': 4.956919626471729e-05,\n",
       " 'code.py': 4.956919626471729e-05,\n",
       " 'sentence.py': 4.956919626471729e-05,\n",
       " 'core': 0.00012117224710656886,\n",
       " 'image_retriever.py': 3.5517390624136416e-05,\n",
       " 'base_multi_modal_retriever.py': 3.5517390624136416e-05,\n",
       " 'base_query_engine.py': 3.5517390624136416e-05,\n",
       " 'logger': 3.9508255011136775e-05,\n",
       " 'response': 0.00012130140792322407,\n",
       " 'pprint_utils.py': 2.8781222330304924e-05,\n",
       " 'notebook_utils.py': 3.033666039125548e-05,\n",
       " 'memory': 0.00011317339980183264,\n",
       " 'chat_memory_buffer.py': 3.520095965345064e-05,\n",
       " 'llm_predictor': 4.645077715710218e-05,\n",
       " 'structured.py': 3.120159997885927e-05,\n",
       " 'mock.py': 3.168841890197699e-05,\n",
       " 'vellum': 3.5364481011216605e-05,\n",
       " 'predictor.py': 2.7729149287460993e-05,\n",
       " 'prompt_registry.py': 2.7729149287460993e-05,\n",
       " 'exceptions.py': 2.7729149287460993e-05,\n",
       " 'playground': 3.9508255011136775e-05,\n",
       " 'callbacks': 7.470850589357187e-05,\n",
       " 'token_counting.py': 3.7186443505120066e-05,\n",
       " 'wandb_callback.py': 2.8520588038973846e-05,\n",
       " 'simple_llm_handler.py': 2.757889092649795e-05,\n",
       " 'aim.py': 3.045464031132542e-05,\n",
       " 'open_inference_callback.py': 2.8520588038973846e-05,\n",
       " 'promptlayer_handler.py': 2.757889092649795e-05,\n",
       " 'finetuning_handler.py': 2.8520588038973846e-05,\n",
       " 'global_handlers.py': 2.757889092649795e-05,\n",
       " 'base_handler.py': 2.757889092649795e-05,\n",
       " 'arize_phoenix_callback.py': 2.757889092649795e-05,\n",
       " 'honeyhive_callback.py': 2.757889092649795e-05,\n",
       " 'llama_debug.py': 2.8520588038973846e-05,\n",
       " 'bridge': 3.86617125467603e-05,\n",
       " 'pydantic.py': 4.071290653438698e-05,\n",
       " 'postprocessor': 3.9508255011136775e-05,\n",
       " 'flag_embedding_reranker.py': 2.7397004691302223e-05,\n",
       " 'cohere_rerank.py': 2.7397004691302223e-05,\n",
       " 'node_recency.py': 2.7397004691302223e-05,\n",
       " 'optimizer.py': 2.7397004691302223e-05,\n",
       " 'node.py': 2.7397004691302223e-05,\n",
       " 'sbert_rerank.py': 2.7397004691302223e-05,\n",
       " 'metadata_replacement.py': 2.7397004691302223e-05,\n",
       " 'longllmlingua.py': 2.7397004691302223e-05,\n",
       " 'pii.py': 2.7397004691302223e-05,\n",
       " 'llm_rerank.py': 2.7397004691302223e-05,\n",
       " 'graph_stores': 3.86617125467603e-05,\n",
       " 'kuzu.py': 3.25489148337854e-05,\n",
       " 'falkordb.py': 2.9597687881923746e-05,\n",
       " 'neo4j.py': 3.326337648103523e-05,\n",
       " 'nebulagraph.py': 2.9597687881923746e-05,\n",
       " 'multi_modal_llms': 3.9508255011136775e-05,\n",
       " 'replicate_multi_modal.py': 2.9690907608811514e-05,\n",
       " 'generic_utils.py': 3.0177726531929236e-05,\n",
       " 'openai_utils.py': 3.0177726531929236e-05,\n",
       " 'objects': 4.173334611483344e-05,\n",
       " 'base_node_mapping.py': 2.9934110863951793e-05,\n",
       " 'tool_node_mapping.py': 2.9934110863951793e-05,\n",
       " 'table_node_mapping.py': 2.9934110863951793e-05,\n",
       " 'agent': 4.355242601956113e-05,\n",
       " 'openai_assistant_agent.py': 2.6416720198521096e-05,\n",
       " 'runner': 2.6416720198521096e-05,\n",
       " 'parallel.py': 3.091668996767763e-05,\n",
       " 'legacy': 2.6416720198521096e-05,\n",
       " 'context_retriever_agent.py': 2.9804947949289785e-05,\n",
       " 'openai_agent.py': 2.9804947949289785e-05,\n",
       " 'retriever_openai_agent.py': 2.9804947949289785e-05,\n",
       " 'react': 3.086368827207248e-05,\n",
       " 'formatter.py': 2.796566518232308e-05,\n",
       " 'step.py': 3.304801582482564e-05,\n",
       " 'agent.py': 2.796566518232308e-05,\n",
       " 'openai': 4.284692852250908e-05,\n",
       " 'param_tuner': 3.9508255011136775e-05,\n",
       " 'finetuning': 4.017347401849311e-05,\n",
       " 'gradient': 2.8371571975929273e-05,\n",
       " 'sentence_transformer.py': 2.6471683059350805e-05,\n",
       " 'common.py': 2.6471683059350805e-05,\n",
       " 'rerankers': 2.770635296857294e-05,\n",
       " 'cohere_reranker.py': 3.1090761707927854e-05,\n",
       " 'dataset_gen.py': 3.8734470817513786e-05,\n",
       " 'cross_encoders': 2.770635296857294e-05,\n",
       " 'cross_encoder.py': 3.3001688985324344e-05,\n",
       " 'langchain_helpers': 3.9508255011136775e-05,\n",
       " 'text_splitter.py': 3.2290664248655386e-05,\n",
       " 'streaming.py': 3.2290664248655386e-05,\n",
       " 'memory_wrapper.py': 3.2290664248655386e-05,\n",
       " 'agents': 6.846284593217396e-05,\n",
       " 'toolkits.py': 2.9768438153595342e-05,\n",
       " 'tools.py': 2.9768438153595342e-05,\n",
       " 'agents.py': 2.9768438153595342e-05,\n",
       " 'storage': 4.155037369693006e-05,\n",
       " 'storage_context.py': 2.815539984496259e-05,\n",
       " 'kvstore': 2.815539984496259e-05,\n",
       " 's3_kvstore.py': 2.6906987558101955e-05,\n",
       " 'firestore_kvstore.py': 2.6906987558101955e-05,\n",
       " 'mongodb_kvstore.py': 2.6906987558101955e-05,\n",
       " 'dynamodb_kvstore.py': 2.6906987558101955e-05,\n",
       " 'redis_kvstore.py': 2.6906987558101955e-05,\n",
       " 'simple_kvstore.py': 2.6906987558101955e-05,\n",
       " 'docstore': 0.00014511194766606963,\n",
       " 'firestore_docstore.py': 3.077049305225759e-05,\n",
       " 'mongo_docstore.py': 3.077049305225759e-05,\n",
       " 'redis_docstore.py': 3.077049305225759e-05,\n",
       " 'simple_docstore.py': 3.077049305225759e-05,\n",
       " 'keyval_docstore.py': 3.077049305225759e-05,\n",
       " 'dynamodb_docstore.py': 3.077049305225759e-05,\n",
       " 'index_store': 3.383603713653674e-05,\n",
       " 'firestore_indexstore.py': 2.7698520340691668e-05,\n",
       " 'simple_index_store.py': 2.7698520340691668e-05,\n",
       " 'keyval_index_store.py': 2.7698520340691668e-05,\n",
       " 'mongo_index_store.py': 2.7698520340691668e-05,\n",
       " 'redis_index_store.py': 2.7698520340691668e-05,\n",
       " 'dynamodb_index_store.py': 2.7698520340691668e-05,\n",
       " 'utilities': 3.9508255011136775e-05,\n",
       " 'sql_wrapper.py': 3.402383534188462e-05,\n",
       " 'joint_qa_summary.py': 2.8917240504218257e-05,\n",
       " 'output_parsers': 4.308516581699325e-05,\n",
       " 'guardrails.py': 2.842007782493368e-05,\n",
       " 'selection.py': 2.842007782493368e-05,\n",
       " 'data_structs': 3.86617125467603e-05,\n",
       " 'struct_type.py': 3.101092388398553e-05,\n",
       " 'data_structs.py': 3.101092388398553e-05,\n",
       " 'document_summary.py': 3.101092388398553e-05,\n",
       " 'table.py': 3.101092388398553e-05,\n",
       " 'download': 4.1991065520276845e-05,\n",
       " 'dataset.py': 2.6718419135251748e-05,\n",
       " 'module.py': 2.6718419135251748e-05,\n",
       " 'tts': 3.9326931554116644e-05,\n",
       " 'bark.py': 3.416053351618594e-05,\n",
       " 'elevenlabs.py': 3.515497985981064e-05,\n",
       " 'program': 5.9522597332613274e-05,\n",
       " 'openai_program.py': 2.9044388352949915e-05,\n",
       " 'llm_program.py': 2.9044388352949915e-05,\n",
       " 'multi_modal_llm_program.py': 2.9044388352949915e-05,\n",
       " 'lmformatenforcer_program.py': 2.9044388352949915e-05,\n",
       " 'guidance_program.py': 2.9044388352949915e-05,\n",
       " 'llm_prompt_program.py': 2.9044388352949915e-05,\n",
       " 'predefined': 2.9044388352949915e-05,\n",
       " 'df.py': 3.3344780847439914e-05,\n",
       " 'evaporate': 3.3344780847439914e-05,\n",
       " 'extractor.py': 3.243037946774102e-05,\n",
       " 'prompts': 7.151449070837776e-05,\n",
       " 'prompt_type.py': 2.7423238491274047e-05,\n",
       " 'system.py': 2.7423238491274047e-05,\n",
       " 'mixin.py': 2.7423238491274047e-05,\n",
       " 'display_utils.py': 2.7423238491274047e-05,\n",
       " 'chat_prompts.py': 2.7423238491274047e-05,\n",
       " 'default_prompt_selectors.py': 2.7423238491274047e-05,\n",
       " 'prompt_utils.py': 2.7423238491274047e-05,\n",
       " 'guidance_utils.py': 2.7423238491274047e-05,\n",
       " 'lmformatenforcer_utils.py': 2.7423238491274047e-05,\n",
       " 'default_prompts.py': 2.7423238491274047e-05,\n",
       " 'response_synthesizers': 4.449867480583627e-05,\n",
       " 'simple_summarize.py': 2.705578377058471e-05,\n",
       " 'compact_and_accumulate.py': 2.705578377058471e-05,\n",
       " 'generation.py': 2.705578377058471e-05,\n",
       " 'accumulate.py': 2.705578377058471e-05,\n",
       " 'refine.py': 2.705578377058471e-05,\n",
       " 'no_text.py': 2.705578377058471e-05,\n",
       " 'type.py': 2.705578377058471e-05,\n",
       " 'factory.py': 2.705578377058471e-05,\n",
       " 'compact_and_refine.py': 2.705578377058471e-05,\n",
       " 'tree_summarize.py': 2.705578377058471e-05,\n",
       " 'evaluation': 7.415157539763509e-05,\n",
       " 'batch_runner.py': 2.6913417936688957e-05,\n",
       " 'faithfulness.py': 2.813828717786715e-05,\n",
       " 'correctness.py': 2.6913417936688957e-05,\n",
       " 'semantic_similarity.py': 2.6913417936688957e-05,\n",
       " 'dataset_generation.py': 2.6913417936688957e-05,\n",
       " 'pairwise.py': 2.6913417936688957e-05,\n",
       " 'guideline.py': 2.6913417936688957e-05,\n",
       " 'eval_utils.py': 2.8027121120301357e-05,\n",
       " 'relevancy.py': 2.813828717786715e-05,\n",
       " 'retrieval': 2.6913417936688957e-05,\n",
       " 'metrics.py': 2.858870381695392e-05,\n",
       " 'evaluator.py': 2.858870381695392e-05,\n",
       " 'metrics_base.py': 2.858870381695392e-05,\n",
       " 'benchmarks': 2.811380024847655e-05,\n",
       " 'hotpotqa.py': 2.8736931876806427e-05,\n",
       " 'beir.py': 3.008832489914554e-05,\n",
       " 'llama_pack': 3.86617125467603e-05,\n",
       " 'chat_engine': 4.017347401849311e-05,\n",
       " 'condense_plus_context.py': 2.7314957453100516e-05,\n",
       " 'condense_question.py': 2.7314957453100516e-05,\n",
       " 'context.py': 2.7314957453100516e-05,\n",
       " 'readers': 3.9508255011136775e-05,\n",
       " 'string_iterable.py': 2.6111532524968513e-05,\n",
       " 'youtube_transcript.py': 2.6111532524968513e-05,\n",
       " 'web.py': 2.6111532524968513e-05,\n",
       " 'mongo.py': 2.6111532524968513e-05,\n",
       " 'database.py': 2.6111532524968513e-05,\n",
       " 'mbox.py': 2.6111532524968513e-05,\n",
       " 'discord_reader.py': 2.6111532524968513e-05,\n",
       " 'twitter.py': 2.6111532524968513e-05,\n",
       " 'obsidian.py': 2.6111532524968513e-05,\n",
       " 'notion.py': 2.6111532524968513e-05,\n",
       " 'slack.py': 2.6111532524968513e-05,\n",
       " 'wikipedia.py': 2.9201212086605013e-05,\n",
       " 'psychic.py': 2.6111532524968513e-05,\n",
       " 'html_reader.py': 2.6655183186805743e-05,\n",
       " 'markdown_reader.py': 2.6655183186805743e-05,\n",
       " 'image_caption_reader.py': 2.6655183186805743e-05,\n",
       " 'ipynb_reader.py': 2.6655183186805743e-05,\n",
       " 'image_vision_llm_reader.py': 2.6655183186805743e-05,\n",
       " 'docs_reader.py': 2.6655183186805743e-05,\n",
       " 'slides_reader.py': 2.6655183186805743e-05,\n",
       " 'tabular_reader.py': 2.6655183186805743e-05,\n",
       " 'image_reader.py': 2.6655183186805743e-05,\n",
       " 'flat_reader.py': 2.6655183186805743e-05,\n",
       " 'mbox_reader.py': 2.6655183186805743e-05,\n",
       " 'epub_reader.py': 2.6655183186805743e-05,\n",
       " 'video_audio_reader.py': 2.6655183186805743e-05,\n",
       " 'redis': 3.3822949449754984e-05,\n",
       " 'make_com': 2.6111532524968513e-05,\n",
       " 'wrapper.py': 3.6373316972121075e-05,\n",
       " 'schema': 0.00017276978292220677,\n",
       " 'google_readers': 2.6111532524968513e-05,\n",
       " 'gsheets.py': 3.2701537939993525e-05,\n",
       " 'gdocs.py': 3.2701537939993525e-05,\n",
       " 'chatgpt_plugin': 2.677675153232485e-05,\n",
       " 'weaviate': 2.8774360429277545e-05,\n",
       " 'reader.py': 3.755151024812168e-05,\n",
       " 'github_readers': 2.6111532524968513e-05,\n",
       " 'github_repository_reader.py': 3.086564842392975e-05,\n",
       " 'github_api_client.py': 3.086564842392975e-05,\n",
       " 'steamship': 2.6111532524968513e-05,\n",
       " 'file_reader.py': 3.6373316972121075e-05,\n",
       " 'question_gen': 3.9508255011136775e-05,\n",
       " 'guidance_generator.py': 2.8824322062196895e-05,\n",
       " 'openai_generator.py': 2.8824322062196895e-05,\n",
       " 'llm_generators.py': 2.8824322062196895e-05,\n",
       " 'llms': 6.280437012815735e-05,\n",
       " 'portkey_utils.py': 2.5844798798856127e-05,\n",
       " 'portkey.py': 2.6435340810137046e-05,\n",
       " 'litellm_utils.py': 2.5844798798856127e-05,\n",
       " 'gemini_utils.py': 2.5844798798856127e-05,\n",
       " 'watsonx_utils.py': 2.5844798798856127e-05,\n",
       " 'mistralai_utils.py': 2.5844798798856127e-05,\n",
       " 'vertex_gemini_utils.py': 2.5844798798856127e-05,\n",
       " 'palm.py': 2.5844798798856127e-05,\n",
       " 'cohere_utils.py': 2.5844798798856127e-05,\n",
       " 'konko_utils.py': 2.5844798798856127e-05,\n",
       " 'mistral.py': 2.5844798798856127e-05,\n",
       " 'vllm.py': 2.6527226126586638e-05,\n",
       " 'vertex_utils.py': 2.5844798798856127e-05,\n",
       " 'custom.py': 2.6706507957144542e-05,\n",
       " 'openrouter.py': 2.5844798798856127e-05,\n",
       " 'langchain_utils.py': 2.5844798798856127e-05,\n",
       " 'xinference_utils.py': 2.5844798798856127e-05,\n",
       " 'localai.py': 2.5844798798856127e-05,\n",
       " 'replicate.py': 2.789899759683122e-05,\n",
       " 'perplexity.py': 2.5844798798856127e-05,\n",
       " 'anyscale.py': 2.5844798798856127e-05,\n",
       " 'llama_cpp.py': 2.5844798798856127e-05,\n",
       " 'llm.py': 2.5844798798856127e-05,\n",
       " 'vertex.py': 2.5844798798856127e-05,\n",
       " 'rungpt.py': 2.5844798798856127e-05,\n",
       " 'vllm_utils.py': 2.5844798798856127e-05,\n",
       " 'anyscale_utils.py': 2.5844798798856127e-05,\n",
       " 'litellm.py': 2.743741960294492e-05,\n",
       " 'konko.py': 2.7761472630584972e-05,\n",
       " 'llama_api.py': 2.5844798798856127e-05,\n",
       " 'openllm.py': 2.6451683494388906e-05,\n",
       " 'anthropic.py': 3.184155991649654e-05,\n",
       " 'openai_like.py': 2.5844798798856127e-05,\n",
       " 'xinference.py': 2.5844798798856127e-05,\n",
       " 'bedrock_utils.py': 2.5844798798856127e-05,\n",
       " 'watsonx.py': 2.5844798798856127e-05,\n",
       " 'ai21.py': 2.8051679879756988e-05,\n",
       " 'ai21_utils.py': 2.5844798798856127e-05,\n",
       " 'anthropic_utils.py': 2.5844798798856127e-05,\n",
       " 'everlyai_utils.py': 2.5844798798856127e-05,\n",
       " 'cohere.py': 3.4460225082537724e-05,\n",
       " 'ollama.py': 2.5844798798856127e-05,\n",
       " 'llama_utils.py': 2.5844798798856127e-05,\n",
       " 'monsterapi.py': 2.6652976600273667e-05,\n",
       " 'predibase.py': 2.5844798798856127e-05,\n",
       " 'everlyai.py': 2.5844798798856127e-05,\n",
       " 'text_splitter': 5.009942761621649e-05,\n",
       " 'query_engine': 6.0234332096065755e-05,\n",
       " 'knowledge_graph_query_engine.py': 2.621968903402682e-05,\n",
       " 'citation_query_engine.py': 2.621968903402682e-05,\n",
       " 'sql_join_query_engine.py': 2.621968903402682e-05,\n",
       " 'retriever_query_engine.py': 2.621968903402682e-05,\n",
       " 'multi_modal.py': 2.621968903402682e-05,\n",
       " 'retry_query_engine.py': 2.621968903402682e-05,\n",
       " 'router_query_engine.py': 2.621968903402682e-05,\n",
       " 'cogniswitch_query_engine.py': 2.621968903402682e-05,\n",
       " 'multistep_query_engine.py': 2.621968903402682e-05,\n",
       " 'pandas_query_engine.py': 2.621968903402682e-05,\n",
       " 'sql_vector_query_engine.py': 2.621968903402682e-05,\n",
       " 'graph_query_engine.py': 2.621968903402682e-05,\n",
       " 'sub_question_query_engine.py': 2.621968903402682e-05,\n",
       " 'transform_query_engine.py': 2.621968903402682e-05,\n",
       " 'retry_source_query_engine.py': 2.621968903402682e-05,\n",
       " 'flare': 2.621968903402682e-05,\n",
       " 'answer_inserter.py': 2.9795137251486444e-05,\n",
       " 'extractors': 3.86617125467603e-05,\n",
       " 'metadata_extractors.py': 3.2141512685634956e-05,\n",
       " 'marvin_metadata_extractor.py': 3.2141512685634956e-05,\n",
       " 'command_line': 3.86617125467603e-05,\n",
       " 'command_line.py': 4.231681190047976e-05,\n",
       " 'llama_dataset': 3.9326931554116644e-05,\n",
       " 'generator.py': 2.880596373347999e-05,\n",
       " 'rag.py': 2.880596373347999e-05,\n",
       " 'evaluator_evaluation.py': 2.880596373347999e-05,\n",
       " 'selectors': 3.9508255011136775e-05,\n",
       " 'embedding_selectors.py': 2.9690907608811514e-05,\n",
       " 'pydantic_selectors.py': 2.9690907608811514e-05,\n",
       " 'llm_selectors.py': 2.9690907608811514e-05,\n",
       " 'tests': 3.967108410548641e-05,\n",
       " 'test_utils.py': 4.331551690380329e-05,\n",
       " 'conftest.py': 5.856881409245937e-05,\n",
       " 'test_schema.py': 2.6204522340114882e-05,\n",
       " 'ruff.toml': 2.6204522340114882e-05,\n",
       " 'mock_utils': 2.6204522340114882e-05,\n",
       " 'mock_text_splitter.py': 2.982850776119417e-05,\n",
       " 'mock_predict.py': 2.982850776119417e-05,\n",
       " 'mock_prompts.py': 2.982850776119417e-05,\n",
       " 'mock_utils.py': 3.444908859580384e-05,\n",
       " 'test_elasticsearch.py': 2.6772355542999627e-05,\n",
       " 'test_docarray.py': 2.565865235938723e-05,\n",
       " 'test_epsilla.py': 2.565865235938723e-05,\n",
       " 'test_simple.py': 2.847245510613805e-05,\n",
       " 'test_astra.py': 2.565865235938723e-05,\n",
       " 'test_mongodb.py': 2.565865235938723e-05,\n",
       " 'test_lancedb.py': 2.565865235938723e-05,\n",
       " 'test_tencentvectordb.py': 2.565865235938723e-05,\n",
       " 'test_milvus.py': 2.565865235938723e-05,\n",
       " 'test_cassandra.py': 2.565865235938723e-05,\n",
       " 'test_qdrant.py': 2.565865235938723e-05,\n",
       " 'test_rockset.py': 2.565865235938723e-05,\n",
       " 'test_singlestoredb.py': 2.565865235938723e-05,\n",
       " 'test_timescalevector.py': 2.565865235938723e-05,\n",
       " 'test_metadata_filters.py': 2.565865235938723e-05,\n",
       " 'test_tair.py': 2.565865235938723e-05,\n",
       " 'test_azurecosmosmongo.py': 2.565865235938723e-05,\n",
       " 'test_cogsearch.py': 2.565865235938723e-05,\n",
       " 'test_google.py': 2.911146238836153e-05,\n",
       " 'test_weaviate.py': 2.565865235938723e-05,\n",
       " 'test_lantern.py': 2.565865235938723e-05,\n",
       " 'test_postgres.py': 2.565865235938723e-05,\n",
       " 'docker-compose': 2.565865235938723e-05,\n",
       " 'elasticsearch.yml': 4.714684183773766e-05,\n",
       " 'test_pipeline.py': 2.8880539514990203e-05,\n",
       " 'test_cache.py': 2.8880539514990203e-05,\n",
       " 'test_loading.py': 2.632927075657671e-05,\n",
       " 'test_loading_graph.py': 2.632927075657671e-05,\n",
       " 'test_service_context.py': 2.632927075657671e-05,\n",
       " 'test_prompt_helper.py': 2.632927075657671e-05,\n",
       " 'test_embedding_retriever.py': 2.7152645391267427e-05,\n",
       " 'test_retrievers.py': 8.048426467044037e-05,\n",
       " 'test_index.py': 7.575487711582847e-05,\n",
       " 'test_json_query.py': 2.7741514152240674e-05,\n",
       " 'test_sql_query.py': 2.7741514152240674e-05,\n",
       " 'test_base.py': 0.00011745656982769585,\n",
       " 'test_vectara.py': 2.7112986009866407e-05,\n",
       " 'test_tree_summarize.py': 2.8781222330304924e-05,\n",
       " 'test_response_builder.py': 2.8781222330304924e-05,\n",
       " 'test_myscale.py': 2.621480504512712e-05,\n",
       " 'mock_faiss.py': 2.621480504512712e-05,\n",
       " 'mock_services.py': 2.621480504512712e-05,\n",
       " 'test_pinecone.py': 2.621480504512712e-05,\n",
       " 'test_faiss.py': 2.621480504512712e-05,\n",
       " 'test_deeplake.py': 2.621480504512712e-05,\n",
       " 'test_output_parser.py': 2.996918903345466e-05,\n",
       " 'test_compose.py': 3.248427915576454e-05,\n",
       " 'test_compose_vector.py': 3.248427915576454e-05,\n",
       " 'test_query_bundle.py': 3.248427915576454e-05,\n",
       " 'test_embedding_utils.py': 3.248427915576454e-05,\n",
       " 'test_ondemand_loader.py': 2.7100903248469673e-05,\n",
       " 'test_query_engine_tool.py': 2.7100903248469673e-05,\n",
       " 'test_gradient.py': 2.695850198246853e-05,\n",
       " 'test_llm_rails.py': 2.6471683059350805e-05,\n",
       " 'test_fastembed.py': 2.6471683059350805e-05,\n",
       " 'test_bedrock.py': 2.695850198246853e-05,\n",
       " 'test_huggingface.py': 2.695850198246853e-05,\n",
       " 'test_html.py': 2.9251757942216366e-05,\n",
       " 'test_unstructured.py': 2.9251757942216366e-05,\n",
       " 'metadata_extractor.py': 2.9251757942216366e-05,\n",
       " 'test_json.py': 3.000531059144647e-05,\n",
       " 'test_markdown_element.py': 2.9251757942216366e-05,\n",
       " 'test_markdown.py': 2.9251757942216366e-05,\n",
       " 'test_chat_memory_buffer.py': 3.520095965345064e-05,\n",
       " 'test_prompt_registry.py': 2.7729149287460993e-05,\n",
       " 'test_predictor.py': 2.7729149287460993e-05,\n",
       " 'test_token_counter.py': 2.757889092649795e-05,\n",
       " 'test_llama_debug.py': 2.757889092649795e-05,\n",
       " 'test_optimizer.py': 2.7397004691302223e-05,\n",
       " 'test_longcontext_reorder.py': 2.7397004691302223e-05,\n",
       " 'test_llm_rerank.py': 2.7397004691302223e-05,\n",
       " 'test_metadata_replacement.py': 2.7397004691302223e-05,\n",
       " 'test_replicate_multi_modal.py': 2.9690907608811514e-05,\n",
       " 'test_node_mapping.py': 2.9934110863951793e-05,\n",
       " 'test_react_agent.py': 2.796566518232308e-05,\n",
       " 'test_react_output_parser.py': 2.796566518232308e-05,\n",
       " 'test_openai_assistant_agent.py': 3.0440330518240967e-05,\n",
       " 'test_openai_agent.py': 3.0440330518240967e-05,\n",
       " 'test_storage_context.py': 2.815539984496259e-05,\n",
       " 'test_simple_kvstore.py': 2.6906987558101955e-05,\n",
       " 'mock_mongodb.py': 2.6906987558101955e-05,\n",
       " 'test_dynamodb_kvstore.py': 2.6906987558101955e-05,\n",
       " 'test_firestore_kvstore.py': 2.6906987558101955e-05,\n",
       " 'test_s3_kvstore.py': 2.6906987558101955e-05,\n",
       " 'test_mongodb_kvstore.py': 2.6906987558101955e-05,\n",
       " 'test_redis_kvstore.py': 2.6906987558101955e-05,\n",
       " 'test_mongo_docstore.py': 3.077049305225759e-05,\n",
       " 'test_dynamodb_docstore.py': 3.077049305225759e-05,\n",
       " 'test_simple_docstore.py': 3.077049305225759e-05,\n",
       " 'test_redis_docstore.py': 3.077049305225759e-05,\n",
       " 'test_firestore_docstore.py': 3.077049305225759e-05,\n",
       " 'test_dynamodb_index_store.py': 2.7698520340691668e-05,\n",
       " 'test_firestore_indexstore.py': 2.7698520340691668e-05,\n",
       " 'test_simple_index_store.py': 2.7698520340691668e-05,\n",
       " 'test_sql_wrapper.py': 3.402383534188462e-05,\n",
       " 'test_pydantic.py': 2.842007782493368e-05,\n",
       " 'test_selection.py': 2.842007782493368e-05,\n",
       " 'token_predictor': 2.6204522340114882e-05,\n",
       " 'test_llm_program.py': 2.9044388352949915e-05,\n",
       " 'test_lmformatenforcer.py': 2.9044388352949915e-05,\n",
       " 'test_guidance.py': 2.9044388352949915e-05,\n",
       " 'test_multi_modal_llm_program.py': 2.9044388352949915e-05,\n",
       " 'test_guidance_utils.py': 2.7423238491274047e-05,\n",
       " 'test_mixin.py': 2.7423238491274047e-05,\n",
       " 'test_refine.py': 2.705578377058471e-05,\n",
       " 'test_dataset_generation.py': 2.6913417936688957e-05,\n",
       " 'test_condense_plus_context.py': 2.7314957453100516e-05,\n",
       " 'test_condense_question.py': 2.7314957453100516e-05,\n",
       " 'test_simplewebreader.py': 2.6111532524968513e-05,\n",
       " 'test_load_reader.py': 2.6111532524968513e-05,\n",
       " 'test_html_reader.py': 2.6111532524968513e-05,\n",
       " 'test_file.py': 2.6111532524968513e-05,\n",
       " 'test_string_iterable.py': 2.6111532524968513e-05,\n",
       " 'test_mongo.py': 2.6111532524968513e-05,\n",
       " 'test_llm_generators.py': 2.8824322062196895e-05,\n",
       " 'test_guidance_generator.py': 2.8824322062196895e-05,\n",
       " 'test_anthropic.py': 2.5844798798856127e-05,\n",
       " 'test_openai.py': 2.5844798798856127e-05,\n",
       " 'test_palm.py': 2.5844798798856127e-05,\n",
       " 'test_vllm.py': 2.5844798798856127e-05,\n",
       " 'test_konko.py': 2.5844798798856127e-05,\n",
       " 'test_openai_utils.py': 2.5844798798856127e-05,\n",
       " 'test_cohere.py': 2.5844798798856127e-05,\n",
       " 'test_xinference.py': 2.5844798798856127e-05,\n",
       " 'test_langchain.py': 2.5844798798856127e-05,\n",
       " 'test_localai.py': 2.5844798798856127e-05,\n",
       " 'test_anthropic_utils.py': 2.5844798798856127e-05,\n",
       " 'test_watsonx.py': 2.5844798798856127e-05,\n",
       " 'test_llama_utils.py': 2.5844798798856127e-05,\n",
       " 'test_litellm.py': 2.5844798798856127e-05,\n",
       " 'test_custom.py': 2.5844798798856127e-05,\n",
       " 'test_vertex.py': 2.5844798798856127e-05,\n",
       " 'test_gemini.py': 2.5844798798856127e-05,\n",
       " 'test_openai_like.py': 2.5844798798856127e-05,\n",
       " 'test_ai21.py': 2.5844798798856127e-05,\n",
       " 'test_rungpt.py': 2.5844798798856127e-05,\n",
       " 'test_sentence_splitter.py': 3.23220543525152e-05,\n",
       " 'test_code_splitter.py': 3.23220543525152e-05,\n",
       " 'test_token_splitter.py': 3.23220543525152e-05,\n",
       " 'test_cogniswitch_query_engine.py': 2.621968903402682e-05,\n",
       " 'test_pandas.py': 2.621968903402682e-05,\n",
       " 'test_retriever_query_engine.py': 2.621968903402682e-05,\n",
       " 'test_llm_selectors.py': 2.9690907608811514e-05,\n",
       " 'docs': 5.2940974282117275e-05,\n",
       " 'index.rst': 2.8220638157666743e-05,\n",
       " 'conf.py': 2.74122995112069e-05,\n",
       " 'DOCS_README.md': 2.74122995112069e-05,\n",
       " 'make.bat': 2.74122995112069e-05,\n",
       " 'contributing': 2.74122995112069e-05,\n",
       " 'documentation.rst': 3.7078890249406155e-05,\n",
       " 'contributing.rst': 3.7078890249406155e-05,\n",
       " 'changes': 2.74122995112069e-05,\n",
       " 'deprecated_terms.md': 3.7078890249406155e-05,\n",
       " 'changelog.rst': 3.7078890249406155e-05,\n",
       " 'understanding': 2.74122995112069e-05,\n",
       " 'understanding.md': 2.7962626625442352e-05,\n",
       " 'using_llms': 2.7962626625442352e-05,\n",
       " 'privacy.md': 3.724750145979732e-05,\n",
       " 'using_llms.md': 3.724750145979732e-05,\n",
       " 'loading': 3.05672733751463e-05,\n",
       " 'llamahub.md': 2.9065294836318256e-05,\n",
       " 'loading.md': 2.9065294836318256e-05,\n",
       " 'evaluating': 3.05672733751463e-05,\n",
       " 'evaluating.md': 2.8241447067300514e-05,\n",
       " 'cost_analysis': 2.8241447067300514e-05,\n",
       " 'usage_pattern.md': 6.34713977182213e-05,\n",
       " 'root.md': 0.0001546531563286904,\n",
       " 'storing': 3.05672733751463e-05,\n",
       " 'storing.md': 2.9065294836318256e-05,\n",
       " 'tracing_and_debugging': 2.7962626625442352e-05,\n",
       " 'tracing_and_debugging.md': 4.9137023043856225e-05,\n",
       " 'indexing': 3.05672733751463e-05,\n",
       " 'indexing.md': 2.8601880466245776e-05,\n",
       " 'querying': 3.05672733751463e-05,\n",
       " 'querying.md': 2.9683180663081565e-05,\n",
       " 'putting_it_all_together': 2.7962626625442352e-05,\n",
       " 'structured_data.md': 2.773588419255019e-05,\n",
       " 'putting_it_all_together.md': 2.773588419255019e-05,\n",
       " 'q_and_a.md': 3.2424248342017286e-05,\n",
       " 'graphs.md': 2.773588419255019e-05,\n",
       " 'agents.md': 3.2424248342017286e-05,\n",
       " 'apps.md': 2.773588419255019e-05,\n",
       " 'q_and_a': 2.773588419255019e-05,\n",
       " 'unified_query.md': 3.7201923048704195e-05,\n",
       " 'terms_definitions_tutorial.md': 3.7201923048704195e-05,\n",
       " 'structured_data': 2.773588419255019e-05,\n",
       " 'Airbyte_demo.ipynb': 4.904586622166998e-05,\n",
       " 'chatbots': 2.773588419255019e-05,\n",
       " 'building_a_chatbot.md': 4.904586622166998e-05,\n",
       " 'apps': 2.773588419255019e-05,\n",
       " 'fullstack_app_guide.md': 3.7201923048704195e-05,\n",
       " 'fullstack_with_delphic.md': 3.7201923048704195e-05,\n",
       " '_static': 2.74122995112069e-05,\n",
       " 'faiss_index_0.png': 2.565865235938723e-05,\n",
       " 'faiss_index_1.png': 2.565865235938723e-05,\n",
       " 'weaviate_reader_1.png': 2.565865235938723e-05,\n",
       " 'qdrant_index_0.png': 2.565865235938723e-05,\n",
       " 'weaviate_reader_0.png': 2.565865235938723e-05,\n",
       " 'pinecone_reader.png': 2.565865235938723e-05,\n",
       " 'pinecone_index_0.png': 2.565865235938723e-05,\n",
       " 'qdrant_reader.png': 2.565865235938723e-05,\n",
       " 'weaviate_index_0.png': 2.565865235938723e-05,\n",
       " 'simple_index_0.png': 2.565865235938723e-05,\n",
       " 'vector_store.png': 2.632927075657671e-05,\n",
       " 'keyword.png': 2.632927075657671e-05,\n",
       " 'keyword_query.png': 2.632927075657671e-05,\n",
       " 'tree_summarize.png': 2.632927075657671e-05,\n",
       " 'tree_query.png': 2.632927075657671e-05,\n",
       " 'list.png': 2.632927075657671e-05,\n",
       " 'list_query.png': 2.632927075657671e-05,\n",
       " 'list_filter_query.png': 2.632927075657671e-05,\n",
       " 'vector_store_query.png': 2.632927075657671e-05,\n",
       " 'tree.png': 2.632927075657671e-05,\n",
       " 'create_and_refine.png': 2.632927075657671e-05,\n",
       " 'doc_example.jpeg': 2.6471683059350805e-05,\n",
       " 'node_postprocessors': 3.0916960702415014e-05,\n",
       " 'prev_next.png': 3.1972903233959724e-05,\n",
       " 'recency.png': 3.1972903233959724e-05,\n",
       " 'response_1.jpeg': 2.8781222330304924e-05,\n",
       " 'data_connectors': 2.725697892242819e-05,\n",
       " 'llamahub.png': 2.621716412654846e-05,\n",
       " 'css': 2.6591759915071854e-05,\n",
       " 'algolia.css': 3.667587999770754e-05,\n",
       " 'custom.css': 3.667587999770754e-05,\n",
       " 'js': 2.6591759915071854e-05,\n",
       " 'mendablesearch.js': 3.667587999770754e-05,\n",
       " 'algolia.js': 3.667587999770754e-05,\n",
       " 'production_rag': 2.6591759915071854e-05,\n",
       " 'joint_qa_summary.png': 3.101692993672297e-05,\n",
       " 'decouple_chunks.png': 3.101692993672297e-05,\n",
       " 'structured_retrieval.png': 3.101692993672297e-05,\n",
       " 'doc_agents.png': 3.101692993672297e-05,\n",
       " 'agent_step_execute.png': 2.9768438153595342e-05,\n",
       " 'structured_output': 2.6591759915071854e-05,\n",
       " 'diagram1.png': 3.667587999770754e-05,\n",
       " 'program2.png': 3.667587999770754e-05,\n",
       " 'query_transformations': 2.725697892242819e-05,\n",
       " 'multi_step_diagram.png': 2.999757483011268e-05,\n",
       " 'single_step_diagram.png': 2.999757483011268e-05,\n",
       " 'storage.png': 2.815539984496259e-05,\n",
       " 'integrations': 0.00010260120102657501,\n",
       " 'honeyhive.png': 2.9939793211324236e-05,\n",
       " 'arize_phoenix.png': 2.9939793211324236e-05,\n",
       " 'openllmetry.png': 2.9939793211324236e-05,\n",
       " 'trulens.png': 2.9939793211324236e-05,\n",
       " 'perfetto.png': 2.9939793211324236e-05,\n",
       " 'wandb.png': 2.9939793211324236e-05,\n",
       " 'diagram_b1.png': 2.8917240504218257e-05,\n",
       " 'diagram_b0.png': 2.8917240504218257e-05,\n",
       " 'diagram_q1.png': 2.8917240504218257e-05,\n",
       " 'diagram_q2.png': 2.8917240504218257e-05,\n",
       " 'diagram.png': 2.8917240504218257e-05,\n",
       " 'eval_response_context.png': 2.6913417936688957e-05,\n",
       " 'eval_query_sources.png': 2.6913417936688957e-05,\n",
       " 'eval_query_response_context.png': 2.6913417936688957e-05,\n",
       " 'contribution': 2.6591759915071854e-05,\n",
       " 'contrib.png': 4.7993780119676675e-05,\n",
       " 'disclosure.png': 3.248427915576454e-05,\n",
       " 'query_classes.png': 3.248427915576454e-05,\n",
       " 'getting_started': 2.8646079550540346e-05,\n",
       " 'querying.jpg': 2.7582592546565415e-05,\n",
       " 'rag.jpg': 2.7582592546565415e-05,\n",
       " 'indexing.jpg': 2.7582592546565415e-05,\n",
       " 'basic_rag.png': 2.7582592546565415e-05,\n",
       " 'stages.png': 2.7582592546565415e-05,\n",
       " 'api_reference': 2.74122995112069e-05,\n",
       " 'evaluation.rst': 2.616631852219825e-05,\n",
       " 'llm_predictor.rst': 2.616631852219825e-05,\n",
       " 'multi_modal.rst': 2.616631852219825e-05,\n",
       " 'callbacks.rst': 2.616631852219825e-05,\n",
       " 'agents.rst': 2.616631852219825e-05,\n",
       " 'response.rst': 2.616631852219825e-05,\n",
       " 'prompts.rst': 2.616631852219825e-05,\n",
       " 'llms.rst': 2.616631852219825e-05,\n",
       " 'playground.rst': 2.616631852219825e-05,\n",
       " 'storage.rst': 2.616631852219825e-05,\n",
       " 'service_context.rst': 2.616631852219825e-05,\n",
       " 'composability.rst': 2.616631852219825e-05,\n",
       " 'node.rst': 2.616631852219825e-05,\n",
       " 'readers.rst': 2.616631852219825e-05,\n",
       " 'node_postprocessor.rst': 2.616631852219825e-05,\n",
       " 'struct_store.rst': 2.7137609403036556e-05,\n",
       " 'finetuning.rst': 2.616631852219825e-05,\n",
       " 'example_notebooks.rst': 2.616631852219825e-05,\n",
       " 'query.rst': 2.616631852219825e-05,\n",
       " 'memory.rst': 2.616631852219825e-05,\n",
       " 'indices.rst': 2.616631852219825e-05,\n",
       " 'vector_store.rst': 3.06022689751947e-05,\n",
       " 'list.rst': 2.7804849005970514e-05,\n",
       " 'empty.rst': 2.7804849005970514e-05,\n",
       " 'kg.rst': 2.7804849005970514e-05,\n",
       " 'tree.rst': 2.7804849005970514e-05,\n",
       " 'table.rst': 2.7804849005970514e-05,\n",
       " 'service_context': 7.516317002540346e-05,\n",
       " 'embeddings.rst': 3.4525971713464364e-05,\n",
       " 'prompt_helper.rst': 3.4525971713464364e-05,\n",
       " 'node_parser.rst': 3.4525971713464364e-05,\n",
       " 'langchain_integrations': 2.616631852219825e-05,\n",
       " 'base.rst': 4.763898059148303e-05,\n",
       " 'docstore.rst': 2.815539984496259e-05,\n",
       " 'index_store.rst': 2.815539984496259e-05,\n",
       " 'kv_store.rst': 2.815539984496259e-05,\n",
       " 'indices_save_load.rst': 2.815539984496259e-05,\n",
       " 'openai.rst': 2.7069668040034323e-05,\n",
       " 'replicate.rst': 2.7069668040034323e-05,\n",
       " 'litellm.rst': 2.5844798798856127e-05,\n",
       " 'openai_like.rst': 2.5844798798856127e-05,\n",
       " 'palm.rst': 2.5844798798856127e-05,\n",
       " 'azure_openai.rst': 2.5844798798856127e-05,\n",
       " 'langchain.rst': 2.5844798798856127e-05,\n",
       " 'huggingface.rst': 2.5844798798856127e-05,\n",
       " 'gradient_base_model.rst': 2.5844798798856127e-05,\n",
       " 'predibase.rst': 2.5844798798856127e-05,\n",
       " 'gradient_model_adapter.rst': 2.5844798798856127e-05,\n",
       " 'llama_cpp.rst': 2.5844798798856127e-05,\n",
       " 'xinference.rst': 2.5844798798856127e-05,\n",
       " 'openllm.rst': 2.5844798798856127e-05,\n",
       " 'anthropic.rst': 2.5844798798856127e-05,\n",
       " 'response_synthesizer.rst': 3.248427915576454e-05,\n",
       " 'query_engines.rst': 3.248427915576454e-05,\n",
       " 'chat_engines.rst': 3.248427915576454e-05,\n",
       " 'retrievers.rst': 3.248427915576454e-05,\n",
       " 'query_transform.rst': 3.248427915576454e-05,\n",
       " 'query_bundle.rst': 3.248427915576454e-05,\n",
       " 'transform.rst': 2.683355812513221e-05,\n",
       " 'chat_engines': 4.0410626878470474e-05,\n",
       " 'condense_plus_context_chat_engine.rst': 3.113467445132301e-05,\n",
       " 'simple_chat_engine.rst': 3.113467445132301e-05,\n",
       " 'condense_question_chat_engine.rst': 3.113467445132301e-05,\n",
       " 'query_engines': 3.248427915576454e-05,\n",
       " 'sql_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'knowledge_graph_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'router_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'retriever_router_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'graph_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'retriever_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'sql_join_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'multistep_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'flare_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'citation_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'transform_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'sub_question_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'pandas_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'examples': 4.053761210069922e-05,\n",
       " 'citation': 2.6023198883094744e-05,\n",
       " 'pdf_page_reference.ipynb': 4.752703601584818e-05,\n",
       " 'TypesenseDemo.ipynb': 2.565865235938723e-05,\n",
       " 'BagelAutoRetriever.ipynb': 2.565865235938723e-05,\n",
       " 'RocksetIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'TencentVectorDBIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'QdrantIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'Timescalevector.ipynb': 2.565865235938723e-05,\n",
       " 'MongoDBAtlasVectorSearch.ipynb': 2.565865235938723e-05,\n",
       " 'DocArrayInMemoryIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'chroma_auto_retriever.ipynb': 2.565865235938723e-05,\n",
       " 'ZepIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'FaissIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'SimpleIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'qdrant_hybrid.ipynb': 2.565865235938723e-05,\n",
       " 'DeepLakeIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'pinecone_metadata_filter.ipynb': 2.565865235938723e-05,\n",
       " 'elasticsearch_auto_retriever.ipynb': 2.565865235938723e-05,\n",
       " 'SimpleIndexOnS3.ipynb': 2.565865235938723e-05,\n",
       " 'CassandraIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'Elasticsearch_demo.ipynb': 2.565865235938723e-05,\n",
       " 'AwadbDemo.ipynb': 2.565865235938723e-05,\n",
       " 'postgres.ipynb': 2.565865235938723e-05,\n",
       " 'chroma_metadata_filter.ipynb': 2.565865235938723e-05,\n",
       " 'AzureCosmosDBMongoDBvCoreDemo.ipynb': 2.565865235938723e-05,\n",
       " 'Neo4jVectorDemo.ipynb': 2.565865235938723e-05,\n",
       " 'ElasticsearchIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'SimpleIndexDemoLlama-Local.ipynb': 2.565865235938723e-05,\n",
       " 'MyScaleIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'MetalIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'AsyncIndexCreationDemo.ipynb': 2.565865235938723e-05,\n",
       " 'TairIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'PineconeIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'RedisIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'SimpleIndexDemoLlama2.ipynb': 2.565865235938723e-05,\n",
       " 'WeaviateIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'SupabaseVectorIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'PGVectoRsDemo.ipynb': 2.565865235938723e-05,\n",
       " 'WeaviateIndex_metadata_filter.ipynb': 2.565865235938723e-05,\n",
       " 'WeaviateIndexDemo-Hybrid.ipynb': 2.565865235938723e-05,\n",
       " 'DocArrayHnswIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'DashvectorIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'OpensearchDemo.ipynb': 2.565865235938723e-05,\n",
       " 'PineconeIndexDemo-Hybrid.ipynb': 2.565865235938723e-05,\n",
       " 'Qdrant_metadata_filter.ipynb': 2.565865235938723e-05,\n",
       " 'CognitiveSearchIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'SimpleIndexDemoMMR.ipynb': 2.565865235938723e-05,\n",
       " 'pinecone_auto_retriever.ipynb': 2.565865235938723e-05,\n",
       " 'ChromaIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'LanceDBIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'BagelIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'EpsillaIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'MilvusIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'AstraDBIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'Lantern.ipynb': 2.565865235938723e-05,\n",
       " 'index_faiss_core.index': 2.565865235938723e-05,\n",
       " 'PineconeIndexDemo-0.6.0.ipynb': 2.565865235938723e-05,\n",
       " 'existing_data': 2.565865235938723e-05,\n",
       " 'pinecone_existing_data.ipynb': 3.625241085673803e-05,\n",
       " 'weaviate_existing_data.ipynb': 3.625241085673803e-05,\n",
       " 'vectaraDemo.ipynb': 2.7112986009866407e-05,\n",
       " 'manage_retrieval_benchmark.ipynb': 2.7112986009866407e-05,\n",
       " 'GoogleDemo.ipynb': 2.7112986009866407e-05,\n",
       " 'zcpDemo.ipynb': 2.7112986009866407e-05,\n",
       " 'bm25_retriever.ipynb': 2.683355812513221e-05,\n",
       " 'router_retriever.ipynb': 2.683355812513221e-05,\n",
       " 'recursive_retriever_nodes.ipynb': 2.683355812513221e-05,\n",
       " 'auto_vs_recursive_retriever.ipynb': 2.683355812513221e-05,\n",
       " 'ensemble_retrieval.ipynb': 2.683355812513221e-05,\n",
       " 'simple_fusion.ipynb': 2.683355812513221e-05,\n",
       " 'auto_merging_retriever.ipynb': 2.683355812513221e-05,\n",
       " 'recurisve_retriever_nodes_braintrust.ipynb': 2.683355812513221e-05,\n",
       " 'deep_memory.ipynb': 2.683355812513221e-05,\n",
       " 'you_retriever.ipynb': 2.683355812513221e-05,\n",
       " 'reciprocal_rerank_fusion.ipynb': 2.683355812513221e-05,\n",
       " 'async_ingestion_pipeline.ipynb': 2.8880539514990203e-05,\n",
       " 'document_management_pipeline.ipynb': 2.8880539514990203e-05,\n",
       " 'ingestion_gdrive.ipynb': 2.8880539514990203e-05,\n",
       " 'advanced_ingestion_pipeline.ipynb': 2.8880539514990203e-05,\n",
       " 'redis_ingestion_pipeline.ipynb': 2.8880539514990203e-05,\n",
       " 'OnDemandLoaderTool.ipynb': 2.7100903248469673e-05,\n",
       " 'llm': 0.00011662926027491783,\n",
       " 'rungpt.ipynb': 2.7098566806047295e-05,\n",
       " 'watsonx.ipynb': 2.7098566806047295e-05,\n",
       " 'openllm.ipynb': 2.7098566806047295e-05,\n",
       " 'openai_json_vs_function_calling.ipynb': 2.7098566806047295e-05,\n",
       " 'portkey.ipynb': 2.7098566806047295e-05,\n",
       " 'everlyai.ipynb': 2.7098566806047295e-05,\n",
       " 'palm.ipynb': 2.7098566806047295e-05,\n",
       " 'cohere.ipynb': 2.7098566806047295e-05,\n",
       " 'vertex.ipynb': 2.7098566806047295e-05,\n",
       " 'predibase.ipynb': 2.7098566806047295e-05,\n",
       " 'llama_api.ipynb': 2.7098566806047295e-05,\n",
       " 'clarifai.ipynb': 2.8212269989659695e-05,\n",
       " 'bedrock.ipynb': 2.8212269989659695e-05,\n",
       " 'llama_2.ipynb': 2.7098566806047295e-05,\n",
       " 'gradient_model_adapter.ipynb': 2.7098566806047295e-05,\n",
       " 'xinference_local_deployment.ipynb': 2.7098566806047295e-05,\n",
       " 'azure_openai.ipynb': 2.7098566806047295e-05,\n",
       " 'gemini.ipynb': 2.9437139230837888e-05,\n",
       " 'huggingface.ipynb': 2.8212269989659695e-05,\n",
       " 'anyscale.ipynb': 2.7098566806047295e-05,\n",
       " 'vicuna.ipynb': 2.7098566806047295e-05,\n",
       " 'openrouter.ipynb': 2.7098566806047295e-05,\n",
       " 'llama_2_rap_battle.ipynb': 2.7098566806047295e-05,\n",
       " 'vllm.ipynb': 2.7098566806047295e-05,\n",
       " 'localai.ipynb': 2.7098566806047295e-05,\n",
       " 'llm_predictor.ipynb': 2.7098566806047295e-05,\n",
       " 'mistralai.ipynb': 2.8212269989659695e-05,\n",
       " 'monsterapi.ipynb': 2.7098566806047295e-05,\n",
       " 'ai21.ipynb': 2.7098566806047295e-05,\n",
       " 'llama_2_llama_cpp.ipynb': 2.7098566806047295e-05,\n",
       " 'perplexity.ipynb': 2.7098566806047295e-05,\n",
       " 'litellm.ipynb': 2.7098566806047295e-05,\n",
       " 'ollama.ipynb': 2.7098566806047295e-05,\n",
       " 'langchain.ipynb': 2.7098566806047295e-05,\n",
       " 'openai.ipynb': 2.7098566806047295e-05,\n",
       " 'anthropic.ipynb': 2.7098566806047295e-05,\n",
       " 'gradient_base_model.ipynb': 2.7098566806047295e-05,\n",
       " 'azure_playground.png': 2.7098566806047295e-05,\n",
       " 'azure_env.png': 2.7098566806047295e-05,\n",
       " 'Konko.ipynb': 2.7098566806047295e-05,\n",
       " 'fastembed.ipynb': 2.6471683059350805e-05,\n",
       " 'text_embedding_inference.ipynb': 2.6471683059350805e-05,\n",
       " 'voyageai.ipynb': 2.6471683059350805e-05,\n",
       " 'ollama_embedding.ipynb': 2.6471683059350805e-05,\n",
       " 'gradient.ipynb': 2.6471683059350805e-05,\n",
       " 'custom_embeddings.ipynb': 2.6471683059350805e-05,\n",
       " 'jinaai_embeddings.ipynb': 2.6471683059350805e-05,\n",
       " 'jina_embeddings.ipynb': 2.6471683059350805e-05,\n",
       " 'llm_rails.ipynb': 2.6471683059350805e-05,\n",
       " 'google_palm.ipynb': 2.6471683059350805e-05,\n",
       " 'Langchain.ipynb': 2.6471683059350805e-05,\n",
       " 'elasticsearch.ipynb': 2.6471683059350805e-05,\n",
       " ...}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform page rank on the graph\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "page_rank = nx.pagerank(G)\n",
    "page_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.1214549799908395e-05,\n",
       " 3.139584137403907e-05,\n",
       " 2.818730587641715e-05,\n",
       " 0.00021939831410859982,\n",
       " 3.34182345009083e-05)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_rank['OpenAIAgent'], page_rank['AgentRunner'], page_rank[\"BaseAgentRunner\"], page_rank[\"BaseReader\"], page_rank[\"BaseAgent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs2 = VectorSearch(\n",
    "    collection_name=\"explanations\",\n",
    "    collection_path=\"repomanager/statemanager/state/{}/meta/storage\".format(repo_id),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PDFReader', 'class'),\n",
       " ('DocxReader', 'class'),\n",
       " ('BaseReader', 'class'),\n",
       " ('GoogleDocsReader', 'class'),\n",
       " ('MyScaleReader', 'class')]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How to load a pdf\"\n",
    "res = vs2.search(query, top_k=5, type=\"class\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BaseReader', 'class'),\n",
       " ('PDFReader', 'class'),\n",
       " ('GoogleDocsReader', 'class'),\n",
       " ('DocxReader', 'class'),\n",
       " ('MyScaleReader', 'class')]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank res by page rank\n",
    "res_ranked = sorted(res, key=lambda x: page_rank[x[0]], reverse=True)\n",
    "res_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
