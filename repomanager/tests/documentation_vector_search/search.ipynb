{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chinmayshrivastava/Documents/GitHub/RepoStateManager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinmayshrivastava/Documents/GitHub/RepoStateManager/venv/lib/python3.11/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# navigate two directories up\n",
    "dir = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "sys.path.append(dir)\n",
    "# print working directory\n",
    "# mavigate to the top of working directory\n",
    "os.chdir(dir)\n",
    "print(os.getcwd())\n",
    "from repomanager.statemanager.vspace.vsearch import VectorSearch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chinmayshrivastava/Documents/GitHub/RepoStateManager'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/chinmayshrivastava/Documents/GitHub/RepoStateManager/repomanager/tests/documentation_vector_search',\n",
       " '/Users/chinmayshrivastava/anaconda3/lib/python311.zip',\n",
       " '/Users/chinmayshrivastava/anaconda3/lib/python3.11',\n",
       " '/Users/chinmayshrivastava/anaconda3/lib/python3.11/lib-dynload',\n",
       " '',\n",
       " '/Users/chinmayshrivastava/Documents/GitHub/RepoStateManager/venv/lib/python3.11/site-packages',\n",
       " '/Users/chinmayshrivastava/Documents/GitHub/RepoStateManager']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"45526e5b-f544-4016-8381-f88f5ca095ea\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 31986 nodes and 53803 edges\n"
     ]
    }
   ],
   "source": [
    "G = pickle.load(open(\"repomanager/statemanager/state/{}/state_0.pkl\".format(repo_id), \"rb\"))\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = VectorSearch(\n",
    "    collection_name=\"docs\",\n",
    "    collection_path=\"repomanager/statemanager/state/{}/meta/storage\".format(repo_id),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14163"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs.collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16178828477859497,\n",
       " 0.1697811484336853,\n",
       " 0.1707085371017456,\n",
       " 0.17160683870315552,\n",
       " 0.17160683870315552]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"For the user query, match the best use case or insight from a code base.\n",
    "user query: How to rerank\"\"\"\n",
    "nodes = [x[\"node\"] for x in vs.search_all(query)['metadatas'][0]]\n",
    "distances = vs.search_all(query)['distances'][0]\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define Advanced Retriever\n",
      "We define an advanced retriever that performs the following steps:\n",
      "\n",
      "Query generation/rewriting: generate multiple queries given the original user query\n",
      "Perform retrieval for each query over an ensemble of retrievers.\n",
      "Reranking/fusion: fuse results from all queries, and apply a reranking step to \"fuse\" the top relevant results!\n",
      "\n",
      "Then in the next section we'll plug this into our response synthesis module.\n",
      "\n",
      "Step 1: Query Generation/Rewriting\n",
      "The first step is to generate queries from the original query to better match the query intent, and increase precision/recall of the retrieved results. For instance, we might be able to rewrite the query into smaller queries.\n",
      "We can do this by prompting ChatGPT.\n",
      "\n",
      "input:\n",
      "```python\n",
      "from llama_index import PromptTemplate\n",
      "```\n",
      "output:\n",
      "input:\n",
      "```python\n",
      "query_str = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\"\n",
      "```\n",
      "output:\n",
      "input:\n",
      "```python\n",
      "query_gen_prompt_str = (\n",
      "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
      "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
      "    \"related to the following input query:\\n\"\n",
      "    \"Query: {query}\\n\"\n",
      "    \"Queries:\\n\"\n",
      ")\n",
      "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
      "```\n",
      "output:\n",
      "input:\n",
      "```python\n",
      "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
      "    fmt_prompt = query_gen_prompt.format(\n",
      "        num_queries=num_queries - 1, query=query_str\n",
      "    )\n",
      "    response = llm.complete(fmt_prompt)\n",
      "    queries = response.text.split(\"\\n\")\n",
      "    return queries\n",
      "```\n",
      "output:\n",
      "input:\n",
      "```python\n",
      "queries = generate_queries(llm, query_str, num_queries=4)\n",
      "```\n",
      "output:\n",
      "input:\n",
      "```python\n",
      "print(queries)\n",
      "```\n",
      "output:\n",
      "\n",
      "Directly retrieve top 2 most similar nodes\n",
      "\n",
      "input:\n",
      "```python\n",
      "query_engine = index.as_query_engine(\n",
      "    similarity_top_k=2,\n",
      ")\n",
      "response = query_engine.query(\n",
      "    \"What did Sam Altman do in this essay?\",\n",
      ")\n",
      "```\n",
      "output:\n",
      "\n",
      "Advanced RAG\n",
      "The GoogleIndex is built based on GoogleVectorStore and GoogleTextSynthesizer.\n",
      "These components can be combined with other powerful constructs in LlamaIndex to produce advanced RAG applications.\n",
      "Below we show a few examples.\n",
      "\n",
      "Reranker + Google Retriever\n",
      "Converting content into vectors is a lossy process. LLM-based Reranking\n",
      "remediates this by reranking the retrieved content using LLM, which has higher\n",
      "fidelity because it has access to both the actual query and the passage.\n",
      "\n",
      "input:\n",
      "```python\n",
      "from llama_index.response_synthesizers.google.generativeai import (\n",
      "    GoogleTextSynthesizer,\n",
      ")\n",
      "from llama_index.vector_stores.google.generativeai import (\n",
      "    GoogleVectorStore,\n",
      "    google_service_context,\n",
      ")\n",
      "from llama_index import ServiceContext, VectorStoreIndex\n",
      "from llama_index.llms import PaLM\n",
      "from llama_index.postprocessor import LLMRerank\n",
      "from llama_index.query_engine import RetrieverQueryEngine\n",
      "from llama_index.retrievers import VectorIndexRetriever\n",
      "\n",
      "# Set up the query engine with a reranker.\n",
      "store = GoogleVectorStore.from_corpus(corpus_id=\"some-corpus-id\")\n",
      "index = VectorStoreIndex.from_vector_store(\n",
      "    vector_store=store, service_context=google_service_context\n",
      ")\n",
      "response_synthesizer = GoogleTextSynthesizer.from_defaults(\n",
      "    temperature=0.7, answer_style=GenerateAnswerRequest.AnswerStyle.ABSTRACTIVE\n",
      ")\n",
      "reranker = LLMRerank(\n",
      "    top_n=10, service_context=ServiceContext.from_defaults(llm=PaLM())\n",
      ")\n",
      "query_engine = RetrieverQueryEngine.from_args(\n",
      "    retriever=VectorIndexRetriever(\n",
      "        index=index,\n",
      "        similarity_top_k=20,\n",
      "    ),\n",
      "    response_synthesizer=response_synthesizer,\n",
      "    node_postprocessors=[reranker],\n",
      ")\n",
      "\n",
      "# Query for better result!\n",
      "response = query_engine.query(\"What movie should I watch with my family?\")\n",
      "```\n",
      "output:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for node in nodes:\n",
    "    # get the node from the graph and get the content\n",
    "    print(G.nodes[node][\"content\"])\n",
    "    # print(G.nodes[node[:len(node)-1]+f\"{int(node[-1])-1}\"][\"content\"])\n",
    "    i = i + 1\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(\n",
    "    query: str,\n",
    "    vs: VectorSearch,\n",
    "    G,\n",
    "    top_k: int = 5,\n",
    "    max_depth: int = 3,\n",
    "    ):\n",
    "\n",
    "    nodes = [x[\"node\"] for x in vs.search_all(query, top_k=top_k)['metadatas'][0]]\n",
    "    docs = []\n",
    "    for node in nodes:\n",
    "        # get the node from the graph and get the content\n",
    "        _nodes = [node]\n",
    "        for i in range(max_depth):\n",
    "            _nodes.append(_nodes[i][:len(_nodes[i])-1]+f\"{int(_nodes[i][-1])-1}\")\n",
    "            _nodes.append(_nodes[i][:len(_nodes[i])-1]+f\"{int(_nodes[i][-1])+1}\")\n",
    "        _nodes = list(set(_nodes))\n",
    "        _nodes = [x for x in _nodes if x in G.nodes]\n",
    "        _docs = [G.nodes[x][\"content\"] for x in _nodes]\n",
    "        docs.append(_docs)\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Build Retriever-Enabled OpenAI Agent\\nWe build a top-level agent that can orchestrate across the different document agents to answer any user query.\\nThis RetrieverOpenAIAgent performs tool retrieval before tool use (unlike a default agent that tries to put all tools in the prompt).\\nImprovements from V0: We make the following improvements compared to the \"base\" version in V0.\\n\\nAdding in reranking: we use Cohere reranker to better filter the candidate set of documents.\\nAdding in a query planning tool: we add an explicit query planning tool that\\'s dynamically created based on the set of retrieved tools.\\n\\n\\ninput:\\n```python\\n# define tool for each document agent\\nall_tools = []\\nfor file_base, agent in agents_dict.items():\\n    summary = extra_info_dict[file_base][\"summary\"]\\n    doc_tool = QueryEngineTool(\\n        query_engine=agent,\\n        metadata=ToolMetadata(\\n            name=f\"tool_{file_base}\",\\n            description=summary,\\n        ),\\n    )\\n    all_tools.append(doc_tool)\\n```\\noutput:\\ninput:\\n```python\\nprint(all_tools[0].metadata)\\n```\\noutput:\\ninput:\\n```python\\n# define an \"object\" index and retriever over these tools\\nfrom llama_index import VectorStoreIndex\\nfrom llama_index.objects import (\\n    ObjectIndex,\\n    SimpleToolNodeMapping,\\n    ObjectRetriever,\\n)\\nfrom llama_index.retrievers import BaseRetriever\\nfrom llama_index.postprocessor import CohereRerank\\nfrom llama_index.tools import QueryPlanTool\\nfrom llama_index.query_engine import SubQuestionQueryEngine\\nfrom llama_index.llms import OpenAI\\n\\nllm = OpenAI(model_name=\"gpt-4-0613\")\\n\\ntool_mapping = SimpleToolNodeMapping.from_objects(all_tools)\\nobj_index = ObjectIndex.from_objects(\\n    all_tools,\\n    tool_mapping,\\n    VectorStoreIndex,\\n)\\nvector_node_retriever = obj_index.as_node_retriever(similarity_top_k=10)\\n\\n\\n# define a custom retriever with reranking\\nclass CustomRetriever(BaseRetriever):\\n    def __init__(self, vector_retriever, postprocessor=None):\\n        self._vector_retriever = vector_retriever\\n        self._postprocessor = postprocessor or CohereRerank(top_n=5)\\n        super().__init__()\\n\\n    def _retrieve(self, query_bundle):\\n        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\\n        filtered_nodes = self._postprocessor.postprocess_nodes(\\n            retrieved_nodes, query_bundle=query_bundle\\n        )\\n\\n        return filtered_nodes\\n\\n\\n# define a custom object retriever that adds in a query planning tool\\nclass CustomObjectRetriever(ObjectRetriever):\\n    def __init__(self, retriever, object_node_mapping, all_tools, llm=None):\\n        self._retriever = retriever\\n        self._object_node_mapping = object_node_mapping\\n        self._llm = llm or OpenAI(\"gpt-4-0613\")\\n\\n    def retrieve(self, query_bundle):\\n        nodes = self._retriever.retrieve(query_bundle)\\n        tools = [self._object_node_mapping.from_node(n.node) for n in nodes]\\n\\n        sub_question_sc = ServiceContext.from_defaults(llm=self._llm)\\n        sub_question_engine = SubQuestionQueryEngine.from_defaults(\\n            query_engine_tools=tools, service_context=sub_question_sc\\n        )\\n        sub_question_description = f\"\"\"\\\\\\nUseful for any queries that involve comparing multiple documents. ALWAYS use this tool for comparison queries - make sure to call this \\\\\\ntool with the original query. Do NOT use the other tools for any queries involving multiple documents.\\n\"\"\"\\n        sub_question_tool = QueryEngineTool(\\n            query_engine=sub_question_engine,\\n            metadata=ToolMetadata(\\n                name=\"compare_tool\", description=sub_question_description\\n            ),\\n        )\\n\\n        return tools + [sub_question_tool]\\n```\\noutput:\\ninput:\\n```python\\ncustom_node_retriever = CustomRetriever(vector_node_retriever)\\n\\n# wrap it with ObjectRetriever to return objects\\ncustom_obj_retriever = CustomObjectRetriever(\\n    custom_node_retriever, tool_mapping, all_tools, llm=llm\\n)\\n```\\noutput:\\ninput:\\n```python\\ntmps = custom_obj_retriever.retrieve(\"hello\")\\nprint(len(tmps))\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index.agent import FnRetrieverOpenAIAgent, ReActAgent\\n\\ntop_agent = FnRetrieverOpenAIAgent.from_retriever(\\n    custom_obj_retriever,\\n    system_prompt=\"\"\" \\\\\\nYou are an agent designed to answer queries about the documentation.\\nPlease always use the tools provided to answer a question. Do not rely on prior knowledge.\\\\\\n\\n\"\"\",\\n    llm=llm,\\n    verbose=True,\\n)\\n\\n# top_agent = ReActAgent.from_tools(\\n#     tool_retriever=custom_obj_retriever,\\n#     system_prompt=\"\"\" \\\\\\n# You are an agent designed to answer queries about the documentation.\\n# Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\\\\n\\n# \"\"\",\\n#     llm=llm,\\n#     verbose=True,\\n# )\\n```\\noutput:\\n',\n",
       "  'Define Baseline Vector Store Index\\nAs a point of comparison, we define a \"naive\" RAG pipeline which dumps all docs into a single vector index collection.\\nWe set the top_k = 4\\n\\ninput:\\n```python\\nall_nodes = [\\n    n for extra_info in extra_info_dict.values() for n in extra_info[\"nodes\"]\\n]\\n```\\noutput:\\ninput:\\n```python\\nbase_index = VectorStoreIndex(all_nodes)\\nbase_query_engine = base_index.as_query_engine(similarity_top_k=4)\\n```\\noutput:\\n',\n",
       "  'Building Multi-Document Agents\\nIn this section we show you how to construct the multi-document agent. We first build a document agent for each document, and then define the top-level parent agent with an object index.\\n\\ninput:\\n```python\\nfrom llama_index import VectorStoreIndex, SummaryIndex\\n```\\noutput:\\ninput:\\n```python\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n```\\noutput:\\n',\n",
       "  'Running Example Queries\\nLet\\'s run some example queries, ranging from QA / summaries over a single document to QA / summarization over multiple documents.\\n\\ninput:\\n```python\\nresponse = top_agent.query(\\n    \"Tell me about the different types of evaluation in LlamaIndex\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nprint(response)\\n```\\noutput:\\ninput:\\n```python\\n# baseline\\nresponse = base_query_engine.query(\\n    \"Tell me about the different types of evaluation in LlamaIndex\"\\n)\\nprint(str(response))\\n```\\noutput:\\ninput:\\n```python\\nresponse = top_agent.query(\\n    \"Compare the content in the contributions page vs. index page.\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nprint(response)\\n```\\noutput:\\ninput:\\n```python\\nresponse = top_agent.query(\\n    \"Can you compare the tree index and list index at a very high-level?\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nprint(str(response))\\n```\\noutput:\\n',\n",
       "  'Build Document Agent for each Document\\nIn this section we define \"document agents\" for each document.\\nWe define both a vector index (for semantic search) and summary index (for summarization) for each document. The two query engines are then converted into tools that are passed to an OpenAI function calling agent.\\nThis document agent can dynamically choose to perform semantic search or summarization within a given document.\\nWe create a separate document agent for each city.\\n\\ninput:\\n```python\\nfrom llama_index.agent import OpenAIAgent\\nfrom llama_index import load_index_from_storage, StorageContext\\nfrom llama_index.tools import QueryEngineTool, ToolMetadata\\nfrom llama_index.node_parser import SentenceSplitter\\nimport os\\nfrom tqdm.notebook import tqdm\\nimport pickle\\n\\n\\nasync def build_agent_per_doc(nodes, file_base):\\n    print(file_base)\\n\\n    vi_out_path = f\"./data/llamaindex_docs/{file_base}\"\\n    summary_out_path = f\"./data/llamaindex_docs/{file_base}_summary.pkl\"\\n    if not os.path.exists(vi_out_path):\\n        Path(\"./data/llamaindex_docs/\").mkdir(parents=True, exist_ok=True)\\n        # build vector index\\n        vector_index = VectorStoreIndex(nodes, service_context=service_context)\\n        vector_index.storage_context.persist(persist_dir=vi_out_path)\\n    else:\\n        vector_index = load_index_from_storage(\\n            StorageContext.from_defaults(persist_dir=vi_out_path),\\n            service_context=service_context,\\n        )\\n\\n    # build summary index\\n    summary_index = SummaryIndex(nodes, service_context=service_context)\\n\\n    # define query engines\\n    vector_query_engine = vector_index.as_query_engine()\\n    summary_query_engine = summary_index.as_query_engine(\\n        response_mode=\"tree_summarize\"\\n    )\\n\\n    # extract a summary\\n    if not os.path.exists(summary_out_path):\\n        Path(summary_out_path).parent.mkdir(parents=True, exist_ok=True)\\n        summary = str(\\n            await summary_query_engine.aquery(\\n                \"Extract a concise 1-2 line summary of this document\"\\n            )\\n        )\\n        pickle.dump(summary, open(summary_out_path, \"wb\"))\\n    else:\\n        summary = pickle.load(open(summary_out_path, \"rb\"))\\n\\n    # define tools\\n    query_engine_tools = [\\n        QueryEngineTool(\\n            query_engine=vector_query_engine,\\n            metadata=ToolMetadata(\\n                name=f\"vector_tool_{file_base}\",\\n                description=f\"Useful for questions related to specific facts\",\\n            ),\\n        ),\\n        QueryEngineTool(\\n            query_engine=summary_query_engine,\\n            metadata=ToolMetadata(\\n                name=f\"summary_tool_{file_base}\",\\n                description=f\"Useful for summarization questions\",\\n            ),\\n        ),\\n    ]\\n\\n    # build agent\\n    function_llm = OpenAI(model=\"gpt-4\")\\n    agent = OpenAIAgent.from_tools(\\n        query_engine_tools,\\n        llm=function_llm,\\n        verbose=True,\\n        system_prompt=f\"\"\"\\\\\\nYou are a specialized agent designed to answer queries about the `{file_base}.html` part of the LlamaIndex docs.\\nYou must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\\\\n\"\"\",\\n    )\\n\\n    return agent, summary\\n\\n\\nasync def build_agents(docs):\\n    node_parser = SentenceSplitter()\\n\\n    # Build agents dictionary\\n    agents_dict = {}\\n    extra_info_dict = {}\\n\\n    # # this is for the baseline\\n    # all_nodes = []\\n\\n    for idx, doc in enumerate(tqdm(docs)):\\n        nodes = node_parser.get_nodes_from_documents([doc])\\n        # all_nodes.extend(nodes)\\n\\n        # ID will be base + parent\\n        file_path = Path(doc.metadata[\"path\"])\\n        file_base = str(file_path.parent.stem) + \"_\" + str(file_path.stem)\\n        agent, summary = await build_agent_per_doc(nodes, file_base)\\n\\n        agents_dict[file_base] = agent\\n        extra_info_dict[file_base] = {\"summary\": summary, \"nodes\": nodes}\\n\\n    return agents_dict, extra_info_dict\\n```\\noutput:\\ninput:\\n```python\\nagents_dict, extra_info_dict = await build_agents(docs)\\n```\\noutput:\\n'],\n",
       " ['Async execution: Here we try another query with async execution\\n\\ninput:\\n```python\\n# Try another query with async execution\\n\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n\\nresponse = await agent.achat(\\n    \"Compare and contrast the risks of Uber and Lyft in 2021, then give an\"\\n    \" analysis\"\\n)\\nprint(str(response))\\n```\\noutput:\\n',\n",
       "  'Compare gpt-3.5-turbo vs. gpt-3.5-turbo-instruct\\nWe compare the performance of the two agents in being able to answer some complex queries.\\n\\nTaking a look at a turbo-instruct agent\\n\\ninput:\\n```python\\nllm_instruct = OpenAI(model=\"gpt-3.5-turbo-instruct\")\\nagent_instruct = ReActAgent.from_tools(\\n    query_engine_tools, llm=llm_instruct, verbose=True\\n)\\n```\\noutput:\\ninput:\\n```python\\nresponse = agent_instruct.chat(\"What was Lyft\\'s revenue growth in 2021?\")\\nprint(str(response))\\n```\\noutput:\\n',\n",
       "  'Download Data\\n\\ninput:\\n```python\\n!mkdir -p \\'data/10k/\\'\\n!wget \\'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf\\' -O \\'data/10k/uber_2021.pdf\\'\\n!wget \\'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/lyft_2021.pdf\\' -O \\'data/10k/lyft_2021.pdf\\'\\n```\\noutput:\\ninput:\\n```python\\nif not index_loaded:\\n    # load data\\n    lyft_docs = SimpleDirectoryReader(\\n        input_files=[\"./data/10k/lyft_2021.pdf\"]\\n    ).load_data()\\n    uber_docs = SimpleDirectoryReader(\\n        input_files=[\"./data/10k/uber_2021.pdf\"]\\n    ).load_data()\\n\\n    # build index\\n    lyft_index = VectorStoreIndex.from_documents(lyft_docs)\\n    uber_index = VectorStoreIndex.from_documents(uber_docs)\\n\\n    # persist index\\n    lyft_index.storage_context.persist(persist_dir=\"./storage/lyft\")\\n    uber_index.storage_context.persist(persist_dir=\"./storage/uber\")\\n```\\noutput:\\ninput:\\n```python\\nlyft_engine = lyft_index.as_query_engine(similarity_top_k=3)\\nuber_engine = uber_index.as_query_engine(similarity_top_k=3)\\n```\\noutput:\\ninput:\\n```python\\nquery_engine_tools = [\\n    QueryEngineTool(\\n        query_engine=lyft_engine,\\n        metadata=ToolMetadata(\\n            name=\"lyft_10k\",\\n            description=(\\n                \"Provides information about Lyft financials for year 2021. \"\\n                \"Use a detailed plain text question as input to the tool.\"\\n            ),\\n        ),\\n    ),\\n    QueryEngineTool(\\n        query_engine=uber_engine,\\n        metadata=ToolMetadata(\\n            name=\"uber_10k\",\\n            description=(\\n                \"Provides information about Uber financials for year 2021. \"\\n                \"Use a detailed plain text question as input to the tool.\"\\n            ),\\n        ),\\n    ),\\n]\\n```\\noutput:\\n',\n",
       "  'Run Some Example Queries\\nWe run some example queries using the agent, showcasing some of the agent\\'s abilities to do chain-of-thought-reasoning and tool use to synthesize the right answer.\\nWe also show queries.\\n\\ninput:\\n```python\\nresponse = agent.chat(\\n    \"Compare and contrast the revenue growth of Uber and Lyft in 2021, then\"\\n    \" give an analysis\"\\n)\\nprint(str(response))\\n```\\noutput:\\n',\n",
       "  'Setup ReAct Agent\\nHere we setup two ReAct agents: one powered by standard gpt-3.5-turbo, and the other powered by gpt-3.5-turbo-instruct.\\n\\ninput:\\n```python\\nfrom llama_index.agent import ReActAgent\\nfrom llama_index.llms import OpenAI\\n```\\noutput:\\ninput:\\n```python\\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\\nagent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)\\n```\\noutput:\\ninput:\\n```python\\nresponse = agent.chat(\"What was Lyft\\'s revenue growth in 2021?\")\\nprint(str(response))\\n```\\noutput:\\n'],\n",
       " [\"Download Data\\n\\ninput:\\n```python\\n!mkdir -p 'data/paul_graham/'\\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'\\n```\\noutput:\\n\",\n",
       "  'Get started in 5 lines of code\\n\\nLoad data and build index\\n\\ninput:\\n```python\\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\\nfrom llama_index.llms import OpenAI, Anthropic\\n\\nservice_context = ServiceContext.from_defaults(llm=OpenAI())\\ndata = SimpleDirectoryReader(input_dir=\"./data/paul_graham/\").load_data()\\nindex = VectorStoreIndex.from_documents(data, service_context=service_context)\\n```\\noutput:\\n',\n",
       "  \"\\n\\nChat Engine - ReAct Agent Mode\\n\\nReAct is an agent based chat mode built on top of a query engine over your data.\\n\\nFor each chat interaction, the agent enter a ReAct loop:\\n\\nfirst decide whether to use the query engine tool and come up with appropriate input\\n(optional) use the query engine tool and observe its output\\ndecide whether to repeat or give final response\\n\\n\\nThis approach is flexible, since it can flexibility choose between querying the knowledge base or not.\\nHowever, the performance is also more dependent on the quality of the LLM.\\nYou might need to do more coercing to make sure it chooses to query the knowledge base at right times, instead of hallucinating an answer.\\n\\nIf you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.\\n\\ninput:\\n```python\\n!pip install llama-index\\n```\\noutput:\\n\"],\n",
       " ['Define Assistant Agent\\n\\ninput:\\n```python\\nfrom llama_index.agent import OpenAIAssistantAgent\\n\\nagent = OpenAIAssistantAgent.from_new(\\n    name=\"QA bot\",\\n    instructions=\"You are a bot designed to answer questions about the author\",\\n    openai_tools=[],\\n    tools=[summary_tool, vector_tool],\\n    verbose=True,\\n    run_retrieve_sleep_time=1.0,\\n)\\n```\\noutput:\\n',\n",
       "  'Results: A bit flaky\\n\\ninput:\\n```python\\nresponse = agent.chat(\"Can you give me a summary about the author\\'s life?\")\\nprint(str(response))\\n```\\noutput:\\ninput:\\n```python\\nresponse = agent.query(\"What did the author do after RICS?\")\\nprint(str(response))\\n```\\noutput:\\n',\n",
       "  'Setup Vector + Summary Indexes/Query Engines/Tools\\n\\ninput:\\n```python\\nfrom llama_index.llms import OpenAI\\nfrom llama_index import (\\n    ServiceContext,\\n    StorageContext,\\n    SummaryIndex,\\n    VectorStoreIndex,\\n)\\n\\n# initialize service context (set chunk size)\\nllm = OpenAI()\\nservice_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm)\\nnodes = service_context.node_parser.get_nodes_from_documents(documents)\\n\\n# initialize storage context (by default it\\'s in-memory)\\nstorage_context = StorageContext.from_defaults()\\nstorage_context.docstore.add_documents(nodes)\\n\\n# Define Summary Index and Vector Index over Same Data\\nsummary_index = SummaryIndex(nodes, storage_context=storage_context)\\nvector_index = VectorStoreIndex(nodes, storage_context=storage_context)\\n\\n# define query engines\\nsummary_query_engine = summary_index.as_query_engine(\\n    response_mode=\"tree_summarize\",\\n    use_async=True,\\n)\\nvector_query_engine = vector_index.as_query_engine()\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index.tools.query_engine import QueryEngineTool\\n\\nsummary_tool = QueryEngineTool.from_defaults(\\n    query_engine=summary_query_engine,\\n    name=\"summary_tool\",\\n    description=(\\n        \"Useful for summarization questions related to the author\\'s life\"\\n    ),\\n)\\n\\nvector_tool = QueryEngineTool.from_defaults(\\n    query_engine=vector_query_engine,\\n    name=\"vector_tool\",\\n    description=(\\n        \"Useful for retrieving specific context to answer specific questions about the author\\'s life\"\\n    ),\\n)\\n```\\noutput:\\n',\n",
       "  'Define Function Tool\\nHere we define the function interface, which is passed to OpenAI to perform auto-retrieval.\\nWe were not able to get OpenAI to work with nested pydantic objects or tuples as arguments,\\nso we converted the metadata filter keys and values into lists for the function API to work with.\\n\\ninput:\\n```python\\n# define function tool\\nfrom llama_index.tools import FunctionTool\\nfrom llama_index.vector_stores.types import (\\n    VectorStoreInfo,\\n    MetadataInfo,\\n    ExactMatchFilter,\\n    MetadataFilters,\\n)\\nfrom llama_index.retrievers import VectorIndexRetriever\\nfrom llama_index.query_engine import RetrieverQueryEngine\\n\\nfrom typing import List, Tuple, Any\\nfrom pydantic import BaseModel, Field\\n\\n# hardcode top k for now\\ntop_k = 3\\n\\n# define vector store info describing schema of vector store\\nvector_store_info = VectorStoreInfo(\\n    content_info=\"brief biography of celebrities\",\\n    metadata_info=[\\n        MetadataInfo(\\n            name=\"category\",\\n            type=\"str\",\\n            description=(\\n                \"Category of the celebrity, one of [Sports, Entertainment,\"\\n                \" Business, Music]\"\\n            ),\\n        ),\\n        MetadataInfo(\\n            name=\"country\",\\n            type=\"str\",\\n            description=(\\n                \"Country of the celebrity, one of [United States, Barbados,\"\\n                \" Portugal]\"\\n            ),\\n        ),\\n    ],\\n)\\n\\n\\n# define pydantic model for auto-retrieval function\\nclass AutoRetrieveModel(BaseModel):\\n    query: str = Field(..., description=\"natural language query string\")\\n    filter_key_list: List[str] = Field(\\n        ..., description=\"List of metadata filter field names\"\\n    )\\n    filter_value_list: List[str] = Field(\\n        ...,\\n        description=(\\n            \"List of metadata filter field values (corresponding to names\"\\n            \" specified in filter_key_list)\"\\n        ),\\n    )\\n\\n\\ndef auto_retrieve_fn(\\n    query: str, filter_key_list: List[str], filter_value_list: List[str]\\n):\\n    \"\"\"Auto retrieval function.\\n\\n    Performs auto-retrieval from a vector database, and then applies a set of filters.\\n\\n    \"\"\"\\n    query = query or \"Query\"\\n\\n    exact_match_filters = [\\n        ExactMatchFilter(key=k, value=v)\\n        for k, v in zip(filter_key_list, filter_value_list)\\n    ]\\n    retriever = VectorIndexRetriever(\\n        index,\\n        filters=MetadataFilters(filters=exact_match_filters),\\n        top_k=top_k,\\n    )\\n    results = retriever.retrieve(query)\\n    return [r.get_content() for r in results]\\n\\n\\ndescription = f\"\"\"\\\\\\nUse this tool to look up biographical information about celebrities.\\nThe vector database schema is given below:\\n{vector_store_info.json()}\\n\"\"\"\\n\\nauto_retrieve_tool = FunctionTool.from_defaults(\\n    fn=auto_retrieve_fn,\\n    name=\"celebrity_bios\",\\n    description=description,\\n    fn_schema=AutoRetrieveModel,\\n)\\n```\\noutput:\\ninput:\\n```python\\nauto_retrieve_fn(\\n    \"celebrity from the United States\",\\n    filter_key_list=[\"country\"],\\n    filter_value_list=[\"United States\"],\\n)\\n```\\noutput:\\n',\n",
       "  'AutoRetrieval from a Vector Database\\nOur existing \"auto-retrieval\" capabilities (in VectorIndexAutoRetriever) allow an LLM to infer the right query parameters for a vector database - including both the query string and metadata filter.\\nSince the Assistant API can call functions + infer function parameters, we explore its capabilities in performing auto-retrieval here.\\n\\nIf you\\'re opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.\\n\\ninput:\\n```python\\nimport pinecone\\nimport os\\n\\napi_key = os.environ[\"PINECONE_API_KEY\"]\\npinecone.init(api_key=api_key, environment=\"us-west1-gcp\")\\n```\\noutput:\\ninput:\\n```python\\n# dimensions are for text-embedding-ada-002\\ntry:\\n    pinecone.create_index(\\n        \"quickstart\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\"\\n    )\\nexcept Exception:\\n    # most likely index already exists\\n    pass\\n```\\noutput:\\ninput:\\n```python\\npinecone_index = pinecone.Index(\"quickstart\")\\n```\\noutput:\\ninput:\\n```python\\n# Optional: delete data in your pinecone index\\npinecone_index.delete(deleteAll=True, namespace=\"test\")\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index import VectorStoreIndex, StorageContext\\nfrom llama_index.vector_stores import PineconeVectorStore\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index.schema import TextNode\\n\\nnodes = [\\n    TextNode(\\n        text=(\\n            \"Michael Jordan is a retired professional basketball player,\"\\n            \" widely regarded as one of the greatest basketball players of all\"\\n            \" time.\"\\n        ),\\n        metadata={\\n            \"category\": \"Sports\",\\n            \"country\": \"United States\",\\n        },\\n    ),\\n    TextNode(\\n        text=(\\n            \"Angelina Jolie is an American actress, filmmaker, and\"\\n            \" humanitarian. She has received numerous awards for her acting\"\\n            \" and is known for her philanthropic work.\"\\n        ),\\n        metadata={\\n            \"category\": \"Entertainment\",\\n            \"country\": \"United States\",\\n        },\\n    ),\\n    TextNode(\\n        text=(\\n            \"Elon Musk is a business magnate, industrial designer, and\"\\n            \" engineer. He is the founder, CEO, and lead designer of SpaceX,\"\\n            \" Tesla, Inc., Neuralink, and The Boring Company.\"\\n        ),\\n        metadata={\\n            \"category\": \"Business\",\\n            \"country\": \"United States\",\\n        },\\n    ),\\n    TextNode(\\n        text=(\\n            \"Rihanna is a Barbadian singer, actress, and businesswoman. She\"\\n            \" has achieved significant success in the music industry and is\"\\n            \" known for her versatile musical style.\"\\n        ),\\n        metadata={\\n            \"category\": \"Music\",\\n            \"country\": \"Barbados\",\\n        },\\n    ),\\n    TextNode(\\n        text=(\\n            \"Cristiano Ronaldo is a Portuguese professional footballer who is\"\\n            \" considered one of the greatest football players of all time. He\"\\n            \" has won numerous awards and set multiple records during his\"\\n            \" career.\"\\n        ),\\n        metadata={\\n            \"category\": \"Sports\",\\n            \"country\": \"Portugal\",\\n        },\\n    ),\\n]\\n```\\noutput:\\ninput:\\n```python\\nvector_store = PineconeVectorStore(\\n    pinecone_index=pinecone_index, namespace=\"test\"\\n)\\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\\n```\\noutput:\\ninput:\\n```python\\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\\n```\\noutput:\\n'],\n",
       " ['OpenAI Function Agent with a Query Plan Tool\\nUse OpenAIAgent, built on top of the OpenAI tool use interface.\\nFeed it our QueryPlanTool, which is a Tool that takes in other tools. And the agent to generate a query plan DAG over these tools.\\n\\ninput:\\n```python\\nfrom llama_index.tools import QueryEngineTool\\n\\n\\nquery_tool_sept = QueryEngineTool.from_defaults(\\n    query_engine=sept_engine,\\n    name=\"sept_2022\",\\n    description=(\\n        f\"Provides information about Uber quarterly financials ending\"\\n        f\" September 2022\"\\n    ),\\n)\\nquery_tool_june = QueryEngineTool.from_defaults(\\n    query_engine=june_engine,\\n    name=\"june_2022\",\\n    description=(\\n        f\"Provides information about Uber quarterly financials ending June\"\\n        f\" 2022\"\\n    ),\\n)\\nquery_tool_march = QueryEngineTool.from_defaults(\\n    query_engine=march_engine,\\n    name=\"march_2022\",\\n    description=(\\n        f\"Provides information about Uber quarterly financials ending March\"\\n        f\" 2022\"\\n    ),\\n)\\n```\\noutput:\\ninput:\\n```python\\n# define query plan tool\\nfrom llama_index.tools import QueryPlanTool\\nfrom llama_index import get_response_synthesizer\\n\\nresponse_synthesizer = get_response_synthesizer(\\n    service_context=service_context\\n)\\nquery_plan_tool = QueryPlanTool.from_defaults(\\n    query_engine_tools=[query_tool_sept, query_tool_june, query_tool_march],\\n    response_synthesizer=response_synthesizer,\\n)\\n```\\noutput:\\ninput:\\n```python\\nquery_plan_tool.metadata.to_openai_tool()  # to_openai_function() deprecated\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index.agent import OpenAIAgent\\nfrom llama_index.llms import OpenAI\\n\\n\\nagent = OpenAIAgent.from_tools(\\n    [query_plan_tool],\\n    max_function_calls=10,\\n    llm=OpenAI(temperature=0, model=\"gpt-4-0613\"),\\n    verbose=True,\\n)\\n```\\noutput:\\ninput:\\n```python\\nresponse = agent.query(\"What were the risk factors in sept 2022?\")\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index.tools.query_plan import QueryPlan, QueryNode\\n\\nquery_plan = QueryPlan(\\n    nodes=[\\n        QueryNode(\\n            id=1,\\n            query_str=\"risk factors\",\\n            tool_name=\"sept_2022\",\\n            dependencies=[],\\n        )\\n    ]\\n)\\n```\\noutput:\\ninput:\\n```python\\nQueryPlan.schema()\\n```\\noutput:\\ninput:\\n```python\\nresponse = agent.query(\\n    \"Analyze Uber revenue growth in March, June, and September\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nprint(str(response))\\n```\\noutput:\\ninput:\\n```python\\nresponse = agent.query(\\n    \"Analyze changes in risk factors in march, june, and september for Uber\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nprint(str(response))\\n```\\noutput:\\ninput:\\n```python\\n# response = agent.query(\"Analyze both Uber revenue growth and risk factors over march, june, and september\")\\n```\\noutput:\\ninput:\\n```python\\nprint(str(response))\\n```\\noutput:\\ninput:\\n```python\\nresponse = agent.query(\\n    \"First look at Uber\\'s revenue growth and risk factors in March, \"\\n    + \"then revenue growth and risk factors in September, and then compare and\"\\n    \" contrast the two documents?\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nresponse\\n```\\noutput:\\n',\n",
       "  'Load data\\n\\ninput:\\n```python\\nmarch_2022 = SimpleDirectoryReader(\\n    input_files=[\"./data/10q/uber_10q_march_2022.pdf\"]\\n).load_data()\\njune_2022 = SimpleDirectoryReader(\\n    input_files=[\"./data/10q/uber_10q_june_2022.pdf\"]\\n).load_data()\\nsept_2022 = SimpleDirectoryReader(\\n    input_files=[\"./data/10q/uber_10q_sept_2022.pdf\"]\\n).load_data()\\n```\\noutput:\\n',\n",
       "  'Build indices\\nWe build a vector index / query engine over each of the documents (March, June, September).\\n\\ninput:\\n```python\\nmarch_index = GPTVectorStoreIndex.from_documents(march_2022)\\njune_index = GPTVectorStoreIndex.from_documents(june_2022)\\nsept_index = GPTVectorStoreIndex.from_documents(sept_2022)\\n```\\noutput:\\ninput:\\n```python\\nmarch_engine = march_index.as_query_engine(\\n    similarity_top_k=3, service_context=service_context\\n)\\njune_engine = june_index.as_query_engine(\\n    similarity_top_k=3, service_context=service_context\\n)\\nsept_engine = sept_index.as_query_engine(\\n    similarity_top_k=3, service_context=service_context\\n)\\n```\\noutput:\\n']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"For the user query, match the best use case or insight from a code base.\n",
    "user query: How to use agents\"\"\"\n",
    "get_docs(query, vs, G, top_k=5, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerank\n",
    "\n",
    "import nltk\n",
    "\n",
    "# tokinize\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# rank the documents according to the tokenized query\n",
    "def rank_docs(docs, query):\n",
    "    query_tokens = tokenize(query)\n",
    "    scores = []\n",
    "    for doc in docs:\n",
    "        doc_tokens = tokenize(doc)\n",
    "        score = 0\n",
    "        for token in query_tokens:\n",
    "            score += doc_tokens.count(token)\n",
    "        scores.append(score)\n",
    "    reranked_docs = [x for _,x in sorted(zip(scores,docs), reverse=True)]\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fuse the Indexes!\\nIn this step, we fuse our indexes into a single retriever. This retriever will also generate augment our query by generating extra queries related to the original question, and aggregate the results.\\nThis setup will query 4 times, once with your original query, and generate 3 more queries.\\nBy default, it uses the following prompt to generate extra queries:\\nQUERY_GEN_PROMPT = (\\n    \"You are a helpful assistant that generates multiple search queries based on a \"\\n    \"single input query. Generate {num_queries} search queries, one on each line, \"\\n    \"related to the following input query:\\\\n\"\\n    \"Query: {query}\\\\n\"\\n    \"Queries:\\\\n\"\\n)\\n\\n\\ninput:\\n```python\\nfrom llama_index.retrievers import QueryFusionRetriever\\n\\nretriever = QueryFusionRetriever(\\n    [index_1.as_retriever(), index_2.as_retriever()],\\n    similarity_top_k=2,\\n    num_queries=4,  # set this to 1 to disable query generation\\n    use_async=True,\\n    verbose=True,\\n    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\\n)\\n```\\noutput:\\ninput:\\n```python\\n# apply nested async to run in a notebook\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n```\\noutput:\\ninput:\\n```python\\nnodes_with_scores = retriever.retrieve(\"How do I setup a chroma vector store?\")\\n```\\noutput:\\ninput:\\n```python\\nfor node in nodes_with_scores:\\n    print(f\"Score: {node.score:.2f} - {node.text[:100]}...\")\\n```\\noutput:\\n',\n",
       " 'Setup\\nFor this notebook, we will use two very similar pages of our documentation, each stored in a separaete index.\\n\\ninput:\\n```python\\nfrom llama_index import SimpleDirectoryReader\\n\\ndocuments_1 = SimpleDirectoryReader(\\n    input_files=[\"../../community/integrations/vector_stores.md\"]\\n).load_data()\\ndocuments_2 = SimpleDirectoryReader(\\n    input_files=[\"../../core_modules/data_modules/storage/vector_stores.md\"]\\n).load_data()\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index import VectorStoreIndex\\n\\nindex_1 = VectorStoreIndex.from_documents(documents_1)\\nindex_2 = VectorStoreIndex.from_documents(documents_2)\\n```\\noutput:\\n',\n",
       " 'Simple Fusion Retriever\\nIn this example, we walk through how you can combine retrieval results from multiple queries and multiple indexes.\\nThe retrieved nodes will be returned as the top-k across all queries and indexes, as well as handling de-duplication of any nodes.\\n\\ninput:\\n```python\\nimport os\\nimport openai\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\\n```\\noutput:\\n',\n",
       " 'Setup\\n\\nIf you\\'re opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.\\n\\nDownload Data\\n\\ninput:\\n```python\\n!mkdir -p \\'data/paul_graham/\\'\\n!wget \\'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\\' -O \\'data/paul_graham/paul_graham_essay.txt\\'\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index import SimpleDirectoryReader\\n\\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\\n```\\noutput:\\n',\n",
       " 'Next, we will setup a vector index over the documentation.\\n\\ninput:\\n```python\\nfrom llama_index import VectorStoreIndex, ServiceContext\\n\\nservice_context = ServiceContext.from_defaults(chunk_size=256)\\n\\nindex = VectorStoreIndex.from_documents(\\n    documents, service_context=service_context\\n)\\n```\\noutput:\\n',\n",
       " 'Create a Hybrid Fusion Retriever\\nIn this step, we fuse our index with a BM25 based retriever. This will enable us to capture both semantic relations and keywords in our input queries.\\nSince both of these retrievers calculate a score, we can use the reciprocal rerank algorithm to re-sort our nodes without using an additional models or excessive computation.\\nThis setup will also query 4 times, once with your original query, and generate 3 more queries.\\nBy default, it uses the following prompt to generate extra queries:\\nQUERY_GEN_PROMPT = (\\n    \"You are a helpful assistant that generates multiple search queries based on a \"\\n    \"single input query. Generate {num_queries} search queries, one on each line, \"\\n    \"related to the following input query:\\\\n\"\\n    \"Query: {query}\\\\n\"\\n    \"Queries:\\\\n\"\\n)\\n\\n\\nFirst, we create our retrievers. Each will retrieve the top-2 most similar nodes:\\n\\ninput:\\n```python\\nfrom llama_index.retrievers import BM25Retriever\\n\\nvector_retriever = index.as_retriever(similarity_top_k=2)\\n\\nbm25_retriever = BM25Retriever.from_defaults(\\n    docstore=index.docstore, similarity_top_k=2\\n)\\n```\\noutput:\\n',\n",
       " 'As we can see, both retruned nodes correctly mention Viaweb and Interleaf!\\n\\nUse in a Query Engine!\\nNow, we can plug our retriever into a query engine to synthesize natural language responses.\\n\\ninput:\\n```python\\nfrom llama_index.query_engine import RetrieverQueryEngine\\n\\nquery_engine = RetrieverQueryEngine.from_args(retriever)\\n```\\noutput:\\ninput:\\n```python\\nresponse = query_engine.query(\"What happened at Interleafe and Viaweb?\")\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index.response.notebook_utils import display_response\\n\\ndisplay_response(response)\\n```\\noutput:\\n',\n",
       " 'Next, we can create our fusion retriever, which well return the top-2 most similar nodes from the 4 returned nodes from the retrievers:\\n\\ninput:\\n```python\\nfrom llama_index.retrievers import QueryFusionRetriever\\n\\nretriever = QueryFusionRetriever(\\n    [vector_retriever, bm25_retriever],\\n    similarity_top_k=2,\\n    num_queries=4,  # set this to 1 to disable query generation\\n    mode=\"reciprocal_rerank\",\\n    use_async=True,\\n    verbose=True,\\n    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\\n)\\n```\\noutput:\\ninput:\\n```python\\n# apply nested async to run in a notebook\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n```\\noutput:\\ninput:\\n```python\\nnodes_with_scores = retriever.retrieve(\\n    \"What happened at Interleafe and Viaweb?\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nfor node in nodes_with_scores:\\n    print(f\"Score: {node.score:.2f} - {node.text}...\\\\n-----\\\\n\")\\n```\\noutput:\\n',\n",
       " 'Retrieved context is irrelevant and response is hallucinated.\\n\\ninput:\\n```python\\npprint_response(response)\\n```\\noutput:\\n',\n",
       " 'Retrieve top 10 most relevant nodes, then filter with Cohere Rerank\\n\\ninput:\\n```python\\nimport os\\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\\n\\n\\napi_key = os.environ[\"COHERE_API_KEY\"]\\ncohere_rerank = CohereRerank(api_key=api_key, top_n=2)\\n```\\noutput:\\ninput:\\n```python\\nquery_engine = index.as_query_engine(\\n    similarity_top_k=10,\\n    node_postprocessors=[cohere_rerank],\\n)\\nresponse = query_engine.query(\\n    \"What did Sam Altman do in this essay?\",\\n)\\n```\\noutput:\\ninput:\\n```python\\npprint_response(response)\\n```\\noutput:\\n',\n",
       " 'Directly retrieve top 2 most similar nodes\\n\\ninput:\\n```python\\nquery_engine = index.as_query_engine(\\n    similarity_top_k=2,\\n)\\nresponse = query_engine.query(\\n    \"What did Sam Altman do in this essay?\",\\n)\\n```\\noutput:\\n',\n",
       " 'Download Data\\n\\ninput:\\n```python\\n!mkdir -p \\'data/paul_graham/\\'\\n!wget \\'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\\' -O \\'data/paul_graham/paul_graham_essay.txt\\'\\n```\\noutput:\\ninput:\\n```python\\n# load documents\\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\\n\\n# build index\\nindex = VectorStoreIndex.from_documents(documents=documents)\\n```\\noutput:\\n',\n",
       " 'Next, we will setup a vector index over the documentation.\\n\\ninput:\\n```python\\nfrom llama_index import VectorStoreIndex, ServiceContext\\n\\nservice_context = ServiceContext.from_defaults(chunk_size=256)\\n\\nindex = VectorStoreIndex.from_documents(\\n    documents, service_context=service_context\\n)\\n```\\noutput:\\n',\n",
       " 'Create a Hybrid Fusion Retriever\\nIn this step, we fuse our index with a BM25 based retriever. This will enable us to capture both semantic relations and keywords in our input queries.\\nSince both of these retrievers calculate a score, we can use the reciprocal rerank algorithm to re-sort our nodes without using an additional models or excessive computation.\\nThis setup will also query 4 times, once with your original query, and generate 3 more queries.\\nBy default, it uses the following prompt to generate extra queries:\\nQUERY_GEN_PROMPT = (\\n    \"You are a helpful assistant that generates multiple search queries based on a \"\\n    \"single input query. Generate {num_queries} search queries, one on each line, \"\\n    \"related to the following input query:\\\\n\"\\n    \"Query: {query}\\\\n\"\\n    \"Queries:\\\\n\"\\n)\\n\\n\\nFirst, we create our retrievers. Each will retrieve the top-2 most similar nodes:\\n\\ninput:\\n```python\\nfrom llama_index.retrievers import BM25Retriever\\n\\nvector_retriever = index.as_retriever(similarity_top_k=2)\\n\\nbm25_retriever = BM25Retriever.from_defaults(\\n    docstore=index.docstore, similarity_top_k=2\\n)\\n```\\noutput:\\n',\n",
       " 'As we can see, both retruned nodes correctly mention Viaweb and Interleaf!\\n\\nUse in a Query Engine!\\nNow, we can plug our retriever into a query engine to synthesize natural language responses.\\n\\ninput:\\n```python\\nfrom llama_index.query_engine import RetrieverQueryEngine\\n\\nquery_engine = RetrieverQueryEngine.from_args(retriever)\\n```\\noutput:\\ninput:\\n```python\\nresponse = query_engine.query(\"What happened at Interleafe and Viaweb?\")\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index.response.notebook_utils import display_response\\n\\ndisplay_response(response)\\n```\\noutput:\\n',\n",
       " 'Next, we can create our fusion retriever, which well return the top-2 most similar nodes from the 4 returned nodes from the retrievers:\\n\\ninput:\\n```python\\nfrom llama_index.retrievers import QueryFusionRetriever\\n\\nretriever = QueryFusionRetriever(\\n    [vector_retriever, bm25_retriever],\\n    similarity_top_k=2,\\n    num_queries=4,  # set this to 1 to disable query generation\\n    mode=\"reciprocal_rerank\",\\n    use_async=True,\\n    verbose=True,\\n    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\\n)\\n```\\noutput:\\ninput:\\n```python\\n# apply nested async to run in a notebook\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n```\\noutput:\\ninput:\\n```python\\nnodes_with_scores = retriever.retrieve(\\n    \"What happened at Interleafe and Viaweb?\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nfor node in nodes_with_scores:\\n    print(f\"Score: {node.score:.2f} - {node.text}...\\\\n-----\\\\n\")\\n```\\noutput:\\n',\n",
       " 'Building Response Synthesis with LLMs\\nIn this section we\\'ll show how to use LLMs + Prompts to build a response synthesis module.\\nWe\\'ll start from simple strategies (simply stuffing context into a prompt), to more advanced strategies that can handle context overflows.\\n\\n1. Try a Simple Prompt\\nWe first try to synthesize the response using a single input prompt + LLM call.\\n\\ninput:\\n```python\\nfrom llama_index.llms import OpenAI\\nfrom llama_index.prompts import PromptTemplate\\n\\nllm = OpenAI(model=\"text-davinci-003\")\\n```\\noutput:\\ninput:\\n```python\\nqa_prompt = PromptTemplate(\\n    \"\"\"\\\\\\nContext information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: \\\\\\n\"\"\"\\n)\\n```\\noutput:\\n',\n",
       " 'Build Pinecone Index, Get Retriever\\nWe use our high-level LlamaIndex abstractions to 1) ingest data into Pinecone, and then 2) get a vector retriever.\\nNote that we set chunk sizes to 1024.\\n\\ninput:\\n```python\\nimport pinecone\\nimport os\\n\\napi_key = os.environ[\"PINECONE_API_KEY\"]\\npinecone.init(api_key=api_key, environment=\"us-west1-gcp\")\\n```\\noutput:\\ninput:\\n```python\\n# dimensions are for text-embedding-ada-002\\npinecone.create_index(\\n    \"quickstart\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\"\\n)\\n```\\noutput:\\ninput:\\n```python\\npinecone_index = pinecone.Index(\"quickstart\")\\n```\\noutput:\\ninput:\\n```python\\n# [Optional] drop contents in index\\npinecone_index.delete(deleteAll=True)\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index.vector_stores import PineconeVectorStore\\nfrom llama_index import VectorStoreIndex, ServiceContext\\nfrom llama_index.storage import StorageContext\\n```\\noutput:\\ninput:\\n```python\\nvector_store = PineconeVectorStore(pinecone_index=pinecone_index)\\n# NOTE: set chunk size of 1024\\nservice_context = ServiceContext.from_defaults(chunk_size=1024)\\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\\nindex = VectorStoreIndex.from_documents(\\n    documents, service_context=service_context, storage_context=storage_context\\n)\\n```\\noutput:\\ninput:\\n```python\\nretriever = index.as_retriever()\\n```\\noutput:\\n',\n",
       " 'Given an example question, retrieve the set of relevant nodes and try to put it all in the prompt, separated by newlines.\\n\\ninput:\\n```python\\nquery_str = (\\n    \"Can you tell me about results from RLHF using both model-based and\"\\n    \" human-based evaluation?\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nretrieved_nodes = retriever.retrieve(query_str)\\n```\\noutput:\\ninput:\\n```python\\ndef generate_response(retrieved_nodes, query_str, qa_prompt, llm):\\n    context_str = \"\\\\n\\\\n\".join([r.get_content() for r in retrieved_nodes])\\n    fmt_qa_prompt = qa_prompt.format(\\n        context_str=context_str, query_str=query_str\\n    )\\n    response = llm.complete(fmt_qa_prompt)\\n    return str(response), fmt_qa_prompt\\n```\\noutput:\\ninput:\\n```python\\nresponse, fmt_qa_prompt = generate_response(\\n    retrieved_nodes, query_str, qa_prompt, llm\\n)\\n```\\noutput:\\ninput:\\n```python\\nprint(f\"*****Response******:\\\\n{response}\\\\n\\\\n\")\\n```\\noutput:\\ninput:\\n```python\\nprint(f\"*****Formatted Prompt*****:\\\\n{fmt_qa_prompt}\\\\n\\\\n\")\\n```\\noutput:\\n',\n",
       " 'Load Data\\n\\ninput:\\n```python\\n!mkdir data\\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\"\\n```\\noutput:\\ninput:\\n```python\\nfrom pathlib import Path\\nfrom llama_hub.file.pymu_pdf.base import PyMuPDFReader\\n```\\noutput:\\ninput:\\n```python\\nloader = PyMuPDFReader()\\ndocuments = loader.load(file_path=\"./data/llama2.pdf\")\\n```\\noutput:\\n',\n",
       " 'Given an example question, get a retrieved set of nodes.\\nWe use the retriever to get a set of relevant nodes given a user query. These nodes will then be passed to the response synthesis modules below.\\n\\ninput:\\n```python\\nquery_str = (\\n    \"Can you tell me about results from RLHF using both model-based and\"\\n    \" human-based evaluation?\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nretrieved_nodes = retriever.retrieve(query_str)\\n```\\noutput:\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"Find the best matching usecase or insight.\n",
    "user query: What is fusion retrieval?\"\"\"\n",
    "\n",
    "docs = []\n",
    "for doc in get_docs(query, vs, G, top_k=5, max_depth=3):\n",
    "    docs.extend(doc)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fuse the Indexes!\\nIn this step, we fuse our indexes into a single retriever. This retriever will also generate augment our query by generating extra queries related to the original question, and aggregate the results.\\nThis setup will query 4 times, once with your original query, and generate 3 more queries.\\nBy default, it uses the following prompt to generate extra queries:\\nQUERY_GEN_PROMPT = (\\n    \"You are a helpful assistant that generates multiple search queries based on a \"\\n    \"single input query. Generate {num_queries} search queries, one on each line, \"\\n    \"related to the following input query:\\\\n\"\\n    \"Query: {query}\\\\n\"\\n    \"Queries:\\\\n\"\\n)\\n\\n\\ninput:\\n```python\\nfrom llama_index.retrievers import QueryFusionRetriever\\n\\nretriever = QueryFusionRetriever(\\n    [index_1.as_retriever(), index_2.as_retriever()],\\n    similarity_top_k=2,\\n    num_queries=4,  # set this to 1 to disable query generation\\n    use_async=True,\\n    verbose=True,\\n    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\\n)\\n```\\noutput:\\ninput:\\n```python\\n# apply nested async to run in a notebook\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n```\\noutput:\\ninput:\\n```python\\nnodes_with_scores = retriever.retrieve(\"How do I setup a chroma vector store?\")\\n```\\noutput:\\ninput:\\n```python\\nfor node in nodes_with_scores:\\n    print(f\"Score: {node.score:.2f} - {node.text[:100]}...\")\\n```\\noutput:\\n',\n",
       " 'Create a Hybrid Fusion Retriever\\nIn this step, we fuse our index with a BM25 based retriever. This will enable us to capture both semantic relations and keywords in our input queries.\\nSince both of these retrievers calculate a score, we can use the reciprocal rerank algorithm to re-sort our nodes without using an additional models or excessive computation.\\nThis setup will also query 4 times, once with your original query, and generate 3 more queries.\\nBy default, it uses the following prompt to generate extra queries:\\nQUERY_GEN_PROMPT = (\\n    \"You are a helpful assistant that generates multiple search queries based on a \"\\n    \"single input query. Generate {num_queries} search queries, one on each line, \"\\n    \"related to the following input query:\\\\n\"\\n    \"Query: {query}\\\\n\"\\n    \"Queries:\\\\n\"\\n)\\n\\n\\nFirst, we create our retrievers. Each will retrieve the top-2 most similar nodes:\\n\\ninput:\\n```python\\nfrom llama_index.retrievers import BM25Retriever\\n\\nvector_retriever = index.as_retriever(similarity_top_k=2)\\n\\nbm25_retriever = BM25Retriever.from_defaults(\\n    docstore=index.docstore, similarity_top_k=2\\n)\\n```\\noutput:\\n',\n",
       " 'Create a Hybrid Fusion Retriever\\nIn this step, we fuse our index with a BM25 based retriever. This will enable us to capture both semantic relations and keywords in our input queries.\\nSince both of these retrievers calculate a score, we can use the reciprocal rerank algorithm to re-sort our nodes without using an additional models or excessive computation.\\nThis setup will also query 4 times, once with your original query, and generate 3 more queries.\\nBy default, it uses the following prompt to generate extra queries:\\nQUERY_GEN_PROMPT = (\\n    \"You are a helpful assistant that generates multiple search queries based on a \"\\n    \"single input query. Generate {num_queries} search queries, one on each line, \"\\n    \"related to the following input query:\\\\n\"\\n    \"Query: {query}\\\\n\"\\n    \"Queries:\\\\n\"\\n)\\n\\n\\nFirst, we create our retrievers. Each will retrieve the top-2 most similar nodes:\\n\\ninput:\\n```python\\nfrom llama_index.retrievers import BM25Retriever\\n\\nvector_retriever = index.as_retriever(similarity_top_k=2)\\n\\nbm25_retriever = BM25Retriever.from_defaults(\\n    docstore=index.docstore, similarity_top_k=2\\n)\\n```\\noutput:\\n',\n",
       " 'Next, we can create our fusion retriever, which well return the top-2 most similar nodes from the 4 returned nodes from the retrievers:\\n\\ninput:\\n```python\\nfrom llama_index.retrievers import QueryFusionRetriever\\n\\nretriever = QueryFusionRetriever(\\n    [vector_retriever, bm25_retriever],\\n    similarity_top_k=2,\\n    num_queries=4,  # set this to 1 to disable query generation\\n    mode=\"reciprocal_rerank\",\\n    use_async=True,\\n    verbose=True,\\n    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\\n)\\n```\\noutput:\\ninput:\\n```python\\n# apply nested async to run in a notebook\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n```\\noutput:\\ninput:\\n```python\\nnodes_with_scores = retriever.retrieve(\\n    \"What happened at Interleafe and Viaweb?\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nfor node in nodes_with_scores:\\n    print(f\"Score: {node.score:.2f} - {node.text}...\\\\n-----\\\\n\")\\n```\\noutput:\\n',\n",
       " 'Next, we can create our fusion retriever, which well return the top-2 most similar nodes from the 4 returned nodes from the retrievers:\\n\\ninput:\\n```python\\nfrom llama_index.retrievers import QueryFusionRetriever\\n\\nretriever = QueryFusionRetriever(\\n    [vector_retriever, bm25_retriever],\\n    similarity_top_k=2,\\n    num_queries=4,  # set this to 1 to disable query generation\\n    mode=\"reciprocal_rerank\",\\n    use_async=True,\\n    verbose=True,\\n    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\\n)\\n```\\noutput:\\ninput:\\n```python\\n# apply nested async to run in a notebook\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n```\\noutput:\\ninput:\\n```python\\nnodes_with_scores = retriever.retrieve(\\n    \"What happened at Interleafe and Viaweb?\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nfor node in nodes_with_scores:\\n    print(f\"Score: {node.score:.2f} - {node.text}...\\\\n-----\\\\n\")\\n```\\noutput:\\n',\n",
       " 'Given an example question, retrieve the set of relevant nodes and try to put it all in the prompt, separated by newlines.\\n\\ninput:\\n```python\\nquery_str = (\\n    \"Can you tell me about results from RLHF using both model-based and\"\\n    \" human-based evaluation?\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nretrieved_nodes = retriever.retrieve(query_str)\\n```\\noutput:\\ninput:\\n```python\\ndef generate_response(retrieved_nodes, query_str, qa_prompt, llm):\\n    context_str = \"\\\\n\\\\n\".join([r.get_content() for r in retrieved_nodes])\\n    fmt_qa_prompt = qa_prompt.format(\\n        context_str=context_str, query_str=query_str\\n    )\\n    response = llm.complete(fmt_qa_prompt)\\n    return str(response), fmt_qa_prompt\\n```\\noutput:\\ninput:\\n```python\\nresponse, fmt_qa_prompt = generate_response(\\n    retrieved_nodes, query_str, qa_prompt, llm\\n)\\n```\\noutput:\\ninput:\\n```python\\nprint(f\"*****Response******:\\\\n{response}\\\\n\\\\n\")\\n```\\noutput:\\ninput:\\n```python\\nprint(f\"*****Formatted Prompt*****:\\\\n{fmt_qa_prompt}\\\\n\\\\n\")\\n```\\noutput:\\n',\n",
       " 'Building Response Synthesis with LLMs\\nIn this section we\\'ll show how to use LLMs + Prompts to build a response synthesis module.\\nWe\\'ll start from simple strategies (simply stuffing context into a prompt), to more advanced strategies that can handle context overflows.\\n\\n1. Try a Simple Prompt\\nWe first try to synthesize the response using a single input prompt + LLM call.\\n\\ninput:\\n```python\\nfrom llama_index.llms import OpenAI\\nfrom llama_index.prompts import PromptTemplate\\n\\nllm = OpenAI(model=\"text-davinci-003\")\\n```\\noutput:\\ninput:\\n```python\\nqa_prompt = PromptTemplate(\\n    \"\"\"\\\\\\nContext information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: \\\\\\n\"\"\"\\n)\\n```\\noutput:\\n',\n",
       " 'Build Pinecone Index, Get Retriever\\nWe use our high-level LlamaIndex abstractions to 1) ingest data into Pinecone, and then 2) get a vector retriever.\\nNote that we set chunk sizes to 1024.\\n\\ninput:\\n```python\\nimport pinecone\\nimport os\\n\\napi_key = os.environ[\"PINECONE_API_KEY\"]\\npinecone.init(api_key=api_key, environment=\"us-west1-gcp\")\\n```\\noutput:\\ninput:\\n```python\\n# dimensions are for text-embedding-ada-002\\npinecone.create_index(\\n    \"quickstart\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\"\\n)\\n```\\noutput:\\ninput:\\n```python\\npinecone_index = pinecone.Index(\"quickstart\")\\n```\\noutput:\\ninput:\\n```python\\n# [Optional] drop contents in index\\npinecone_index.delete(deleteAll=True)\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index.vector_stores import PineconeVectorStore\\nfrom llama_index import VectorStoreIndex, ServiceContext\\nfrom llama_index.storage import StorageContext\\n```\\noutput:\\ninput:\\n```python\\nvector_store = PineconeVectorStore(pinecone_index=pinecone_index)\\n# NOTE: set chunk size of 1024\\nservice_context = ServiceContext.from_defaults(chunk_size=1024)\\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\\nindex = VectorStoreIndex.from_documents(\\n    documents, service_context=service_context, storage_context=storage_context\\n)\\n```\\noutput:\\ninput:\\n```python\\nretriever = index.as_retriever()\\n```\\noutput:\\n',\n",
       " 'Given an example question, get a retrieved set of nodes.\\nWe use the retriever to get a set of relevant nodes given a user query. These nodes will then be passed to the response synthesis modules below.\\n\\ninput:\\n```python\\nquery_str = (\\n    \"Can you tell me about results from RLHF using both model-based and\"\\n    \" human-based evaluation?\"\\n)\\n```\\noutput:\\ninput:\\n```python\\nretrieved_nodes = retriever.retrieve(query_str)\\n```\\noutput:\\n',\n",
       " 'As we can see, both retruned nodes correctly mention Viaweb and Interleaf!\\n\\nUse in a Query Engine!\\nNow, we can plug our retriever into a query engine to synthesize natural language responses.\\n\\ninput:\\n```python\\nfrom llama_index.query_engine import RetrieverQueryEngine\\n\\nquery_engine = RetrieverQueryEngine.from_args(retriever)\\n```\\noutput:\\ninput:\\n```python\\nresponse = query_engine.query(\"What happened at Interleafe and Viaweb?\")\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index.response.notebook_utils import display_response\\n\\ndisplay_response(response)\\n```\\noutput:\\n',\n",
       " 'As we can see, both retruned nodes correctly mention Viaweb and Interleaf!\\n\\nUse in a Query Engine!\\nNow, we can plug our retriever into a query engine to synthesize natural language responses.\\n\\ninput:\\n```python\\nfrom llama_index.query_engine import RetrieverQueryEngine\\n\\nquery_engine = RetrieverQueryEngine.from_args(retriever)\\n```\\noutput:\\ninput:\\n```python\\nresponse = query_engine.query(\"What happened at Interleafe and Viaweb?\")\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index.response.notebook_utils import display_response\\n\\ndisplay_response(response)\\n```\\noutput:\\n',\n",
       " 'Retrieve top 10 most relevant nodes, then filter with Cohere Rerank\\n\\ninput:\\n```python\\nimport os\\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\\n\\n\\napi_key = os.environ[\"COHERE_API_KEY\"]\\ncohere_rerank = CohereRerank(api_key=api_key, top_n=2)\\n```\\noutput:\\ninput:\\n```python\\nquery_engine = index.as_query_engine(\\n    similarity_top_k=10,\\n    node_postprocessors=[cohere_rerank],\\n)\\nresponse = query_engine.query(\\n    \"What did Sam Altman do in this essay?\",\\n)\\n```\\noutput:\\ninput:\\n```python\\npprint_response(response)\\n```\\noutput:\\n',\n",
       " 'Load Data\\n\\ninput:\\n```python\\n!mkdir data\\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\"\\n```\\noutput:\\ninput:\\n```python\\nfrom pathlib import Path\\nfrom llama_hub.file.pymu_pdf.base import PyMuPDFReader\\n```\\noutput:\\ninput:\\n```python\\nloader = PyMuPDFReader()\\ndocuments = loader.load(file_path=\"./data/llama2.pdf\")\\n```\\noutput:\\n',\n",
       " 'Simple Fusion Retriever\\nIn this example, we walk through how you can combine retrieval results from multiple queries and multiple indexes.\\nThe retrieved nodes will be returned as the top-k across all queries and indexes, as well as handling de-duplication of any nodes.\\n\\ninput:\\n```python\\nimport os\\nimport openai\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\\n```\\noutput:\\n',\n",
       " 'Setup\\n\\nIf you\\'re opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.\\n\\nDownload Data\\n\\ninput:\\n```python\\n!mkdir -p \\'data/paul_graham/\\'\\n!wget \\'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\\' -O \\'data/paul_graham/paul_graham_essay.txt\\'\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index import SimpleDirectoryReader\\n\\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\\n```\\noutput:\\n',\n",
       " 'Setup\\nFor this notebook, we will use two very similar pages of our documentation, each stored in a separaete index.\\n\\ninput:\\n```python\\nfrom llama_index import SimpleDirectoryReader\\n\\ndocuments_1 = SimpleDirectoryReader(\\n    input_files=[\"../../community/integrations/vector_stores.md\"]\\n).load_data()\\ndocuments_2 = SimpleDirectoryReader(\\n    input_files=[\"../../core_modules/data_modules/storage/vector_stores.md\"]\\n).load_data()\\n```\\noutput:\\ninput:\\n```python\\nfrom llama_index import VectorStoreIndex\\n\\nindex_1 = VectorStoreIndex.from_documents(documents_1)\\nindex_2 = VectorStoreIndex.from_documents(documents_2)\\n```\\noutput:\\n',\n",
       " 'Retrieved context is irrelevant and response is hallucinated.\\n\\ninput:\\n```python\\npprint_response(response)\\n```\\noutput:\\n',\n",
       " 'Download Data\\n\\ninput:\\n```python\\n!mkdir -p \\'data/paul_graham/\\'\\n!wget \\'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\\' -O \\'data/paul_graham/paul_graham_essay.txt\\'\\n```\\noutput:\\ninput:\\n```python\\n# load documents\\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\\n\\n# build index\\nindex = VectorStoreIndex.from_documents(documents=documents)\\n```\\noutput:\\n',\n",
       " 'Next, we will setup a vector index over the documentation.\\n\\ninput:\\n```python\\nfrom llama_index import VectorStoreIndex, ServiceContext\\n\\nservice_context = ServiceContext.from_defaults(chunk_size=256)\\n\\nindex = VectorStoreIndex.from_documents(\\n    documents, service_context=service_context\\n)\\n```\\noutput:\\n',\n",
       " 'Next, we will setup a vector index over the documentation.\\n\\ninput:\\n```python\\nfrom llama_index import VectorStoreIndex, ServiceContext\\n\\nservice_context = ServiceContext.from_defaults(chunk_size=256)\\n\\nindex = VectorStoreIndex.from_documents(\\n    documents, service_context=service_context\\n)\\n```\\noutput:\\n',\n",
       " 'Directly retrieve top 2 most similar nodes\\n\\ninput:\\n```python\\nquery_engine = index.as_query_engine(\\n    similarity_top_k=2,\\n)\\nresponse = query_engine.query(\\n    \"What did Sam Altman do in this essay?\",\\n)\\n```\\noutput:\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_docs(docs, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repomanager.statemanager.llms.completion import get_llm\n",
    "\n",
    "llm = get_llm()\n",
    "llm2 = get_llm(model='gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1025"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'How to load vector store and graph store together?'\n",
    "_query = \"\"\"Find the best matching usecase or insight.\n",
    "user query: {}\"\"\".format(query)\n",
    "docs = []\n",
    "for doc in get_docs(_query, vs, G, top_k=5, max_depth=3):\n",
    "    docs.extend(doc)\n",
    "reranked_docs = rank_docs(docs, _query)\n",
    "top_three = reranked_docs[:3]\n",
    "prompt = \"\"\"For the user query, use the information attached to answer it in short form.\n",
    "Provide simple examples (if possible):\\n\"\"\"\n",
    "i = 0\n",
    "for doc in top_three:\n",
    "    prompt += \"Source {}\\n\".format(i)\n",
    "    prompt += doc + \"\\n\"\n",
    "    i += 1\n",
    "\n",
    "prompt += \"Query:\\n\"\n",
    "prompt += query + \"\\n\"\n",
    "prompt += \"Answer:\\n\"\n",
    "# print the number of words in the prompt\n",
    "len(prompt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To load a vector store and graph store together, you can use the following code:\n",
       "\n",
       "```python\n",
       "from llama_index import VectorStoreIndex, GraphStoreIndex\n",
       "from llama_index.vector_stores import ChromaVectorStore\n",
       "from llama_index.storage.storage_context import StorageContext\n",
       "\n",
       "# Load vector store\n",
       "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
       "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
       "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)\n",
       "\n",
       "# Load graph store\n",
       "graph_store = GraphStoreIndex(graph_collection=graph_collection)\n",
       "index.add_graph_store(graph_store)\n",
       "```\n",
       "\n",
       "This code initializes a vector store using Chroma as the vector store and a graph store using the specified graph collection. The vector store and graph store are then added to the index."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import display and markdown\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(llm.complete(prompt).text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To load a vector store and graph store together, you need to initialize a client, create a collection to store your data, assign the vector store to a StorageContext, and initialize your VectorStoreIndex using that StorageContext. Here is an example using Chroma as the vector store:\n",
       "\n",
       "```python\n",
       "import chromadb\n",
       "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
       "from llama_index.vector_stores import ChromaVectorStore\n",
       "from llama_index.storage.storage_context import StorageContext\n",
       "\n",
       "# load some documents\n",
       "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
       "\n",
       "# initialize client, setting path to save data\n",
       "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
       "\n",
       "# create collection\n",
       "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
       "\n",
       "# assign chroma as the vector_store to the context\n",
       "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
       "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
       "\n",
       "# create your index\n",
       "index = VectorStoreIndex.from_documents(\n",
       "    documents, storage_context=storage_context\n",
       ")\n",
       "\n",
       "# create a query engine and query\n",
       "query_engine = index.as_query_engine()\n",
       "response = query_engine.query(\"What is the meaning of life?\")\n",
       "print(response)\n",
       "```\n",
       "\n",
       "If you've already created and stored your embeddings, you can load them directly without loading your documents or creating a new VectorStoreIndex:\n",
       "\n",
       "```python\n",
       "import chromadb\n",
       "from llama_index import VectorStoreIndex\n",
       "from llama_index.vector_stores import ChromaVectorStore\n",
       "from llama_index.storage.storage_context import StorageContext\n",
       "\n",
       "# initialize client\n",
       "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
       "\n",
       "# get collection\n",
       "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
       "\n",
       "# assign chroma as the vector_store to the context\n",
       "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
       "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
       "\n",
       "# load your index from stored vectors\n",
       "index = VectorStoreIndex.from_vector_store(\n",
       "    vector_store, storage_context=storage_context\n",
       ")\n",
       "\n",
       "# create a query engine\n",
       "query_engine = index.as_query_engine()\n",
       "response = query_engine.query(\"What is llama2?\")\n",
       "print(response)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(llm2.complete(prompt).text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root': 2.535797987573841e-05,\n",
       " 'LICENSE': 2.6558362187526002e-05,\n",
       " 'CHANGELOG.md': 2.6558362187526002e-05,\n",
       " '.pre-commit-config.yaml': 2.6558362187526002e-05,\n",
       " 'Makefile': 2.8612681822994494e-05,\n",
       " 'CITATION.cff': 2.6558362187526002e-05,\n",
       " 'pyproject.toml': 2.6558362187526002e-05,\n",
       " 'README.md': 9.657432073854376e-05,\n",
       " 'CONTRIBUTING.md': 2.6558362187526002e-05,\n",
       " 'poetry.lock': 2.6558362187526002e-05,\n",
       " '.readthedocs.yaml': 2.6558362187526002e-05,\n",
       " 'experimental': 2.7223581194882335e-05,\n",
       " 'splitter_playground': 2.8673215955606596e-05,\n",
       " 'app.py': 4.9838092699503767e-05,\n",
       " 'cli': 2.8673215955606596e-05,\n",
       " 'configuration.py': 3.858561624856383e-05,\n",
       " 'cli_add.py': 3.0910707139595646e-05,\n",
       " '__init__.py': 0.0006499568398591388,\n",
       " 'cli_init.py': 3.0910707139595646e-05,\n",
       " 'cli_query.py': 3.0910707139595646e-05,\n",
       " '__main__.py': 2.885513885056203e-05,\n",
       " 'openai_fine_tuning': 2.8673215955606596e-05,\n",
       " 'openai_fine_tuning.ipynb': 3.586639057649473e-05,\n",
       " 'launch_training.py': 3.351801748366019e-05,\n",
       " 'validate_json.py': 3.8600368126162755e-05,\n",
       " 'classifier': 2.8673215955606596e-05,\n",
       " 'TitanicModel.ipynb': 3.351801748366019e-05,\n",
       " 'utils.py': 0.00014988520612378177,\n",
       " 'data': 0.00012977482168272304,\n",
       " 'train.csv': 3.2778176795107434e-05,\n",
       " 'llama_index': 0.0009357380766111104,\n",
       " 'service_context.py': 3.96330034275986e-05,\n",
       " 'async_utils.py': 3.86617125467603e-05,\n",
       " 'exec_utils.py': 3.86617125467603e-05,\n",
       " 'constants.py': 3.86617125467603e-05,\n",
       " 'types.py': 9.124868213253399e-05,\n",
       " 'VERSION': 3.86617125467603e-05,\n",
       " 'py.typed': 3.86617125467603e-05,\n",
       " 'schema.py': 5.8252856984362784e-05,\n",
       " 'img_utils.py': 3.86617125467603e-05,\n",
       " 'vector_stores': 4.683990408780988e-05,\n",
       " 'cassandra.py': 2.565865235938723e-05,\n",
       " 'elasticsearch.py': 2.8628025728518726e-05,\n",
       " 'milvus.py': 2.7359546833052083e-05,\n",
       " 'loading.py': 4.5504948975681114e-05,\n",
       " 'azurecosmosmongo.py': 2.565865235938723e-05,\n",
       " 'dynamodb.py': 2.565865235938723e-05,\n",
       " 'myscale.py': 2.6412205008617335e-05,\n",
       " 'tencentvectordb.py': 2.565865235938723e-05,\n",
       " 'pinecone.py': 2.701041929418009e-05,\n",
       " 'opensearch.py': 2.565865235938723e-05,\n",
       " 'awadb.py': 2.741591224177029e-05,\n",
       " 'supabase.py': 2.565865235938723e-05,\n",
       " 'rocksetdb.py': 2.565865235938723e-05,\n",
       " 'bagel.py': 2.7103513282162543e-05,\n",
       " 'qdrant.py': 2.6412205008617335e-05,\n",
       " 'registry.py': 4.1935108431177175e-05,\n",
       " 'cogsearch.py': 2.565865235938723e-05,\n",
       " 'weaviate_utils.py': 2.565865235938723e-05,\n",
       " 'mongodb.py': 2.565865235938723e-05,\n",
       " 'pgvecto_rs.py': 2.565865235938723e-05,\n",
       " 'zep.py': 2.565865235938723e-05,\n",
       " 'epsilla.py': 2.565865235938723e-05,\n",
       " 'faiss.py': 3.1018591051469385e-05,\n",
       " 'timescalevector.py': 2.565865235938723e-05,\n",
       " 'dashvector.py': 2.741591224177029e-05,\n",
       " 'redis.py': 2.863784786687862e-05,\n",
       " 'weaviate.py': 3.0471604679458753e-05,\n",
       " 'lancedb.py': 2.6689738293711165e-05,\n",
       " 'postgres.py': 2.565865235938723e-05,\n",
       " 'deeplake.py': 2.85790474514367e-05,\n",
       " 'qdrant_utils.py': 2.565865235938723e-05,\n",
       " 'neo4jvector.py': 2.565865235938723e-05,\n",
       " 'metal.py': 2.6412205008617335e-05,\n",
       " 'chroma.py': 2.6412205008617335e-05,\n",
       " 'tair.py': 2.565865235938723e-05,\n",
       " 'lantern.py': 2.565865235938723e-05,\n",
       " 'simple.py': 3.185533794293468e-05,\n",
       " 'chatgpt_plugin.py': 2.565865235938723e-05,\n",
       " 'singlestoredb.py': 2.6797540209724712e-05,\n",
       " 'astra.py': 2.565865235938723e-05,\n",
       " 'typesense.py': 2.64924360184994e-05,\n",
       " 'google': 4.292967425575106e-05,\n",
       " 'generativeai': 6.130775767209199e-05,\n",
       " 'genai_extension.py': 4.300530328946892e-05,\n",
       " 'base.py': 0.000328796493167291,\n",
       " 'docarray': 3.097161088708195e-05,\n",
       " 'hnsw.py': 3.193280299530685e-05,\n",
       " 'in_memory.py': 3.193280299530685e-05,\n",
       " 'token_counter': 3.86617125467603e-05,\n",
       " 'mock_embed_model.py': 3.666386789223265e-05,\n",
       " 'retrievers': 4.7310056003531476e-05,\n",
       " 'recursive_retriever.py': 2.683355812513221e-05,\n",
       " 'bm25_retriever.py': 2.683355812513221e-05,\n",
       " 'fusion_retriever.py': 2.683355812513221e-05,\n",
       " 'you_retriever.py': 2.683355812513221e-05,\n",
       " 'transform_retriever.py': 2.683355812513221e-05,\n",
       " 'auto_merging_retriever.py': 2.683355812513221e-05,\n",
       " 'router_retriever.py': 2.683355812513221e-05,\n",
       " 'ingestion': 4.017347401849311e-05,\n",
       " 'cache.py': 2.8880539514990203e-05,\n",
       " 'pipeline.py': 2.8880539514990203e-05,\n",
       " 'indices': 5.5579829620058494e-05,\n",
       " 'postprocessor.py': 2.632927075657671e-05,\n",
       " 'managed.tar.gz': 2.632927075657671e-05,\n",
       " 'base_retriever.py': 3.648868150497472e-05,\n",
       " 'prompt_helper.py': 2.632927075657671e-05,\n",
       " 'tree': 2.7614932103470912e-05,\n",
       " 'tree_root_retriever.py': 2.7152645391267427e-05,\n",
       " 'select_leaf_retriever.py': 2.7152645391267427e-05,\n",
       " 'select_leaf_embedding_retriever.py': 2.7152645391267427e-05,\n",
       " 'inserter.py': 2.7152645391267427e-05,\n",
       " 'all_leaf_retriever.py': 2.7152645391267427e-05,\n",
       " 'struct_store': 3.6928133535677114e-05,\n",
       " 'pandas.py': 5.03210741204466e-05,\n",
       " 'sql_query.py': 2.7741514152240674e-05,\n",
       " 'json_query.py': 2.7741514152240674e-05,\n",
       " 'container_builder.py': 2.7741514152240674e-05,\n",
       " 'sql_retriever.py': 2.7741514152240674e-05,\n",
       " 'sql.py': 2.7741514152240674e-05,\n",
       " 'managed': 2.6994489763933046e-05,\n",
       " 'vectara': 2.7112986009866407e-05,\n",
       " 'query.py': 3.113335333082793e-05,\n",
       " 'retriever.py': 4.9234796701638645e-05,\n",
       " 'colbert_index': 2.7112986009866407e-05,\n",
       " 'zilliz': 2.7112986009866407e-05,\n",
       " 'knowledge_graph': 3.575676668433914e-05,\n",
       " 'retrievers.py': 8.532280827904133e-05,\n",
       " 'empty': 3.5215239114736184e-05,\n",
       " 'keyword_table': 2.632927075657671e-05,\n",
       " 'simple_base.py': 2.7583270397311495e-05,\n",
       " 'rake_base.py': 2.7583270397311495e-05,\n",
       " 'vector_store': 3.740889476834224e-05,\n",
       " 'auto_retriever': 2.7690383294520923e-05,\n",
       " 'output_parser.py': 4.048037390224586e-05,\n",
       " 'prompts.py': 5.413438330171884e-05,\n",
       " 'auto_retriever.py': 2.996918903345466e-05,\n",
       " 'composability': 4.0866783466932055e-05,\n",
       " 'graph.py': 2.8917240504218257e-05,\n",
       " 'common': 3.78505239629959e-05,\n",
       " 'common_tree': 2.632927075657671e-05,\n",
       " 'list': 0.0003143265408686665,\n",
       " 'multi_modal': 2.9358266471343444e-05,\n",
       " 'document_summary': 2.632927075657671e-05,\n",
       " 'query': 0.00030564916637516106,\n",
       " 'embedding_utils.py': 3.248427915576454e-05,\n",
       " 'query_transform': 3.248427915576454e-05,\n",
       " 'feedback_transform.py': 2.997856071034807e-05,\n",
       " 'tools': 4.944452322845471e-05,\n",
       " 'query_engine.py': 2.7100903248469673e-05,\n",
       " 'query_plan.py': 2.7100903248469673e-05,\n",
       " 'download.py': 4.26083277719356e-05,\n",
       " 'ondemand_loader_tool.py': 2.7100903248469673e-05,\n",
       " 'function_tool.py': 2.7100903248469673e-05,\n",
       " 'retriever_tool.py': 2.7100903248469673e-05,\n",
       " 'tool_spec': 2.7100903248469673e-05,\n",
       " 'notion': 2.9122073163351662e-05,\n",
       " 'slack': 2.9122073163351662e-05,\n",
       " 'load_and_search': 2.9122073163351662e-05,\n",
       " 'embeddings': 9.17400948920954e-05,\n",
       " 'cohereai.py': 2.6471683059350805e-05,\n",
       " 'multi_modal_base.py': 2.6471683059350805e-05,\n",
       " 'google_palm.py': 2.6471683059350805e-05,\n",
       " 'adapter.py': 2.6471683059350805e-05,\n",
       " 'gradient.py': 2.695850198246853e-05,\n",
       " 'pooling.py': 2.6471683059350805e-05,\n",
       " 'voyageai.py': 2.8901447003969253e-05,\n",
       " 'google.py': 2.9416060865262863e-05,\n",
       " 'gemini.py': 3.1291429715541635e-05,\n",
       " 'instructor.py': 2.6471683059350805e-05,\n",
       " 'adapter_utils.py': 2.6471683059350805e-05,\n",
       " 'huggingface_optimum.py': 2.6471683059350805e-05,\n",
       " 'jinaai.py': 2.6471683059350805e-05,\n",
       " 'openai.py': 4.0559841706928625e-05,\n",
       " 'azure_openai.py': 2.695850198246853e-05,\n",
       " 'huggingface_utils.py': 2.6471683059350805e-05,\n",
       " 'huggingface.py': 2.695850198246853e-05,\n",
       " 'ollama_embedding.py': 2.6471683059350805e-05,\n",
       " 'llm_rails.py': 2.6471683059350805e-05,\n",
       " 'langchain.py': 6.994804387339805e-05,\n",
       " 'mistralai.py': 2.6471683059350805e-05,\n",
       " 'clarifai.py': 2.695850198246853e-05,\n",
       " 'clip.py': 2.7938527864625497e-05,\n",
       " 'text_embeddings_inference.py': 2.6471683059350805e-05,\n",
       " 'bedrock.py': 2.695850198246853e-05,\n",
       " 'fastembed.py': 2.924784698476064e-05,\n",
       " 'node_parser': 6.786261037069115e-05,\n",
       " 'node_utils.py': 2.9251757942216366e-05,\n",
       " 'interface.py': 3.603529075211291e-05,\n",
       " 'relational': 2.9251757942216366e-05,\n",
       " 'markdown_element.py': 3.0202593941676538e-05,\n",
       " 'unstructured_element.py': 3.0202593941676538e-05,\n",
       " 'hierarchical.py': 3.0202593941676538e-05,\n",
       " 'base_element.py': 3.0202593941676538e-05,\n",
       " 'file': 3.000531059144647e-05,\n",
       " 'simple_file.py': 2.6655183186805743e-05,\n",
       " 'html.py': 2.6655183186805743e-05,\n",
       " 'markdown.py': 2.6655183186805743e-05,\n",
       " 'json.py': 0.00011248073014218226,\n",
       " 'text': 0.0003688186728144326,\n",
       " 'sentence_window.py': 5.3462974331195244e-05,\n",
       " 'token.py': 4.956919626471729e-05,\n",
       " 'code.py': 4.956919626471729e-05,\n",
       " 'sentence.py': 4.956919626471729e-05,\n",
       " 'core': 0.00012117224710656886,\n",
       " 'image_retriever.py': 3.5517390624136416e-05,\n",
       " 'base_multi_modal_retriever.py': 3.5517390624136416e-05,\n",
       " 'base_query_engine.py': 3.5517390624136416e-05,\n",
       " 'logger': 3.9508255011136775e-05,\n",
       " 'response': 0.00012130140792322407,\n",
       " 'pprint_utils.py': 2.8781222330304924e-05,\n",
       " 'notebook_utils.py': 3.033666039125548e-05,\n",
       " 'memory': 0.00011317339980183264,\n",
       " 'chat_memory_buffer.py': 3.520095965345064e-05,\n",
       " 'llm_predictor': 4.645077715710218e-05,\n",
       " 'structured.py': 3.120159997885927e-05,\n",
       " 'mock.py': 3.168841890197699e-05,\n",
       " 'vellum': 3.5364481011216605e-05,\n",
       " 'predictor.py': 2.7729149287460993e-05,\n",
       " 'prompt_registry.py': 2.7729149287460993e-05,\n",
       " 'exceptions.py': 2.7729149287460993e-05,\n",
       " 'playground': 3.9508255011136775e-05,\n",
       " 'callbacks': 7.470850589357187e-05,\n",
       " 'token_counting.py': 3.7186443505120066e-05,\n",
       " 'wandb_callback.py': 2.8520588038973846e-05,\n",
       " 'simple_llm_handler.py': 2.757889092649795e-05,\n",
       " 'aim.py': 3.045464031132542e-05,\n",
       " 'open_inference_callback.py': 2.8520588038973846e-05,\n",
       " 'promptlayer_handler.py': 2.757889092649795e-05,\n",
       " 'finetuning_handler.py': 2.8520588038973846e-05,\n",
       " 'global_handlers.py': 2.757889092649795e-05,\n",
       " 'base_handler.py': 2.757889092649795e-05,\n",
       " 'arize_phoenix_callback.py': 2.757889092649795e-05,\n",
       " 'honeyhive_callback.py': 2.757889092649795e-05,\n",
       " 'llama_debug.py': 2.8520588038973846e-05,\n",
       " 'bridge': 3.86617125467603e-05,\n",
       " 'pydantic.py': 4.071290653438698e-05,\n",
       " 'postprocessor': 3.9508255011136775e-05,\n",
       " 'flag_embedding_reranker.py': 2.7397004691302223e-05,\n",
       " 'cohere_rerank.py': 2.7397004691302223e-05,\n",
       " 'node_recency.py': 2.7397004691302223e-05,\n",
       " 'optimizer.py': 2.7397004691302223e-05,\n",
       " 'node.py': 2.7397004691302223e-05,\n",
       " 'sbert_rerank.py': 2.7397004691302223e-05,\n",
       " 'metadata_replacement.py': 2.7397004691302223e-05,\n",
       " 'longllmlingua.py': 2.7397004691302223e-05,\n",
       " 'pii.py': 2.7397004691302223e-05,\n",
       " 'llm_rerank.py': 2.7397004691302223e-05,\n",
       " 'graph_stores': 3.86617125467603e-05,\n",
       " 'kuzu.py': 3.25489148337854e-05,\n",
       " 'falkordb.py': 2.9597687881923746e-05,\n",
       " 'neo4j.py': 3.326337648103523e-05,\n",
       " 'nebulagraph.py': 2.9597687881923746e-05,\n",
       " 'multi_modal_llms': 3.9508255011136775e-05,\n",
       " 'replicate_multi_modal.py': 2.9690907608811514e-05,\n",
       " 'generic_utils.py': 3.0177726531929236e-05,\n",
       " 'openai_utils.py': 3.0177726531929236e-05,\n",
       " 'objects': 4.173334611483344e-05,\n",
       " 'base_node_mapping.py': 2.9934110863951793e-05,\n",
       " 'tool_node_mapping.py': 2.9934110863951793e-05,\n",
       " 'table_node_mapping.py': 2.9934110863951793e-05,\n",
       " 'agent': 4.355242601956113e-05,\n",
       " 'openai_assistant_agent.py': 2.6416720198521096e-05,\n",
       " 'runner': 2.6416720198521096e-05,\n",
       " 'parallel.py': 3.091668996767763e-05,\n",
       " 'legacy': 2.6416720198521096e-05,\n",
       " 'context_retriever_agent.py': 2.9804947949289785e-05,\n",
       " 'openai_agent.py': 2.9804947949289785e-05,\n",
       " 'retriever_openai_agent.py': 2.9804947949289785e-05,\n",
       " 'react': 3.086368827207248e-05,\n",
       " 'formatter.py': 2.796566518232308e-05,\n",
       " 'step.py': 3.304801582482564e-05,\n",
       " 'agent.py': 2.796566518232308e-05,\n",
       " 'openai': 4.284692852250908e-05,\n",
       " 'param_tuner': 3.9508255011136775e-05,\n",
       " 'finetuning': 4.017347401849311e-05,\n",
       " 'gradient': 2.8371571975929273e-05,\n",
       " 'sentence_transformer.py': 2.6471683059350805e-05,\n",
       " 'common.py': 2.6471683059350805e-05,\n",
       " 'rerankers': 2.770635296857294e-05,\n",
       " 'cohere_reranker.py': 3.1090761707927854e-05,\n",
       " 'dataset_gen.py': 3.8734470817513786e-05,\n",
       " 'cross_encoders': 2.770635296857294e-05,\n",
       " 'cross_encoder.py': 3.3001688985324344e-05,\n",
       " 'langchain_helpers': 3.9508255011136775e-05,\n",
       " 'text_splitter.py': 3.2290664248655386e-05,\n",
       " 'streaming.py': 3.2290664248655386e-05,\n",
       " 'memory_wrapper.py': 3.2290664248655386e-05,\n",
       " 'agents': 6.846284593217396e-05,\n",
       " 'toolkits.py': 2.9768438153595342e-05,\n",
       " 'tools.py': 2.9768438153595342e-05,\n",
       " 'agents.py': 2.9768438153595342e-05,\n",
       " 'storage': 4.155037369693006e-05,\n",
       " 'storage_context.py': 2.815539984496259e-05,\n",
       " 'kvstore': 2.815539984496259e-05,\n",
       " 's3_kvstore.py': 2.6906987558101955e-05,\n",
       " 'firestore_kvstore.py': 2.6906987558101955e-05,\n",
       " 'mongodb_kvstore.py': 2.6906987558101955e-05,\n",
       " 'dynamodb_kvstore.py': 2.6906987558101955e-05,\n",
       " 'redis_kvstore.py': 2.6906987558101955e-05,\n",
       " 'simple_kvstore.py': 2.6906987558101955e-05,\n",
       " 'docstore': 0.00014511194766606963,\n",
       " 'firestore_docstore.py': 3.077049305225759e-05,\n",
       " 'mongo_docstore.py': 3.077049305225759e-05,\n",
       " 'redis_docstore.py': 3.077049305225759e-05,\n",
       " 'simple_docstore.py': 3.077049305225759e-05,\n",
       " 'keyval_docstore.py': 3.077049305225759e-05,\n",
       " 'dynamodb_docstore.py': 3.077049305225759e-05,\n",
       " 'index_store': 3.383603713653674e-05,\n",
       " 'firestore_indexstore.py': 2.7698520340691668e-05,\n",
       " 'simple_index_store.py': 2.7698520340691668e-05,\n",
       " 'keyval_index_store.py': 2.7698520340691668e-05,\n",
       " 'mongo_index_store.py': 2.7698520340691668e-05,\n",
       " 'redis_index_store.py': 2.7698520340691668e-05,\n",
       " 'dynamodb_index_store.py': 2.7698520340691668e-05,\n",
       " 'utilities': 3.9508255011136775e-05,\n",
       " 'sql_wrapper.py': 3.402383534188462e-05,\n",
       " 'joint_qa_summary.py': 2.8917240504218257e-05,\n",
       " 'output_parsers': 4.308516581699325e-05,\n",
       " 'guardrails.py': 2.842007782493368e-05,\n",
       " 'selection.py': 2.842007782493368e-05,\n",
       " 'data_structs': 3.86617125467603e-05,\n",
       " 'struct_type.py': 3.101092388398553e-05,\n",
       " 'data_structs.py': 3.101092388398553e-05,\n",
       " 'document_summary.py': 3.101092388398553e-05,\n",
       " 'table.py': 3.101092388398553e-05,\n",
       " 'download': 4.1991065520276845e-05,\n",
       " 'dataset.py': 2.6718419135251748e-05,\n",
       " 'module.py': 2.6718419135251748e-05,\n",
       " 'tts': 3.9326931554116644e-05,\n",
       " 'bark.py': 3.416053351618594e-05,\n",
       " 'elevenlabs.py': 3.515497985981064e-05,\n",
       " 'program': 5.9522597332613274e-05,\n",
       " 'openai_program.py': 2.9044388352949915e-05,\n",
       " 'llm_program.py': 2.9044388352949915e-05,\n",
       " 'multi_modal_llm_program.py': 2.9044388352949915e-05,\n",
       " 'lmformatenforcer_program.py': 2.9044388352949915e-05,\n",
       " 'guidance_program.py': 2.9044388352949915e-05,\n",
       " 'llm_prompt_program.py': 2.9044388352949915e-05,\n",
       " 'predefined': 2.9044388352949915e-05,\n",
       " 'df.py': 3.3344780847439914e-05,\n",
       " 'evaporate': 3.3344780847439914e-05,\n",
       " 'extractor.py': 3.243037946774102e-05,\n",
       " 'prompts': 7.151449070837776e-05,\n",
       " 'prompt_type.py': 2.7423238491274047e-05,\n",
       " 'system.py': 2.7423238491274047e-05,\n",
       " 'mixin.py': 2.7423238491274047e-05,\n",
       " 'display_utils.py': 2.7423238491274047e-05,\n",
       " 'chat_prompts.py': 2.7423238491274047e-05,\n",
       " 'default_prompt_selectors.py': 2.7423238491274047e-05,\n",
       " 'prompt_utils.py': 2.7423238491274047e-05,\n",
       " 'guidance_utils.py': 2.7423238491274047e-05,\n",
       " 'lmformatenforcer_utils.py': 2.7423238491274047e-05,\n",
       " 'default_prompts.py': 2.7423238491274047e-05,\n",
       " 'response_synthesizers': 4.449867480583627e-05,\n",
       " 'simple_summarize.py': 2.705578377058471e-05,\n",
       " 'compact_and_accumulate.py': 2.705578377058471e-05,\n",
       " 'generation.py': 2.705578377058471e-05,\n",
       " 'accumulate.py': 2.705578377058471e-05,\n",
       " 'refine.py': 2.705578377058471e-05,\n",
       " 'no_text.py': 2.705578377058471e-05,\n",
       " 'type.py': 2.705578377058471e-05,\n",
       " 'factory.py': 2.705578377058471e-05,\n",
       " 'compact_and_refine.py': 2.705578377058471e-05,\n",
       " 'tree_summarize.py': 2.705578377058471e-05,\n",
       " 'evaluation': 7.415157539763509e-05,\n",
       " 'batch_runner.py': 2.6913417936688957e-05,\n",
       " 'faithfulness.py': 2.813828717786715e-05,\n",
       " 'correctness.py': 2.6913417936688957e-05,\n",
       " 'semantic_similarity.py': 2.6913417936688957e-05,\n",
       " 'dataset_generation.py': 2.6913417936688957e-05,\n",
       " 'pairwise.py': 2.6913417936688957e-05,\n",
       " 'guideline.py': 2.6913417936688957e-05,\n",
       " 'eval_utils.py': 2.8027121120301357e-05,\n",
       " 'relevancy.py': 2.813828717786715e-05,\n",
       " 'retrieval': 2.6913417936688957e-05,\n",
       " 'metrics.py': 2.858870381695392e-05,\n",
       " 'evaluator.py': 2.858870381695392e-05,\n",
       " 'metrics_base.py': 2.858870381695392e-05,\n",
       " 'benchmarks': 2.811380024847655e-05,\n",
       " 'hotpotqa.py': 2.8736931876806427e-05,\n",
       " 'beir.py': 3.008832489914554e-05,\n",
       " 'llama_pack': 3.86617125467603e-05,\n",
       " 'chat_engine': 4.017347401849311e-05,\n",
       " 'condense_plus_context.py': 2.7314957453100516e-05,\n",
       " 'condense_question.py': 2.7314957453100516e-05,\n",
       " 'context.py': 2.7314957453100516e-05,\n",
       " 'readers': 3.9508255011136775e-05,\n",
       " 'string_iterable.py': 2.6111532524968513e-05,\n",
       " 'youtube_transcript.py': 2.6111532524968513e-05,\n",
       " 'web.py': 2.6111532524968513e-05,\n",
       " 'mongo.py': 2.6111532524968513e-05,\n",
       " 'database.py': 2.6111532524968513e-05,\n",
       " 'mbox.py': 2.6111532524968513e-05,\n",
       " 'discord_reader.py': 2.6111532524968513e-05,\n",
       " 'twitter.py': 2.6111532524968513e-05,\n",
       " 'obsidian.py': 2.6111532524968513e-05,\n",
       " 'notion.py': 2.6111532524968513e-05,\n",
       " 'slack.py': 2.6111532524968513e-05,\n",
       " 'wikipedia.py': 2.9201212086605013e-05,\n",
       " 'psychic.py': 2.6111532524968513e-05,\n",
       " 'html_reader.py': 2.6655183186805743e-05,\n",
       " 'markdown_reader.py': 2.6655183186805743e-05,\n",
       " 'image_caption_reader.py': 2.6655183186805743e-05,\n",
       " 'ipynb_reader.py': 2.6655183186805743e-05,\n",
       " 'image_vision_llm_reader.py': 2.6655183186805743e-05,\n",
       " 'docs_reader.py': 2.6655183186805743e-05,\n",
       " 'slides_reader.py': 2.6655183186805743e-05,\n",
       " 'tabular_reader.py': 2.6655183186805743e-05,\n",
       " 'image_reader.py': 2.6655183186805743e-05,\n",
       " 'flat_reader.py': 2.6655183186805743e-05,\n",
       " 'mbox_reader.py': 2.6655183186805743e-05,\n",
       " 'epub_reader.py': 2.6655183186805743e-05,\n",
       " 'video_audio_reader.py': 2.6655183186805743e-05,\n",
       " 'redis': 3.3822949449754984e-05,\n",
       " 'make_com': 2.6111532524968513e-05,\n",
       " 'wrapper.py': 3.6373316972121075e-05,\n",
       " 'schema': 0.00017276978292220677,\n",
       " 'google_readers': 2.6111532524968513e-05,\n",
       " 'gsheets.py': 3.2701537939993525e-05,\n",
       " 'gdocs.py': 3.2701537939993525e-05,\n",
       " 'chatgpt_plugin': 2.677675153232485e-05,\n",
       " 'weaviate': 2.8774360429277545e-05,\n",
       " 'reader.py': 3.755151024812168e-05,\n",
       " 'github_readers': 2.6111532524968513e-05,\n",
       " 'github_repository_reader.py': 3.086564842392975e-05,\n",
       " 'github_api_client.py': 3.086564842392975e-05,\n",
       " 'steamship': 2.6111532524968513e-05,\n",
       " 'file_reader.py': 3.6373316972121075e-05,\n",
       " 'question_gen': 3.9508255011136775e-05,\n",
       " 'guidance_generator.py': 2.8824322062196895e-05,\n",
       " 'openai_generator.py': 2.8824322062196895e-05,\n",
       " 'llm_generators.py': 2.8824322062196895e-05,\n",
       " 'llms': 6.280437012815735e-05,\n",
       " 'portkey_utils.py': 2.5844798798856127e-05,\n",
       " 'portkey.py': 2.6435340810137046e-05,\n",
       " 'litellm_utils.py': 2.5844798798856127e-05,\n",
       " 'gemini_utils.py': 2.5844798798856127e-05,\n",
       " 'watsonx_utils.py': 2.5844798798856127e-05,\n",
       " 'mistralai_utils.py': 2.5844798798856127e-05,\n",
       " 'vertex_gemini_utils.py': 2.5844798798856127e-05,\n",
       " 'palm.py': 2.5844798798856127e-05,\n",
       " 'cohere_utils.py': 2.5844798798856127e-05,\n",
       " 'konko_utils.py': 2.5844798798856127e-05,\n",
       " 'mistral.py': 2.5844798798856127e-05,\n",
       " 'vllm.py': 2.6527226126586638e-05,\n",
       " 'vertex_utils.py': 2.5844798798856127e-05,\n",
       " 'custom.py': 2.6706507957144542e-05,\n",
       " 'openrouter.py': 2.5844798798856127e-05,\n",
       " 'langchain_utils.py': 2.5844798798856127e-05,\n",
       " 'xinference_utils.py': 2.5844798798856127e-05,\n",
       " 'localai.py': 2.5844798798856127e-05,\n",
       " 'replicate.py': 2.789899759683122e-05,\n",
       " 'perplexity.py': 2.5844798798856127e-05,\n",
       " 'anyscale.py': 2.5844798798856127e-05,\n",
       " 'llama_cpp.py': 2.5844798798856127e-05,\n",
       " 'llm.py': 2.5844798798856127e-05,\n",
       " 'vertex.py': 2.5844798798856127e-05,\n",
       " 'rungpt.py': 2.5844798798856127e-05,\n",
       " 'vllm_utils.py': 2.5844798798856127e-05,\n",
       " 'anyscale_utils.py': 2.5844798798856127e-05,\n",
       " 'litellm.py': 2.743741960294492e-05,\n",
       " 'konko.py': 2.7761472630584972e-05,\n",
       " 'llama_api.py': 2.5844798798856127e-05,\n",
       " 'openllm.py': 2.6451683494388906e-05,\n",
       " 'anthropic.py': 3.184155991649654e-05,\n",
       " 'openai_like.py': 2.5844798798856127e-05,\n",
       " 'xinference.py': 2.5844798798856127e-05,\n",
       " 'bedrock_utils.py': 2.5844798798856127e-05,\n",
       " 'watsonx.py': 2.5844798798856127e-05,\n",
       " 'ai21.py': 2.8051679879756988e-05,\n",
       " 'ai21_utils.py': 2.5844798798856127e-05,\n",
       " 'anthropic_utils.py': 2.5844798798856127e-05,\n",
       " 'everlyai_utils.py': 2.5844798798856127e-05,\n",
       " 'cohere.py': 3.4460225082537724e-05,\n",
       " 'ollama.py': 2.5844798798856127e-05,\n",
       " 'llama_utils.py': 2.5844798798856127e-05,\n",
       " 'monsterapi.py': 2.6652976600273667e-05,\n",
       " 'predibase.py': 2.5844798798856127e-05,\n",
       " 'everlyai.py': 2.5844798798856127e-05,\n",
       " 'text_splitter': 5.009942761621649e-05,\n",
       " 'query_engine': 6.0234332096065755e-05,\n",
       " 'knowledge_graph_query_engine.py': 2.621968903402682e-05,\n",
       " 'citation_query_engine.py': 2.621968903402682e-05,\n",
       " 'sql_join_query_engine.py': 2.621968903402682e-05,\n",
       " 'retriever_query_engine.py': 2.621968903402682e-05,\n",
       " 'multi_modal.py': 2.621968903402682e-05,\n",
       " 'retry_query_engine.py': 2.621968903402682e-05,\n",
       " 'router_query_engine.py': 2.621968903402682e-05,\n",
       " 'cogniswitch_query_engine.py': 2.621968903402682e-05,\n",
       " 'multistep_query_engine.py': 2.621968903402682e-05,\n",
       " 'pandas_query_engine.py': 2.621968903402682e-05,\n",
       " 'sql_vector_query_engine.py': 2.621968903402682e-05,\n",
       " 'graph_query_engine.py': 2.621968903402682e-05,\n",
       " 'sub_question_query_engine.py': 2.621968903402682e-05,\n",
       " 'transform_query_engine.py': 2.621968903402682e-05,\n",
       " 'retry_source_query_engine.py': 2.621968903402682e-05,\n",
       " 'flare': 2.621968903402682e-05,\n",
       " 'answer_inserter.py': 2.9795137251486444e-05,\n",
       " 'extractors': 3.86617125467603e-05,\n",
       " 'metadata_extractors.py': 3.2141512685634956e-05,\n",
       " 'marvin_metadata_extractor.py': 3.2141512685634956e-05,\n",
       " 'command_line': 3.86617125467603e-05,\n",
       " 'command_line.py': 4.231681190047976e-05,\n",
       " 'llama_dataset': 3.9326931554116644e-05,\n",
       " 'generator.py': 2.880596373347999e-05,\n",
       " 'rag.py': 2.880596373347999e-05,\n",
       " 'evaluator_evaluation.py': 2.880596373347999e-05,\n",
       " 'selectors': 3.9508255011136775e-05,\n",
       " 'embedding_selectors.py': 2.9690907608811514e-05,\n",
       " 'pydantic_selectors.py': 2.9690907608811514e-05,\n",
       " 'llm_selectors.py': 2.9690907608811514e-05,\n",
       " 'tests': 3.967108410548641e-05,\n",
       " 'test_utils.py': 4.331551690380329e-05,\n",
       " 'conftest.py': 5.856881409245937e-05,\n",
       " 'test_schema.py': 2.6204522340114882e-05,\n",
       " 'ruff.toml': 2.6204522340114882e-05,\n",
       " 'mock_utils': 2.6204522340114882e-05,\n",
       " 'mock_text_splitter.py': 2.982850776119417e-05,\n",
       " 'mock_predict.py': 2.982850776119417e-05,\n",
       " 'mock_prompts.py': 2.982850776119417e-05,\n",
       " 'mock_utils.py': 3.444908859580384e-05,\n",
       " 'test_elasticsearch.py': 2.6772355542999627e-05,\n",
       " 'test_docarray.py': 2.565865235938723e-05,\n",
       " 'test_epsilla.py': 2.565865235938723e-05,\n",
       " 'test_simple.py': 2.847245510613805e-05,\n",
       " 'test_astra.py': 2.565865235938723e-05,\n",
       " 'test_mongodb.py': 2.565865235938723e-05,\n",
       " 'test_lancedb.py': 2.565865235938723e-05,\n",
       " 'test_tencentvectordb.py': 2.565865235938723e-05,\n",
       " 'test_milvus.py': 2.565865235938723e-05,\n",
       " 'test_cassandra.py': 2.565865235938723e-05,\n",
       " 'test_qdrant.py': 2.565865235938723e-05,\n",
       " 'test_rockset.py': 2.565865235938723e-05,\n",
       " 'test_singlestoredb.py': 2.565865235938723e-05,\n",
       " 'test_timescalevector.py': 2.565865235938723e-05,\n",
       " 'test_metadata_filters.py': 2.565865235938723e-05,\n",
       " 'test_tair.py': 2.565865235938723e-05,\n",
       " 'test_azurecosmosmongo.py': 2.565865235938723e-05,\n",
       " 'test_cogsearch.py': 2.565865235938723e-05,\n",
       " 'test_google.py': 2.911146238836153e-05,\n",
       " 'test_weaviate.py': 2.565865235938723e-05,\n",
       " 'test_lantern.py': 2.565865235938723e-05,\n",
       " 'test_postgres.py': 2.565865235938723e-05,\n",
       " 'docker-compose': 2.565865235938723e-05,\n",
       " 'elasticsearch.yml': 4.714684183773766e-05,\n",
       " 'test_pipeline.py': 2.8880539514990203e-05,\n",
       " 'test_cache.py': 2.8880539514990203e-05,\n",
       " 'test_loading.py': 2.632927075657671e-05,\n",
       " 'test_loading_graph.py': 2.632927075657671e-05,\n",
       " 'test_service_context.py': 2.632927075657671e-05,\n",
       " 'test_prompt_helper.py': 2.632927075657671e-05,\n",
       " 'test_embedding_retriever.py': 2.7152645391267427e-05,\n",
       " 'test_retrievers.py': 8.048426467044037e-05,\n",
       " 'test_index.py': 7.575487711582847e-05,\n",
       " 'test_json_query.py': 2.7741514152240674e-05,\n",
       " 'test_sql_query.py': 2.7741514152240674e-05,\n",
       " 'test_base.py': 0.00011745656982769585,\n",
       " 'test_vectara.py': 2.7112986009866407e-05,\n",
       " 'test_tree_summarize.py': 2.8781222330304924e-05,\n",
       " 'test_response_builder.py': 2.8781222330304924e-05,\n",
       " 'test_myscale.py': 2.621480504512712e-05,\n",
       " 'mock_faiss.py': 2.621480504512712e-05,\n",
       " 'mock_services.py': 2.621480504512712e-05,\n",
       " 'test_pinecone.py': 2.621480504512712e-05,\n",
       " 'test_faiss.py': 2.621480504512712e-05,\n",
       " 'test_deeplake.py': 2.621480504512712e-05,\n",
       " 'test_output_parser.py': 2.996918903345466e-05,\n",
       " 'test_compose.py': 3.248427915576454e-05,\n",
       " 'test_compose_vector.py': 3.248427915576454e-05,\n",
       " 'test_query_bundle.py': 3.248427915576454e-05,\n",
       " 'test_embedding_utils.py': 3.248427915576454e-05,\n",
       " 'test_ondemand_loader.py': 2.7100903248469673e-05,\n",
       " 'test_query_engine_tool.py': 2.7100903248469673e-05,\n",
       " 'test_gradient.py': 2.695850198246853e-05,\n",
       " 'test_llm_rails.py': 2.6471683059350805e-05,\n",
       " 'test_fastembed.py': 2.6471683059350805e-05,\n",
       " 'test_bedrock.py': 2.695850198246853e-05,\n",
       " 'test_huggingface.py': 2.695850198246853e-05,\n",
       " 'test_html.py': 2.9251757942216366e-05,\n",
       " 'test_unstructured.py': 2.9251757942216366e-05,\n",
       " 'metadata_extractor.py': 2.9251757942216366e-05,\n",
       " 'test_json.py': 3.000531059144647e-05,\n",
       " 'test_markdown_element.py': 2.9251757942216366e-05,\n",
       " 'test_markdown.py': 2.9251757942216366e-05,\n",
       " 'test_chat_memory_buffer.py': 3.520095965345064e-05,\n",
       " 'test_prompt_registry.py': 2.7729149287460993e-05,\n",
       " 'test_predictor.py': 2.7729149287460993e-05,\n",
       " 'test_token_counter.py': 2.757889092649795e-05,\n",
       " 'test_llama_debug.py': 2.757889092649795e-05,\n",
       " 'test_optimizer.py': 2.7397004691302223e-05,\n",
       " 'test_longcontext_reorder.py': 2.7397004691302223e-05,\n",
       " 'test_llm_rerank.py': 2.7397004691302223e-05,\n",
       " 'test_metadata_replacement.py': 2.7397004691302223e-05,\n",
       " 'test_replicate_multi_modal.py': 2.9690907608811514e-05,\n",
       " 'test_node_mapping.py': 2.9934110863951793e-05,\n",
       " 'test_react_agent.py': 2.796566518232308e-05,\n",
       " 'test_react_output_parser.py': 2.796566518232308e-05,\n",
       " 'test_openai_assistant_agent.py': 3.0440330518240967e-05,\n",
       " 'test_openai_agent.py': 3.0440330518240967e-05,\n",
       " 'test_storage_context.py': 2.815539984496259e-05,\n",
       " 'test_simple_kvstore.py': 2.6906987558101955e-05,\n",
       " 'mock_mongodb.py': 2.6906987558101955e-05,\n",
       " 'test_dynamodb_kvstore.py': 2.6906987558101955e-05,\n",
       " 'test_firestore_kvstore.py': 2.6906987558101955e-05,\n",
       " 'test_s3_kvstore.py': 2.6906987558101955e-05,\n",
       " 'test_mongodb_kvstore.py': 2.6906987558101955e-05,\n",
       " 'test_redis_kvstore.py': 2.6906987558101955e-05,\n",
       " 'test_mongo_docstore.py': 3.077049305225759e-05,\n",
       " 'test_dynamodb_docstore.py': 3.077049305225759e-05,\n",
       " 'test_simple_docstore.py': 3.077049305225759e-05,\n",
       " 'test_redis_docstore.py': 3.077049305225759e-05,\n",
       " 'test_firestore_docstore.py': 3.077049305225759e-05,\n",
       " 'test_dynamodb_index_store.py': 2.7698520340691668e-05,\n",
       " 'test_firestore_indexstore.py': 2.7698520340691668e-05,\n",
       " 'test_simple_index_store.py': 2.7698520340691668e-05,\n",
       " 'test_sql_wrapper.py': 3.402383534188462e-05,\n",
       " 'test_pydantic.py': 2.842007782493368e-05,\n",
       " 'test_selection.py': 2.842007782493368e-05,\n",
       " 'token_predictor': 2.6204522340114882e-05,\n",
       " 'test_llm_program.py': 2.9044388352949915e-05,\n",
       " 'test_lmformatenforcer.py': 2.9044388352949915e-05,\n",
       " 'test_guidance.py': 2.9044388352949915e-05,\n",
       " 'test_multi_modal_llm_program.py': 2.9044388352949915e-05,\n",
       " 'test_guidance_utils.py': 2.7423238491274047e-05,\n",
       " 'test_mixin.py': 2.7423238491274047e-05,\n",
       " 'test_refine.py': 2.705578377058471e-05,\n",
       " 'test_dataset_generation.py': 2.6913417936688957e-05,\n",
       " 'test_condense_plus_context.py': 2.7314957453100516e-05,\n",
       " 'test_condense_question.py': 2.7314957453100516e-05,\n",
       " 'test_simplewebreader.py': 2.6111532524968513e-05,\n",
       " 'test_load_reader.py': 2.6111532524968513e-05,\n",
       " 'test_html_reader.py': 2.6111532524968513e-05,\n",
       " 'test_file.py': 2.6111532524968513e-05,\n",
       " 'test_string_iterable.py': 2.6111532524968513e-05,\n",
       " 'test_mongo.py': 2.6111532524968513e-05,\n",
       " 'test_llm_generators.py': 2.8824322062196895e-05,\n",
       " 'test_guidance_generator.py': 2.8824322062196895e-05,\n",
       " 'test_anthropic.py': 2.5844798798856127e-05,\n",
       " 'test_openai.py': 2.5844798798856127e-05,\n",
       " 'test_palm.py': 2.5844798798856127e-05,\n",
       " 'test_vllm.py': 2.5844798798856127e-05,\n",
       " 'test_konko.py': 2.5844798798856127e-05,\n",
       " 'test_openai_utils.py': 2.5844798798856127e-05,\n",
       " 'test_cohere.py': 2.5844798798856127e-05,\n",
       " 'test_xinference.py': 2.5844798798856127e-05,\n",
       " 'test_langchain.py': 2.5844798798856127e-05,\n",
       " 'test_localai.py': 2.5844798798856127e-05,\n",
       " 'test_anthropic_utils.py': 2.5844798798856127e-05,\n",
       " 'test_watsonx.py': 2.5844798798856127e-05,\n",
       " 'test_llama_utils.py': 2.5844798798856127e-05,\n",
       " 'test_litellm.py': 2.5844798798856127e-05,\n",
       " 'test_custom.py': 2.5844798798856127e-05,\n",
       " 'test_vertex.py': 2.5844798798856127e-05,\n",
       " 'test_gemini.py': 2.5844798798856127e-05,\n",
       " 'test_openai_like.py': 2.5844798798856127e-05,\n",
       " 'test_ai21.py': 2.5844798798856127e-05,\n",
       " 'test_rungpt.py': 2.5844798798856127e-05,\n",
       " 'test_sentence_splitter.py': 3.23220543525152e-05,\n",
       " 'test_code_splitter.py': 3.23220543525152e-05,\n",
       " 'test_token_splitter.py': 3.23220543525152e-05,\n",
       " 'test_cogniswitch_query_engine.py': 2.621968903402682e-05,\n",
       " 'test_pandas.py': 2.621968903402682e-05,\n",
       " 'test_retriever_query_engine.py': 2.621968903402682e-05,\n",
       " 'test_llm_selectors.py': 2.9690907608811514e-05,\n",
       " 'docs': 5.2940974282117275e-05,\n",
       " 'index.rst': 2.8220638157666743e-05,\n",
       " 'conf.py': 2.74122995112069e-05,\n",
       " 'DOCS_README.md': 2.74122995112069e-05,\n",
       " 'make.bat': 2.74122995112069e-05,\n",
       " 'contributing': 2.74122995112069e-05,\n",
       " 'documentation.rst': 3.7078890249406155e-05,\n",
       " 'contributing.rst': 3.7078890249406155e-05,\n",
       " 'changes': 2.74122995112069e-05,\n",
       " 'deprecated_terms.md': 3.7078890249406155e-05,\n",
       " 'changelog.rst': 3.7078890249406155e-05,\n",
       " 'understanding': 2.74122995112069e-05,\n",
       " 'understanding.md': 2.7962626625442352e-05,\n",
       " 'using_llms': 2.7962626625442352e-05,\n",
       " 'privacy.md': 3.724750145979732e-05,\n",
       " 'using_llms.md': 3.724750145979732e-05,\n",
       " 'loading': 3.05672733751463e-05,\n",
       " 'llamahub.md': 2.9065294836318256e-05,\n",
       " 'loading.md': 2.9065294836318256e-05,\n",
       " 'evaluating': 3.05672733751463e-05,\n",
       " 'evaluating.md': 2.8241447067300514e-05,\n",
       " 'cost_analysis': 2.8241447067300514e-05,\n",
       " 'usage_pattern.md': 6.34713977182213e-05,\n",
       " 'root.md': 0.0001546531563286904,\n",
       " 'storing': 3.05672733751463e-05,\n",
       " 'storing.md': 2.9065294836318256e-05,\n",
       " 'tracing_and_debugging': 2.7962626625442352e-05,\n",
       " 'tracing_and_debugging.md': 4.9137023043856225e-05,\n",
       " 'indexing': 3.05672733751463e-05,\n",
       " 'indexing.md': 2.8601880466245776e-05,\n",
       " 'querying': 3.05672733751463e-05,\n",
       " 'querying.md': 2.9683180663081565e-05,\n",
       " 'putting_it_all_together': 2.7962626625442352e-05,\n",
       " 'structured_data.md': 2.773588419255019e-05,\n",
       " 'putting_it_all_together.md': 2.773588419255019e-05,\n",
       " 'q_and_a.md': 3.2424248342017286e-05,\n",
       " 'graphs.md': 2.773588419255019e-05,\n",
       " 'agents.md': 3.2424248342017286e-05,\n",
       " 'apps.md': 2.773588419255019e-05,\n",
       " 'q_and_a': 2.773588419255019e-05,\n",
       " 'unified_query.md': 3.7201923048704195e-05,\n",
       " 'terms_definitions_tutorial.md': 3.7201923048704195e-05,\n",
       " 'structured_data': 2.773588419255019e-05,\n",
       " 'Airbyte_demo.ipynb': 4.904586622166998e-05,\n",
       " 'chatbots': 2.773588419255019e-05,\n",
       " 'building_a_chatbot.md': 4.904586622166998e-05,\n",
       " 'apps': 2.773588419255019e-05,\n",
       " 'fullstack_app_guide.md': 3.7201923048704195e-05,\n",
       " 'fullstack_with_delphic.md': 3.7201923048704195e-05,\n",
       " '_static': 2.74122995112069e-05,\n",
       " 'faiss_index_0.png': 2.565865235938723e-05,\n",
       " 'faiss_index_1.png': 2.565865235938723e-05,\n",
       " 'weaviate_reader_1.png': 2.565865235938723e-05,\n",
       " 'qdrant_index_0.png': 2.565865235938723e-05,\n",
       " 'weaviate_reader_0.png': 2.565865235938723e-05,\n",
       " 'pinecone_reader.png': 2.565865235938723e-05,\n",
       " 'pinecone_index_0.png': 2.565865235938723e-05,\n",
       " 'qdrant_reader.png': 2.565865235938723e-05,\n",
       " 'weaviate_index_0.png': 2.565865235938723e-05,\n",
       " 'simple_index_0.png': 2.565865235938723e-05,\n",
       " 'vector_store.png': 2.632927075657671e-05,\n",
       " 'keyword.png': 2.632927075657671e-05,\n",
       " 'keyword_query.png': 2.632927075657671e-05,\n",
       " 'tree_summarize.png': 2.632927075657671e-05,\n",
       " 'tree_query.png': 2.632927075657671e-05,\n",
       " 'list.png': 2.632927075657671e-05,\n",
       " 'list_query.png': 2.632927075657671e-05,\n",
       " 'list_filter_query.png': 2.632927075657671e-05,\n",
       " 'vector_store_query.png': 2.632927075657671e-05,\n",
       " 'tree.png': 2.632927075657671e-05,\n",
       " 'create_and_refine.png': 2.632927075657671e-05,\n",
       " 'doc_example.jpeg': 2.6471683059350805e-05,\n",
       " 'node_postprocessors': 3.0916960702415014e-05,\n",
       " 'prev_next.png': 3.1972903233959724e-05,\n",
       " 'recency.png': 3.1972903233959724e-05,\n",
       " 'response_1.jpeg': 2.8781222330304924e-05,\n",
       " 'data_connectors': 2.725697892242819e-05,\n",
       " 'llamahub.png': 2.621716412654846e-05,\n",
       " 'css': 2.6591759915071854e-05,\n",
       " 'algolia.css': 3.667587999770754e-05,\n",
       " 'custom.css': 3.667587999770754e-05,\n",
       " 'js': 2.6591759915071854e-05,\n",
       " 'mendablesearch.js': 3.667587999770754e-05,\n",
       " 'algolia.js': 3.667587999770754e-05,\n",
       " 'production_rag': 2.6591759915071854e-05,\n",
       " 'joint_qa_summary.png': 3.101692993672297e-05,\n",
       " 'decouple_chunks.png': 3.101692993672297e-05,\n",
       " 'structured_retrieval.png': 3.101692993672297e-05,\n",
       " 'doc_agents.png': 3.101692993672297e-05,\n",
       " 'agent_step_execute.png': 2.9768438153595342e-05,\n",
       " 'structured_output': 2.6591759915071854e-05,\n",
       " 'diagram1.png': 3.667587999770754e-05,\n",
       " 'program2.png': 3.667587999770754e-05,\n",
       " 'query_transformations': 2.725697892242819e-05,\n",
       " 'multi_step_diagram.png': 2.999757483011268e-05,\n",
       " 'single_step_diagram.png': 2.999757483011268e-05,\n",
       " 'storage.png': 2.815539984496259e-05,\n",
       " 'integrations': 0.00010260120102657501,\n",
       " 'honeyhive.png': 2.9939793211324236e-05,\n",
       " 'arize_phoenix.png': 2.9939793211324236e-05,\n",
       " 'openllmetry.png': 2.9939793211324236e-05,\n",
       " 'trulens.png': 2.9939793211324236e-05,\n",
       " 'perfetto.png': 2.9939793211324236e-05,\n",
       " 'wandb.png': 2.9939793211324236e-05,\n",
       " 'diagram_b1.png': 2.8917240504218257e-05,\n",
       " 'diagram_b0.png': 2.8917240504218257e-05,\n",
       " 'diagram_q1.png': 2.8917240504218257e-05,\n",
       " 'diagram_q2.png': 2.8917240504218257e-05,\n",
       " 'diagram.png': 2.8917240504218257e-05,\n",
       " 'eval_response_context.png': 2.6913417936688957e-05,\n",
       " 'eval_query_sources.png': 2.6913417936688957e-05,\n",
       " 'eval_query_response_context.png': 2.6913417936688957e-05,\n",
       " 'contribution': 2.6591759915071854e-05,\n",
       " 'contrib.png': 4.7993780119676675e-05,\n",
       " 'disclosure.png': 3.248427915576454e-05,\n",
       " 'query_classes.png': 3.248427915576454e-05,\n",
       " 'getting_started': 2.8646079550540346e-05,\n",
       " 'querying.jpg': 2.7582592546565415e-05,\n",
       " 'rag.jpg': 2.7582592546565415e-05,\n",
       " 'indexing.jpg': 2.7582592546565415e-05,\n",
       " 'basic_rag.png': 2.7582592546565415e-05,\n",
       " 'stages.png': 2.7582592546565415e-05,\n",
       " 'api_reference': 2.74122995112069e-05,\n",
       " 'evaluation.rst': 2.616631852219825e-05,\n",
       " 'llm_predictor.rst': 2.616631852219825e-05,\n",
       " 'multi_modal.rst': 2.616631852219825e-05,\n",
       " 'callbacks.rst': 2.616631852219825e-05,\n",
       " 'agents.rst': 2.616631852219825e-05,\n",
       " 'response.rst': 2.616631852219825e-05,\n",
       " 'prompts.rst': 2.616631852219825e-05,\n",
       " 'llms.rst': 2.616631852219825e-05,\n",
       " 'playground.rst': 2.616631852219825e-05,\n",
       " 'storage.rst': 2.616631852219825e-05,\n",
       " 'service_context.rst': 2.616631852219825e-05,\n",
       " 'composability.rst': 2.616631852219825e-05,\n",
       " 'node.rst': 2.616631852219825e-05,\n",
       " 'readers.rst': 2.616631852219825e-05,\n",
       " 'node_postprocessor.rst': 2.616631852219825e-05,\n",
       " 'struct_store.rst': 2.7137609403036556e-05,\n",
       " 'finetuning.rst': 2.616631852219825e-05,\n",
       " 'example_notebooks.rst': 2.616631852219825e-05,\n",
       " 'query.rst': 2.616631852219825e-05,\n",
       " 'memory.rst': 2.616631852219825e-05,\n",
       " 'indices.rst': 2.616631852219825e-05,\n",
       " 'vector_store.rst': 3.06022689751947e-05,\n",
       " 'list.rst': 2.7804849005970514e-05,\n",
       " 'empty.rst': 2.7804849005970514e-05,\n",
       " 'kg.rst': 2.7804849005970514e-05,\n",
       " 'tree.rst': 2.7804849005970514e-05,\n",
       " 'table.rst': 2.7804849005970514e-05,\n",
       " 'service_context': 7.516317002540346e-05,\n",
       " 'embeddings.rst': 3.4525971713464364e-05,\n",
       " 'prompt_helper.rst': 3.4525971713464364e-05,\n",
       " 'node_parser.rst': 3.4525971713464364e-05,\n",
       " 'langchain_integrations': 2.616631852219825e-05,\n",
       " 'base.rst': 4.763898059148303e-05,\n",
       " 'docstore.rst': 2.815539984496259e-05,\n",
       " 'index_store.rst': 2.815539984496259e-05,\n",
       " 'kv_store.rst': 2.815539984496259e-05,\n",
       " 'indices_save_load.rst': 2.815539984496259e-05,\n",
       " 'openai.rst': 2.7069668040034323e-05,\n",
       " 'replicate.rst': 2.7069668040034323e-05,\n",
       " 'litellm.rst': 2.5844798798856127e-05,\n",
       " 'openai_like.rst': 2.5844798798856127e-05,\n",
       " 'palm.rst': 2.5844798798856127e-05,\n",
       " 'azure_openai.rst': 2.5844798798856127e-05,\n",
       " 'langchain.rst': 2.5844798798856127e-05,\n",
       " 'huggingface.rst': 2.5844798798856127e-05,\n",
       " 'gradient_base_model.rst': 2.5844798798856127e-05,\n",
       " 'predibase.rst': 2.5844798798856127e-05,\n",
       " 'gradient_model_adapter.rst': 2.5844798798856127e-05,\n",
       " 'llama_cpp.rst': 2.5844798798856127e-05,\n",
       " 'xinference.rst': 2.5844798798856127e-05,\n",
       " 'openllm.rst': 2.5844798798856127e-05,\n",
       " 'anthropic.rst': 2.5844798798856127e-05,\n",
       " 'response_synthesizer.rst': 3.248427915576454e-05,\n",
       " 'query_engines.rst': 3.248427915576454e-05,\n",
       " 'chat_engines.rst': 3.248427915576454e-05,\n",
       " 'retrievers.rst': 3.248427915576454e-05,\n",
       " 'query_transform.rst': 3.248427915576454e-05,\n",
       " 'query_bundle.rst': 3.248427915576454e-05,\n",
       " 'transform.rst': 2.683355812513221e-05,\n",
       " 'chat_engines': 4.0410626878470474e-05,\n",
       " 'condense_plus_context_chat_engine.rst': 3.113467445132301e-05,\n",
       " 'simple_chat_engine.rst': 3.113467445132301e-05,\n",
       " 'condense_question_chat_engine.rst': 3.113467445132301e-05,\n",
       " 'query_engines': 3.248427915576454e-05,\n",
       " 'sql_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'knowledge_graph_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'router_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'retriever_router_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'graph_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'retriever_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'sql_join_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'multistep_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'flare_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'citation_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'transform_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'sub_question_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'pandas_query_engine.rst': 2.7490555645558254e-05,\n",
       " 'examples': 4.053761210069922e-05,\n",
       " 'citation': 2.6023198883094744e-05,\n",
       " 'pdf_page_reference.ipynb': 4.752703601584818e-05,\n",
       " 'TypesenseDemo.ipynb': 2.565865235938723e-05,\n",
       " 'BagelAutoRetriever.ipynb': 2.565865235938723e-05,\n",
       " 'RocksetIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'TencentVectorDBIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'QdrantIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'Timescalevector.ipynb': 2.565865235938723e-05,\n",
       " 'MongoDBAtlasVectorSearch.ipynb': 2.565865235938723e-05,\n",
       " 'DocArrayInMemoryIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'chroma_auto_retriever.ipynb': 2.565865235938723e-05,\n",
       " 'ZepIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'FaissIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'SimpleIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'qdrant_hybrid.ipynb': 2.565865235938723e-05,\n",
       " 'DeepLakeIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'pinecone_metadata_filter.ipynb': 2.565865235938723e-05,\n",
       " 'elasticsearch_auto_retriever.ipynb': 2.565865235938723e-05,\n",
       " 'SimpleIndexOnS3.ipynb': 2.565865235938723e-05,\n",
       " 'CassandraIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'Elasticsearch_demo.ipynb': 2.565865235938723e-05,\n",
       " 'AwadbDemo.ipynb': 2.565865235938723e-05,\n",
       " 'postgres.ipynb': 2.565865235938723e-05,\n",
       " 'chroma_metadata_filter.ipynb': 2.565865235938723e-05,\n",
       " 'AzureCosmosDBMongoDBvCoreDemo.ipynb': 2.565865235938723e-05,\n",
       " 'Neo4jVectorDemo.ipynb': 2.565865235938723e-05,\n",
       " 'ElasticsearchIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'SimpleIndexDemoLlama-Local.ipynb': 2.565865235938723e-05,\n",
       " 'MyScaleIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'MetalIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'AsyncIndexCreationDemo.ipynb': 2.565865235938723e-05,\n",
       " 'TairIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'PineconeIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'RedisIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'SimpleIndexDemoLlama2.ipynb': 2.565865235938723e-05,\n",
       " 'WeaviateIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'SupabaseVectorIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'PGVectoRsDemo.ipynb': 2.565865235938723e-05,\n",
       " 'WeaviateIndex_metadata_filter.ipynb': 2.565865235938723e-05,\n",
       " 'WeaviateIndexDemo-Hybrid.ipynb': 2.565865235938723e-05,\n",
       " 'DocArrayHnswIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'DashvectorIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'OpensearchDemo.ipynb': 2.565865235938723e-05,\n",
       " 'PineconeIndexDemo-Hybrid.ipynb': 2.565865235938723e-05,\n",
       " 'Qdrant_metadata_filter.ipynb': 2.565865235938723e-05,\n",
       " 'CognitiveSearchIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'SimpleIndexDemoMMR.ipynb': 2.565865235938723e-05,\n",
       " 'pinecone_auto_retriever.ipynb': 2.565865235938723e-05,\n",
       " 'ChromaIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'LanceDBIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'BagelIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'EpsillaIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'MilvusIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'AstraDBIndexDemo.ipynb': 2.565865235938723e-05,\n",
       " 'Lantern.ipynb': 2.565865235938723e-05,\n",
       " 'index_faiss_core.index': 2.565865235938723e-05,\n",
       " 'PineconeIndexDemo-0.6.0.ipynb': 2.565865235938723e-05,\n",
       " 'existing_data': 2.565865235938723e-05,\n",
       " 'pinecone_existing_data.ipynb': 3.625241085673803e-05,\n",
       " 'weaviate_existing_data.ipynb': 3.625241085673803e-05,\n",
       " 'vectaraDemo.ipynb': 2.7112986009866407e-05,\n",
       " 'manage_retrieval_benchmark.ipynb': 2.7112986009866407e-05,\n",
       " 'GoogleDemo.ipynb': 2.7112986009866407e-05,\n",
       " 'zcpDemo.ipynb': 2.7112986009866407e-05,\n",
       " 'bm25_retriever.ipynb': 2.683355812513221e-05,\n",
       " 'router_retriever.ipynb': 2.683355812513221e-05,\n",
       " 'recursive_retriever_nodes.ipynb': 2.683355812513221e-05,\n",
       " 'auto_vs_recursive_retriever.ipynb': 2.683355812513221e-05,\n",
       " 'ensemble_retrieval.ipynb': 2.683355812513221e-05,\n",
       " 'simple_fusion.ipynb': 2.683355812513221e-05,\n",
       " 'auto_merging_retriever.ipynb': 2.683355812513221e-05,\n",
       " 'recurisve_retriever_nodes_braintrust.ipynb': 2.683355812513221e-05,\n",
       " 'deep_memory.ipynb': 2.683355812513221e-05,\n",
       " 'you_retriever.ipynb': 2.683355812513221e-05,\n",
       " 'reciprocal_rerank_fusion.ipynb': 2.683355812513221e-05,\n",
       " 'async_ingestion_pipeline.ipynb': 2.8880539514990203e-05,\n",
       " 'document_management_pipeline.ipynb': 2.8880539514990203e-05,\n",
       " 'ingestion_gdrive.ipynb': 2.8880539514990203e-05,\n",
       " 'advanced_ingestion_pipeline.ipynb': 2.8880539514990203e-05,\n",
       " 'redis_ingestion_pipeline.ipynb': 2.8880539514990203e-05,\n",
       " 'OnDemandLoaderTool.ipynb': 2.7100903248469673e-05,\n",
       " 'llm': 0.00011662926027491783,\n",
       " 'rungpt.ipynb': 2.7098566806047295e-05,\n",
       " 'watsonx.ipynb': 2.7098566806047295e-05,\n",
       " 'openllm.ipynb': 2.7098566806047295e-05,\n",
       " 'openai_json_vs_function_calling.ipynb': 2.7098566806047295e-05,\n",
       " 'portkey.ipynb': 2.7098566806047295e-05,\n",
       " 'everlyai.ipynb': 2.7098566806047295e-05,\n",
       " 'palm.ipynb': 2.7098566806047295e-05,\n",
       " 'cohere.ipynb': 2.7098566806047295e-05,\n",
       " 'vertex.ipynb': 2.7098566806047295e-05,\n",
       " 'predibase.ipynb': 2.7098566806047295e-05,\n",
       " 'llama_api.ipynb': 2.7098566806047295e-05,\n",
       " 'clarifai.ipynb': 2.8212269989659695e-05,\n",
       " 'bedrock.ipynb': 2.8212269989659695e-05,\n",
       " 'llama_2.ipynb': 2.7098566806047295e-05,\n",
       " 'gradient_model_adapter.ipynb': 2.7098566806047295e-05,\n",
       " 'xinference_local_deployment.ipynb': 2.7098566806047295e-05,\n",
       " 'azure_openai.ipynb': 2.7098566806047295e-05,\n",
       " 'gemini.ipynb': 2.9437139230837888e-05,\n",
       " 'huggingface.ipynb': 2.8212269989659695e-05,\n",
       " 'anyscale.ipynb': 2.7098566806047295e-05,\n",
       " 'vicuna.ipynb': 2.7098566806047295e-05,\n",
       " 'openrouter.ipynb': 2.7098566806047295e-05,\n",
       " 'llama_2_rap_battle.ipynb': 2.7098566806047295e-05,\n",
       " 'vllm.ipynb': 2.7098566806047295e-05,\n",
       " 'localai.ipynb': 2.7098566806047295e-05,\n",
       " 'llm_predictor.ipynb': 2.7098566806047295e-05,\n",
       " 'mistralai.ipynb': 2.8212269989659695e-05,\n",
       " 'monsterapi.ipynb': 2.7098566806047295e-05,\n",
       " 'ai21.ipynb': 2.7098566806047295e-05,\n",
       " 'llama_2_llama_cpp.ipynb': 2.7098566806047295e-05,\n",
       " 'perplexity.ipynb': 2.7098566806047295e-05,\n",
       " 'litellm.ipynb': 2.7098566806047295e-05,\n",
       " 'ollama.ipynb': 2.7098566806047295e-05,\n",
       " 'langchain.ipynb': 2.7098566806047295e-05,\n",
       " 'openai.ipynb': 2.7098566806047295e-05,\n",
       " 'anthropic.ipynb': 2.7098566806047295e-05,\n",
       " 'gradient_base_model.ipynb': 2.7098566806047295e-05,\n",
       " 'azure_playground.png': 2.7098566806047295e-05,\n",
       " 'azure_env.png': 2.7098566806047295e-05,\n",
       " 'Konko.ipynb': 2.7098566806047295e-05,\n",
       " 'fastembed.ipynb': 2.6471683059350805e-05,\n",
       " 'text_embedding_inference.ipynb': 2.6471683059350805e-05,\n",
       " 'voyageai.ipynb': 2.6471683059350805e-05,\n",
       " 'ollama_embedding.ipynb': 2.6471683059350805e-05,\n",
       " 'gradient.ipynb': 2.6471683059350805e-05,\n",
       " 'custom_embeddings.ipynb': 2.6471683059350805e-05,\n",
       " 'jinaai_embeddings.ipynb': 2.6471683059350805e-05,\n",
       " 'jina_embeddings.ipynb': 2.6471683059350805e-05,\n",
       " 'llm_rails.ipynb': 2.6471683059350805e-05,\n",
       " 'google_palm.ipynb': 2.6471683059350805e-05,\n",
       " 'Langchain.ipynb': 2.6471683059350805e-05,\n",
       " 'elasticsearch.ipynb': 2.6471683059350805e-05,\n",
       " ...}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform page rank on the graph\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "page_rank = nx.pagerank(G)\n",
    "page_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.1214549799908395e-05,\n",
       " 3.139584137403907e-05,\n",
       " 2.818730587641715e-05,\n",
       " 0.00021939831410859982,\n",
       " 3.34182345009083e-05)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_rank['OpenAIAgent'], page_rank['AgentRunner'], page_rank[\"BaseAgentRunner\"], page_rank[\"BaseReader\"], page_rank[\"BaseAgent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs2 = VectorSearch(\n",
    "    collection_name=\"explanations\",\n",
    "    collection_path=\"repomanager/statemanager/state/{}/meta/storage\".format(repo_id),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PDFReader', 'class'),\n",
       " ('DocxReader', 'class'),\n",
       " ('BaseReader', 'class'),\n",
       " ('GoogleDocsReader', 'class'),\n",
       " ('MyScaleReader', 'class')]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How to load a pdf\"\n",
    "res = vs2.search(query, top_k=5, type=\"class\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BaseReader', 'class'),\n",
       " ('PDFReader', 'class'),\n",
       " ('GoogleDocsReader', 'class'),\n",
       " ('DocxReader', 'class'),\n",
       " ('MyScaleReader', 'class')]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank res by page rank\n",
    "res_ranked = sorted(res, key=lambda x: page_rank[x[0]], reverse=True)\n",
    "res_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
